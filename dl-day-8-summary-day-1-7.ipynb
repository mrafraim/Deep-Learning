{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-8-summary-day-1-7?scriptVersionId=286306570\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Day 8: Summary (Day 1 → 7)\n\nWelcome to Day 8!\n\nThis notebook consolidates everything learned so far:\n- What Deep Learning is\n- Neurons, weights, bias, activations\n- Forward propagation\n- Loss functions\n- Gradient Descent intuition\n- Tiny neural networks\n- Manual calculations\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# Big Picture (Mental Model)\n\nDeep Learning pipeline:\n\n$$\n\\text{Input} \\rightarrow \\text{Linear Combination} \\rightarrow \\text{Activation} \\rightarrow \\text{Prediction} \\rightarrow \\text{Loss}\n$$\n\nTraining loop (intuition):\n\n1. Guess parameters (weights, bias)\n2. Make prediction (forward pass)\n3. Measure error (loss)\n4. Adjust parameters (gradient descent)\n5. Repeat\n","metadata":{}},{"cell_type":"markdown","source":"## Single Neuron Recap\n\nMathematical form:\n\n$$\nz = w x + b\n$$\n\nActivation:\n- ReLU: $ \\max(0, z) $\n- Sigmoid: $ \\frac{1}{1 + e^{-z}} $\n\nNeuron output:\n$$\na = f(z)\n$$\n","metadata":{}},{"cell_type":"markdown","source":"# Code: Single Neuron Forward Pass","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nx = 2.5\nw = 0.8\nb = -0.4\n\nz = w * x + b\na_relu = max(0, z)\na_sigmoid = 1 / (1 + np.exp(-z))\n\nz, a_relu, a_sigmoid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T09:10:19.302576Z","iopub.execute_input":"2025-12-15T09:10:19.302962Z","iopub.status.idle":"2025-12-15T09:10:19.317324Z","shell.execute_reply.started":"2025-12-15T09:10:19.302932Z","shell.execute_reply":"2025-12-15T09:10:19.31606Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Forward Propagation (Vectorized)\n\nFor a layer:\n\n$$\nZ = W X + b\n$$\n$$\nA = f(Z)\n$$\n\nWhy vectorization?\n- Faster computation\n- Cleaner math\n- Scales to large networks\n","metadata":{}},{"cell_type":"markdown","source":"# Code: Layer Forward Propagation","metadata":{}},{"cell_type":"code","source":"X = np.array([[1],\n              [2]])\n\nW = np.array([[0.5, -0.3],\n              [0.8,  0.1]])\n\nb = np.array([[0.0],\n              [-0.2]])\n\nZ = np.dot(W, X) + b\nA = np.maximum(0, Z)  # ReLU\n\nZ, A\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T09:11:50.016161Z","iopub.execute_input":"2025-12-15T09:11:50.016548Z","iopub.status.idle":"2025-12-15T09:11:50.031486Z","shell.execute_reply.started":"2025-12-15T09:11:50.016523Z","shell.execute_reply":"2025-12-15T09:11:50.029911Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss Functions\n\nLoss quantifies prediction error.\n\n### Mean Squared Error (Regression)\n$$\nL = (y - \\hat{y})^2\n$$\n\n### Binary Cross-Entropy (Classification)\n$$\nL = -\\left[y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})\\right]\n$$\n\nLower loss = better model.\n","metadata":{}},{"cell_type":"markdown","source":"# Code: Loss Examples","metadata":{}},{"cell_type":"code","source":"y_true = 1\ny_pred = 0.7\n\nmse = (y_true - y_pred)**2\nbce = -(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n\nmse, bce","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T09:13:46.845564Z","iopub.execute_input":"2025-12-15T09:13:46.845954Z","iopub.status.idle":"2025-12-15T09:13:46.853847Z","shell.execute_reply.started":"2025-12-15T09:13:46.845927Z","shell.execute_reply":"2025-12-15T09:13:46.852705Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Gradient Descent (Intuition)\n\nGoal:\n> Find weights that minimize loss.\n\nUpdate rule:\n$$\nw_{new} = w - \\alpha \\frac{dL}{dw}\n$$\n\nWhere:\n- $\\alpha$ = learning rate\n- Gradient points to steepest increase\n- We move in the opposite direction\n","metadata":{}},{"cell_type":"markdown","source":"# Code: Simple Gradient Step","metadata":{}},{"cell_type":"code","source":"w = 1.0\ngrad = 0.6\nlr = 0.1\n\nw_new = w - lr * grad\nw_new\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T09:15:19.19752Z","iopub.execute_input":"2025-12-15T09:15:19.197938Z","iopub.status.idle":"2025-12-15T09:15:19.210614Z","shell.execute_reply.started":"2025-12-15T09:15:19.197909Z","shell.execute_reply":"2025-12-15T09:15:19.20817Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tiny Neural Network\n\n2-Layer Network:\n\n$$Input → \\text{Hidden Layer} → Output$$\n\nEach layer:\n$$\nZ^{(l)} = W^{(l)} A^{(l-1)} + b^{(l)}\n$$\n$$\nA^{(l)} = f(Z^{(l)})\n$$\n\nThis structure scales to deep networks.\n","metadata":{}},{"cell_type":"markdown","source":"# Code: Tiny 2-Layer Network (Forward Only)","metadata":{}},{"cell_type":"code","source":"# Input\nX = np.array([[1],\n              [2]])\n\n# Layer 1\nW1 = np.array([[0.4, -0.2],\n               [0.1,  0.6]])\nb1 = np.zeros((2,1))\n\nZ1 = np.dot(W1, X) + b1\nA1 = np.maximum(0, Z1)\n\n# Layer 2\nW2 = np.array([[0.7, -0.3]])\nb2 = np.array([[0.1]])\n\nZ2 = np.dot(W2, A1) + b2\nA2 = 1 / (1 + np.exp(-Z2))  # Sigmoid\n\nA2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T09:18:51.439247Z","iopub.execute_input":"2025-12-15T09:18:51.439621Z","iopub.status.idle":"2025-12-15T09:18:51.450882Z","shell.execute_reply.started":"2025-12-15T09:18:51.439594Z","shell.execute_reply":"2025-12-15T09:18:51.449436Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Activation Functions Comparison","metadata":{}},{"cell_type":"code","source":"z = np.linspace(-5, 5, 100)\n\nrelu = np.maximum(0, z)\nsigmoid = 1 / (1 + np.exp(-z))\ntanh = np.tanh(z)\n\nplt.figure(figsize=(12, 4))\n\n# ReLU\nplt.subplot(1, 3, 1)\nplt.plot(z, relu)\nplt.title(\"ReLU\")\nplt.xlabel(\"z\")\nplt.ylabel(\"Output\")\nplt.grid(True)\n\n# Sigmoid\nplt.subplot(1, 3, 2)\nplt.plot(z, sigmoid)\nplt.title(\"Sigmoid\")\nplt.xlabel(\"z\")\nplt.ylabel(\"Output\")\nplt.grid(True)\n\n# Tanh\nplt.subplot(1, 3, 3)\nplt.plot(z, tanh)\nplt.title(\"Tanh\")\nplt.xlabel(\"z\")\nplt.ylabel(\"Output\")\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T09:31:55.664647Z","iopub.execute_input":"2025-12-15T09:31:55.665019Z","iopub.status.idle":"2025-12-15T09:31:56.204827Z","shell.execute_reply.started":"2025-12-15T09:31:55.664995Z","shell.execute_reply":"2025-12-15T09:31:56.203637Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss vs Weight Curve","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate weights\nweights = np.linspace(-3, 3, 100)\n\n# Simple scenario\nx = 1\ny_true = 2\n\n# Loss function (MSE)\nlosses = [(w*x - y_true)**2 for w in weights]\n\nplt.figure()\nplt.plot(weights, losses)\nplt.xlabel(\"Weight\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss vs Weight (MSE)\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T09:30:04.835663Z","iopub.execute_input":"2025-12-15T09:30:04.836062Z","iopub.status.idle":"2025-12-15T09:30:05.04608Z","shell.execute_reply.started":"2025-12-15T09:30:04.836038Z","shell.execute_reply":"2025-12-15T09:30:05.044798Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What this shows\n\n- The curve’s lowest point = optimal weight\n\n- Gradient descent moves toward this minimum","metadata":{}},{"cell_type":"markdown","source":"# Gradient Descent Steps","metadata":{}},{"cell_type":"code","source":"# Gradient descent simulation\nw = -2.5\nlr = 0.1\nsteps = []\nloss_steps = []\n\nfor _ in range(20):\n    loss = (w*x - y_true)**2\n    grad = 2*(w*x - y_true)*x\n    w = w - lr * grad\n    steps.append(w)\n    loss_steps.append(loss)\n\nplt.figure()\nplt.plot(loss_steps, marker='o')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Reduction Over Iterations\")\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T09:29:36.880132Z","iopub.execute_input":"2025-12-15T09:29:36.880477Z","iopub.status.idle":"2025-12-15T09:29:37.082733Z","shell.execute_reply.started":"2025-12-15T09:29:36.880442Z","shell.execute_reply":"2025-12-15T09:29:37.081732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Key Takeaways\n\nYou now understand:\n- What Deep Learning actually does\n- How neurons compute outputs\n- Why activations matter\n- How forward propagation works\n- How loss measures error\n- Why gradient descent updates weights\n---","metadata":{}},{"cell_type":"markdown","source":"<p style=\"text-align:center; font-size:18px;\">\n© 2025 Mostafizur Rahman\n</p>\n","metadata":{}}]}