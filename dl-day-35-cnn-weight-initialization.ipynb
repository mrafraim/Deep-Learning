{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-35-cnn-weight-initialization?scriptVersionId=295980165\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"d0847c1f","metadata":{"papermill":{"duration":0.005645,"end_time":"2026-02-05T06:17:24.694631","exception":false,"start_time":"2026-02-05T06:17:24.688986","status":"completed"},"tags":[]},"source":["# Day 35: CNN Weight Initialization\n","*Xavier vs He · Dead ReLU · Empirical Behavior*\n","\n","Welocme to Day 35!\n","\n","Today is not about *choosing an initializer*.\n","\n","It’s about understanding:\n","- Why some CNNs never learn\n","- Why loss curves look “flat but not broken”\n","- Why optimizers get blamed unfairly\n","\n","By the end of today:\n","\n","✔ You’ll diagnose learning failure in minutes  \n","✔ You’ll pick initialization without guessing  \n","✔ You’ll recognize Dead ReLU instantly  \n","\n","If you found this notebook helpful, your **<b style=\"color:skyblue;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"83c686a0","metadata":{"papermill":{"duration":0.004262,"end_time":"2026-02-05T06:17:24.70291","exception":false,"start_time":"2026-02-05T06:17:24.698648","status":"completed"},"tags":[]},"source":["# Why Initialization Matters\n","\n","Training deep CNNs critically depends on **stable gradient flow**.\n","\n","Weight initialization directly controls:\n","- **Activation scale** during the forward pass\n","- **Gradient scale** during backpropagation\n","\n","### What goes wrong with poor initialization\n","- **Weights too small** → activations shrink layer by layer → gradients vanish  \n","- **Weights too large** → activations blow up → gradients explode  \n","\n","Neither case usually causes runtime errors.\n","\n","> Bad initialization doesn’t crash the model.  \n","> It silently prevents effective learning."]},{"cell_type":"markdown","id":"baeb29c2","metadata":{"papermill":{"duration":0.003045,"end_time":"2026-02-05T06:17:24.708631","exception":false,"start_time":"2026-02-05T06:17:24.705586","status":"completed"},"tags":[]},"source":["# What Failure Looks Like in Practice\n","\n","### Common symptoms\n","- **Training loss decreases extremely slowly or plateaus early**  \n","- **Accuracy remains near random guessing**  \n","- **Changing the optimizer has little to no effect**  \n","- **Lowering the learning rate worsens training**  \n","\n","Interpretation\n","> The model is mathematically incapable of propagating signal.\n","\n","Poor initialization causes activations or gradients to shrink or explode across layers, so learning signals never reach earlier layers. Optimization fails **silently**, even though the training loop appears normal.\n"]},{"cell_type":"markdown","id":"2d24c254","metadata":{"papermill":{"duration":0.002958,"end_time":"2026-02-05T06:17:24.715743","exception":false,"start_time":"2026-02-05T06:17:24.712785","status":"completed"},"tags":[]},"source":["# The Core Problem: Variance Propagation\n","\n","Variance propagation describes how the statistical spread (variance) of activations or gradients changes as signals pass through successive layers of a neural network.\n","\n","Formally, for a layer:\n","$$\n","z^{(l)} = \\sum_{i=1}^{n} w_i^{(l)} x_i^{(l)}\n","$$\n","\n","the output variance depends on the input variance:\n","$$\n","\\boxed{\\text{Var}\\!\\left(z^{(l)}\\right)\n","= n \\cdot \\text{Var}\\!\\left(w^{(l)}\\right)\n","\\cdot \\text{Var}\\!\\left(x^{(l)}\\right)}\n","$$\n","\n","Consider a single neuron in a neural network:\n","\n","$$\n","z = \\sum_{i=1}^{n} w_i x_i\n","$$\n","\n","- $z$ → pre-activation output of the neuron (input to the non-linearity)  \n","- $n$ → number of input connections (fan-in)  \n","- $w_i$ → weight associated with the $i$-th input  \n","- $x_i$ → $i$-th input activation  \n","\n","This is a weighted sum of inputs before applying an activation function.\n","\n","\n","### Variance of the neuron output\n","\n","Assuming:\n","- inputs $x_i$ are independent and identically distributed  \n","- weights $w_i$ are independent of inputs  \n","- both have zero mean  \n","\n","the variance of $z$ becomes:\n","\n","$$\n","\\text{Var}(z) = n \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","$$\n","\n","- $\\text{Var}(z)$ → variance of the neuron’s output  \n","- $n$ → number of summed terms (fan-in)  \n","- $\\text{Var}(w)$ → variance of the weight distribution  \n","- $\\text{Var}(x)$ → variance of the input activations  \n","\n","### Why this matters in deep networks\n","\n","As signals propagate through layers:\n","\n","- If $\\text{Var}(z)$ **increases layer by layer** → activations and gradients explode  \n","- If $\\text{Var}(z)$ **decreases layer by layer** → activations and gradients vanish  \n","\n","Both cases make learning ineffective.\n","\n","### Key Insight\n","> Proper initialization chooses $\\text{Var}(w)$ such that  \n","> $\\text{Var}(z)$ remains **approximately constant across layers**.\n","\n","This is the mathematical foundation behind **Xavier and He initialization**."]},{"cell_type":"markdown","id":"59d4476b","metadata":{"papermill":{"duration":0.002464,"end_time":"2026-02-05T06:17:24.721128","exception":false,"start_time":"2026-02-05T06:17:24.718664","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\"> Where Does  Var(z)  Come From?</p>\n","\n","Start with the neuron equation:\n","\n","$$\n","z = \\sum_{i=1}^{n} w_i x_i\n","$$\n","\n","This is a sum of random variables.\n","\n","\n","**Step 1: Variance of a sum**\n","\n","A basic probability rule:\n","\n","If random variables are independent,\n","\n","$$\n","\\text{Var}\\!\\left(\\sum_{i=1}^{n} y_i\\right)\n","= \\sum_{i=1}^{n} \\text{Var}(y_i)\n","$$\n","\n","So we apply this to:\n","\n","$$\n","z = w_i x_i\n","$$\n","\n","Then:\n","\n","$$\n","\\text{Var}(z) = \\sum_{i=1}^{n} \\text{Var}(w_i x_i)\n","$$\n","\n","\n","**Step 2: Variance of a product**\n","\n","Another key rule (under independence and zero mean):\n","\n","$$\n","\\text{Var}(w_i x_i)\n","= \\text{Var}(w_i)\\,\\text{Var}(x_i)\n","$$\n","\n","Why this holds:\n","- $w_i$ and $x_i$ are independent  \n","- $\\mathbb{E}[w_i] = 0$, $\\mathbb{E}[x_i] = 0$  \n","\n","So each term contributes:\n","\n","$$\n","\\text{Var}(w_i x_i) = \\text{Var}(w)\\,\\text{Var}(x)\n","$$\n","\n","\n","**Step 3: Sum all contributions**\n","\n","Since every term has the same variance:\n","\n","$$\n","\\text{Var}(z)\n","= \\sum_{i=1}^{n} \\text{Var}(w)\\,\\text{Var}(x)\n","$$\n","\n","$$\n","\\boxed{\n","\\text{Var}(z) = n \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","}\n","$$\n","\n","\n","### What this means\n","\n","- Each input contributes **a little variance**\n","- Adding $n$ such contributions multiplies variance by $n$\n","\n","> More connections = more variance unless weights are scaled down\n","\n","\n","### Why this breaks deep networks\n","\n","Across layers:\n","\n","$$\n","\\text{Var}(x^{(l+1)}) = n^{(l)} \\text{Var}(w^{(l)}) \\text{Var}(x^{(l)})\n","$$\n","\n","Repeat this 50 times → explosion or collapse.\n","\n","---\n"]},{"cell_type":"markdown","id":"a43aab29","metadata":{"papermill":{"duration":0.002448,"end_time":"2026-02-05T06:17:24.726035","exception":false,"start_time":"2026-02-05T06:17:24.723587","status":"completed"},"tags":[]},"source":["# Xavier (Glorot) Initialization\n"]},{"cell_type":"markdown","id":"d11686fa","metadata":{"papermill":{"duration":0.002758,"end_time":"2026-02-05T06:17:24.731146","exception":false,"start_time":"2026-02-05T06:17:24.728388","status":"completed"},"tags":[]},"source":["## Core idea\n","\n","Xavier initialization is a weight initialization strategy designed to keep the **variance of activations and gradients approximately constant across layers** in deep neural networks.\n","\n","Its objective is to prevent:\n","- **Vanishing signals** (variance shrinking with depth)\n","- **Exploding signals** (variance growing with depth)\n","\n","during **both the forward and backward pass**.\n","\n","For a neuron:\n","$$\n","z = \\sum_{i=1}^{n_{in}} w_i x_i\n","$$\n","\n","Assuming:\n","- inputs $x_i$ are i.i.d. with zero mean and variance $\\text{Var}(x)$  \n","- weights $w_i$ are i.i.d. with zero mean and variance $\\text{Var}(w)$  \n","- weights and inputs are independent  \n","\n","the output variance becomes:\n","$$\n","\\text{Var}(z) = n_{in} \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","$$\n","\n","To keep signal magnitude stable across layers, we want:\n","$$\n","\\text{Var}(z) \\approx \\text{Var}(x)\n","$$\n","\n","This leads to:\n","$$\n","n_{in} \\cdot \\text{Var}(w) \\approx 1\n","$$\n","\n","\n","### Why $n_{out}$ also appears\n","\n","Backpropagation imposes a **similar constraint** on gradient variance, which depends on $n_{out}$ (fan-out).\n","\n","To balance **both forward and backward variance**, Xavier initialization chooses:\n","\n","$$\n","\\boxed{\n","\\text{Var}(w) = \\frac{2}{n_{in} + n_{out}}\n","}\n","$$\n","\n","where:\n","- $n_{in}$ → number of input connections (fan-in)  \n","- $n_{out}$ → number of output connections (fan-out)  \n","- $\\text{Var}(w)$ → variance of the weight distribution  \n","\n","This choice ensures:\n","- activations neither explode nor vanish in the forward pass  \n","- gradients remain well-scaled in the backward pass  \n","\n","### Practical forms\n","\n","Xavier initialization doesn’t set all weights to the same number.  \n","Instead, **each weight is chosen randomly** from a carefully controlled distribution to preserve variance.\n","\n","#### 1️. Uniform Xavier\n","\n","$$\n","w \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{in}+n_{out}}},\\;\n","\\sqrt{\\frac{6}{n_{in}+n_{out}}}\\right)\n","$$\n","\n","- Each weight is drawn **independently at random** from the interval  \n","  $$[-\\sqrt{6/(n_{in}+n_{out})}, \\sqrt{6/(n_{in}+n_{out})}]$$\n","- All values in this range are **equally likely**\n","- Example:  \n","  If $n_{in}=128$, $n_{out}=64$, then $\\sqrt{6/192} \\approx 0.176$  \n","  → $w \\in [-0.176, 0.176]$ randomly\n","\n","\n","#### 2️. Normal (Gaussian) Xavier\n","\n","$$\n","w \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in}+n_{out}}\\right)\n","$$\n","\n","- Each weight is drawn independently from a **bell-shaped curve**  \n","  centered at 0 with variance $2/(n_{in}+n_{out})$\n","- Most weights are near 0, rare large values\n","- Example:  \n","  With the same $n_{in}$ and $n_{out}$, $\\text{Var}(w) \\approx 0.0104$, $\\sigma \\approx 0.102$  \n","  → most weights lie roughly in $[-0.3, 0.3]$\n","\n","#### Why randomness matters\n","\n","- Prevents neurons from being identical (symmetry problem)  \n","- Breaks correlation while controlling scale for stable signal propagation  \n","\n","#### Why these ranges are chosen\n","\n","- Ensures forward and backward variance is approximately constant:  \n","$$\n","\\text{Var}(z^{(l)}) \\approx \\text{Var}(z^{(l-1)})\n","$$\n"]},{"cell_type":"markdown","id":"7b12fcdc","metadata":{"papermill":{"duration":0.00229,"end_time":"2026-02-05T06:17:24.735977","exception":false,"start_time":"2026-02-05T06:17:24.733687","status":"completed"},"tags":[]},"source":["## When Xavier Works Well\n","\n","Xavier initialization is most effective with **symmetric activation functions**:\n","\n","Suitable activations:\n","- `sigmoid`  \n","- `tanh`  \n","\n","Why it works:\n","- These activations produce outputs centered around zero  \n","- Symmetric output ensures variance propagation assumptions hold  \n","- Forward and backward signals remain stable across layers\n"]},{"cell_type":"markdown","id":"e0c143bd","metadata":{"papermill":{"duration":0.002619,"end_time":"2026-02-05T06:17:24.741128","exception":false,"start_time":"2026-02-05T06:17:24.738509","status":"completed"},"tags":[]},"source":["## Important Caveat\n","\n","CNNs almost always use **ReLU activations**, not `tanh`.\n","\n","Problem:\n","- ReLU sets all negative activations to zero  \n","- This halves the effective variance of the signal  \n","- Xavier initialization assumes symmetric activations, so it underestimates the needed variance\n","\n","Consequence:\n","> Learning is slow, unstable, or may fail to converge in deep ReLU networks\n","\n","For ReLU-based CNNs, use **He initialization** instead of Xavier\n"]},{"cell_type":"markdown","id":"18b85be6","metadata":{"papermill":{"duration":0.003062,"end_time":"2026-02-05T06:17:24.746741","exception":false,"start_time":"2026-02-05T06:17:24.743679","status":"completed"},"tags":[]},"source":["## Xavier Initialization in PyTorch\n","\n","```python\n","# Uniform Xavier\n","nn.init.xavier_uniform_(conv.weight)\n","\n","# Normal (Gaussian) Xavier\n","nn.init.xavier_normal_(conv.weight)\n","````\n","\n","Notes:\n","\n","* Each weight is randomly initialized using the Xavier formulas\n","* Works well for symmetric activations like `tanh` or `sigmoid`\n","* Keeps variance of activations and gradients roughly constant across layers"]},{"cell_type":"markdown","id":"0e7b09d5","metadata":{"papermill":{"duration":0.002293,"end_time":"2026-02-05T06:17:24.751481","exception":false,"start_time":"2026-02-05T06:17:24.749188","status":"completed"},"tags":[]},"source":["# He (Kaiming) Initialization\n"]},{"cell_type":"markdown","id":"7fbaa087","metadata":{"papermill":{"duration":0.002265,"end_time":"2026-02-05T06:17:24.756111","exception":false,"start_time":"2026-02-05T06:17:24.753846","status":"completed"},"tags":[]},"source":["To be continue..."]},{"cell_type":"markdown","id":"9aa32378","metadata":{"papermill":{"duration":0.002259,"end_time":"2026-02-05T06:17:24.760754","exception":false,"start_time":"2026-02-05T06:17:24.758495","status":"completed"},"tags":[]},"source":["---\n","\n","<p style=\"text-align:center; color:skyblue; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":3.975668,"end_time":"2026-02-05T06:17:25.183631","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-05T06:17:21.207963","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}