{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-35-cnn-weight-initialization?scriptVersionId=296486200\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"d055f5a5","metadata":{"papermill":{"duration":0.004835,"end_time":"2026-02-08T05:39:33.212993","exception":false,"start_time":"2026-02-08T05:39:33.208158","status":"completed"},"tags":[]},"source":["# Day 35: CNN Weight Initialization\n","*Xavier vs He · Dead ReLU · Empirical Behavior*\n","\n","Welocme to Day 35!\n","\n","Today is not about *choosing an initializer*.\n","\n","It’s about understanding:\n","- Why some CNNs never learn\n","- Why loss curves look “flat but not broken”\n","- Why optimizers get blamed unfairly\n","\n","By the end of today:\n","\n","✔ You’ll diagnose learning failure in minutes  \n","✔ You’ll pick initialization without guessing  \n","✔ You’ll recognize Dead ReLU instantly  \n","\n","If you found this notebook helpful, your **<b style=\"color:skyblue;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"c55143eb","metadata":{"papermill":{"duration":0.003321,"end_time":"2026-02-08T05:39:33.219762","exception":false,"start_time":"2026-02-08T05:39:33.216441","status":"completed"},"tags":[]},"source":["# Why Initialization Matters\n","\n","Training deep CNNs critically depends on **stable gradient flow**.\n","\n","Weight initialization directly controls:\n","- **Activation scale** during the forward pass\n","- **Gradient scale** during backpropagation\n","\n","### What goes wrong with poor initialization\n","- **Weights too small** → activations shrink layer by layer → gradients vanish  \n","- **Weights too large** → activations blow up → gradients explode  \n","\n","Neither case usually causes runtime errors.\n","\n","> Bad initialization doesn’t crash the model.  \n","> It silently prevents effective learning."]},{"cell_type":"markdown","id":"950512b6","metadata":{"papermill":{"duration":0.003241,"end_time":"2026-02-08T05:39:33.226194","exception":false,"start_time":"2026-02-08T05:39:33.222953","status":"completed"},"tags":[]},"source":["# What Failure Looks Like in Practice\n","\n","### Common symptoms\n","- **Training loss decreases extremely slowly or plateaus early**  \n","- **Accuracy remains near random guessing**  \n","- **Changing the optimizer has little to no effect**  \n","- **Lowering the learning rate worsens training**  \n","\n","Interpretation\n","> The model is mathematically incapable of propagating signal.\n","\n","Poor initialization causes activations or gradients to shrink or explode across layers, so learning signals never reach earlier layers. Optimization fails **silently**, even though the training loop appears normal.\n"]},{"cell_type":"markdown","id":"291506d1","metadata":{"papermill":{"duration":0.003153,"end_time":"2026-02-08T05:39:33.232569","exception":false,"start_time":"2026-02-08T05:39:33.229416","status":"completed"},"tags":[]},"source":["# The Core Problem: Variance Propagation\n","\n","Variance propagation describes how the statistical spread (variance) of activations or gradients changes as signals pass through successive layers of a neural network.\n","\n","Formally, for a layer:\n","$$\n","z^{(l)} = \\sum_{i=1}^{n} w_i^{(l)} x_i^{(l)}\n","$$\n","\n","the output variance depends on the input variance:\n","$$\n","\\boxed{\\text{Var}\\!\\left(z^{(l)}\\right)\n","= n \\cdot \\text{Var}\\!\\left(w^{(l)}\\right)\n","\\cdot \\text{Var}\\!\\left(x^{(l)}\\right)}\n","$$\n","\n","Consider a single neuron in a neural network:\n","\n","$$\n","z = \\sum_{i=1}^{n} w_i x_i\n","$$\n","\n","- $z$ → pre-activation output of the neuron (input to the non-linearity)  \n","- $n$ → number of input connections (fan-in)  \n","- $w_i$ → weight associated with the $i$-th input  \n","- $x_i$ → $i$-th input activation  \n","\n","This is a weighted sum of inputs before applying an activation function.\n","\n","\n","### Variance of the neuron output\n","\n","Assuming:\n","- inputs $x_i$ are independent and identically distributed  \n","- weights $w_i$ are independent of inputs  \n","- both have zero mean  \n","\n","the variance of $z$ becomes:\n","\n","$$\n","\\text{Var}(z) = n \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","$$\n","\n","- $\\text{Var}(z)$ → variance of the neuron’s output  \n","- $n$ → number of summed terms (fan-in)  \n","- $\\text{Var}(w)$ → variance of the weight distribution  \n","- $\\text{Var}(x)$ → variance of the input activations  \n","\n","### Why this matters in deep networks\n","\n","As signals propagate through layers:\n","\n","- If $\\text{Var}(z)$ **increases layer by layer** → activations and gradients explode  \n","- If $\\text{Var}(z)$ **decreases layer by layer** → activations and gradients vanish  \n","\n","Both cases make learning ineffective.\n","\n","### Key Insight\n","> Proper initialization chooses $\\text{Var}(w)$ such that  \n","> $\\text{Var}(z)$ remains **approximately constant across layers**.\n","\n","This is the mathematical foundation behind **Xavier and He initialization**."]},{"cell_type":"markdown","id":"9f31a3aa","metadata":{"papermill":{"duration":0.003055,"end_time":"2026-02-08T05:39:33.238749","exception":false,"start_time":"2026-02-08T05:39:33.235694","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\"> Where Does  Var(z)  Come From?</p>\n","\n","Start with the neuron equation:\n","\n","$$\n","z = \\sum_{i=1}^{n} w_i x_i\n","$$\n","\n","This is a sum of random variables.\n","\n","\n","**Step 1: Variance of a sum**\n","\n","A basic probability rule:\n","\n","If random variables are independent,\n","\n","$$\n","\\text{Var}\\!\\left(\\sum_{i=1}^{n} y_i\\right)\n","= \\sum_{i=1}^{n} \\text{Var}(y_i)\n","$$\n","\n","So we apply this to:\n","\n","$$\n","z = w_i x_i\n","$$\n","\n","Then:\n","\n","$$\n","\\text{Var}(z) = \\sum_{i=1}^{n} \\text{Var}(w_i x_i)\n","$$\n","\n","\n","**Step 2: Variance of a product**\n","\n","Another key rule (under independence and zero mean):\n","\n","$$\n","\\text{Var}(w_i x_i)\n","= \\text{Var}(w_i)\\,\\text{Var}(x_i)\n","$$\n","\n","Why this holds:\n","- $w_i$ and $x_i$ are independent  \n","- $\\mathbb{E}[w_i] = 0$, $\\mathbb{E}[x_i] = 0$  \n","\n","So each term contributes:\n","\n","$$\n","\\text{Var}(w_i x_i) = \\text{Var}(w)\\,\\text{Var}(x)\n","$$\n","\n","\n","**Step 3: Sum all contributions**\n","\n","Since every term has the same variance:\n","\n","$$\n","\\text{Var}(z)\n","= \\sum_{i=1}^{n} \\text{Var}(w)\\,\\text{Var}(x)\n","$$\n","\n","$$\n","\\boxed{\n","\\text{Var}(z) = n \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","}\n","$$\n","\n","\n","### What this means\n","\n","- Each input contributes **a little variance**\n","- Adding $n$ such contributions multiplies variance by $n$\n","\n","> More connections = more variance unless weights are scaled down\n","\n","\n","### Why this breaks deep networks\n","\n","Across layers:\n","\n","$$\n","\\text{Var}(x^{(l+1)}) = n^{(l)} \\text{Var}(w^{(l)}) \\text{Var}(x^{(l)})\n","$$\n","\n","Repeat this 50 times → explosion or collapse.\n","\n","---\n"]},{"cell_type":"markdown","id":"7f69662e","metadata":{"papermill":{"duration":0.003078,"end_time":"2026-02-08T05:39:33.24493","exception":false,"start_time":"2026-02-08T05:39:33.241852","status":"completed"},"tags":[]},"source":["# Xavier (Glorot) Initialization\n"]},{"cell_type":"markdown","id":"ded94b90","metadata":{"papermill":{"duration":0.002946,"end_time":"2026-02-08T05:39:33.250847","exception":false,"start_time":"2026-02-08T05:39:33.247901","status":"completed"},"tags":[]},"source":["## Core idea\n","\n","Xavier initialization is a weight initialization strategy designed to keep the **variance of activations and gradients approximately constant across layers** in deep neural networks.\n","\n","Its objective is to prevent:\n","- **Vanishing signals** (variance shrinking with depth)\n","- **Exploding signals** (variance growing with depth)\n","\n","during **both the forward and backward pass**.\n","\n","For a neuron:\n","$$\n","z = \\sum_{i=1}^{n_{in}} w_i x_i\n","$$\n","\n","Assuming:\n","- inputs $x_i$ are i.i.d. with zero mean and variance $\\text{Var}(x)$  \n","- weights $w_i$ are i.i.d. with zero mean and variance $\\text{Var}(w)$  \n","- weights and inputs are independent  \n","\n","the output variance becomes:\n","$$\n","\\text{Var}(z) = n_{in} \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","$$\n","\n","To keep signal magnitude stable across layers, we want:\n","$$\n","\\text{Var}(z) \\approx \\text{Var}(x)\n","$$\n","\n","This leads to:\n","$$\n","n_{in} \\cdot \\text{Var}(w) \\approx 1\n","$$\n","\n","\n","### Why $n_{out}$ also appears\n","\n","Backpropagation imposes a **similar constraint** on gradient variance, which depends on $n_{out}$ (fan-out).\n","\n","To balance **both forward and backward variance**, Xavier initialization chooses:\n","\n","$$\n","\\boxed{\n","\\text{Var}(w) = \\frac{2}{n_{in} + n_{out}}\n","}\n","$$\n","\n","where:\n","- $n_{in}$ → number of input connections (fan-in)  \n","- $n_{out}$ → number of output connections (fan-out)  \n","- $\\text{Var}(w)$ → variance of the weight distribution  \n","\n","This choice ensures:\n","- activations neither explode nor vanish in the forward pass  \n","- gradients remain well-scaled in the backward pass  \n","\n","### Practical forms\n","\n","Xavier initialization doesn’t set all weights to the same number.  \n","Instead, **each weight is chosen randomly** from a carefully controlled distribution to preserve variance.\n","\n","#### 1️. Uniform Xavier\n","\n","$$\n","w \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{in}+n_{out}}},\\;\n","\\sqrt{\\frac{6}{n_{in}+n_{out}}}\\right)\n","$$\n","\n","- Each weight is drawn **independently at random** from the interval  \n","  $$[-\\sqrt{6/(n_{in}+n_{out})}, \\sqrt{6/(n_{in}+n_{out})}]$$\n","- All values in this range are **equally likely**\n","- Example:  \n","  If $n_{in}=128$, $n_{out}=64$, then $\\sqrt{6/192} \\approx 0.176$  \n","  → $w \\in [-0.176, 0.176]$ randomly\n","\n","\n","#### 2️. Normal (Gaussian) Xavier\n","\n","$$\n","w \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in}+n_{out}}\\right)\n","$$\n","\n","- Each weight is drawn independently from a **bell-shaped curve**  \n","  centered at 0 with variance $2/(n_{in}+n_{out})$\n","- Most weights are near 0, rare large values\n","- Example:  \n","  With the same $n_{in}$ and $n_{out}$, $\\text{Var}(w) \\approx 0.0104$, $\\sigma \\approx 0.102$  \n","  → most weights lie roughly in $[-0.3, 0.3]$\n","\n","#### Why randomness matters\n","\n","- Prevents neurons from being identical (symmetry problem)  \n","- Breaks correlation while controlling scale for stable signal propagation  \n","\n","#### Why these ranges are chosen\n","\n","- Ensures forward and backward variance is approximately constant:  \n","$$\n","\\text{Var}(z^{(l)}) \\approx \\text{Var}(z^{(l-1)})\n","$$\n"]},{"cell_type":"markdown","id":"5d08e11f","metadata":{"papermill":{"duration":0.003042,"end_time":"2026-02-08T05:39:33.257035","exception":false,"start_time":"2026-02-08T05:39:33.253993","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\">Understanding the Math Behind Xavier Initialization</p>\n","\n","Xavier is designed for symmetric activations (tanh, sigmoid), assuming no variance is lost after the activation.\n","\n","### 1. Xavier Uniform\n","\n","Rule:\n","$$\n","w \\sim \\mathcal{U}\\Big(-\\sqrt{\\frac{6}{n_{in}+n_{out}}},\\; \\sqrt{\\frac{6}{n_{in}+n_{out}}}\\Big)\n","$$\n","\n","Step-by-step:\n","\n","1. Variance of uniform distribution $U[a,b]$:\n","\n","\n","$$\n","\\text{Var}(U[a,b]) = \\frac{(b-a)^2}{12}\n","$$\n","\n","3. Plug in bounds: $a=-r, b=r$:\n","\n","$$\n","\\text{Var}(U[-r,r]) = \\frac{(r - (-r))^2}{12} = \\frac{4 r^2}{12} = \\frac{r^2}{3}\n","$$\n","\n","3. Set this equal to desired Xavier variance:\n","$$\n","\\frac{r^2}{3} = \\frac{2}{n_{in}+n_{out}} \\quad \\Rightarrow \\quad \\boxed{r = \\sqrt{\\frac{6}{n_{in}+n_{out}}}}\n","$$\n","\n","> Each weight is sampled **uniformly in [-r, r]**, producing the correct variance.\n","\n","\n","### 2. Xavier Normal\n","\n","Rule:\n","$$\n","w \\sim \\mathcal{N}\\left(0,\\; \\frac{2}{n_{in}+n_{out}}\\right)\n","$$\n","\n","Step-by-step:\n","\n","1. **Neuron output**:\n","$$\n","z = \\sum_{i=1}^{n_{in}} w_i x_i\n","$$\n","\n","2. **Variance of output**:\n","$$\n","\\text{Var}(z) = n_{in} \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","$$\n","\n","3. **Forward + backward balance**:  \n","\n","We want both forward and backward variance to remain stable.  \n","- Forward: $n_{in} \\cdot \\text{Var}(w) \\approx 1$  \n","- Backward: $n_{out} \\cdot \\text{Var}(w) \\approx 1$\n","\n","4. **Compromise formula**:\n","\n","We take the average (harmonic mean) of the two constraints:\n","\n","$$\n","\\text{Var}(w) = \\frac{1}{2}\\Big(\\frac{1}{n_{in}} + \\frac{1}{n_{out}}\\Big)^{-1} \\quad \\approx \\frac{2}{n_{in}+n_{out}}\n","$$\n","\n","$$\n","\\boxed{\\text{Var}(w) = \\frac{2}{n_{in}+n_{out}}}\n","$$\n","- That’s why 2 appears in the numerator  \n","- It balances forward and backward variance in one formula\n","\n","> Each weight is sampled from a Gaussian with mean 0 and variance $\\frac{2}{n_{in}+n_{out}}$\n","\n","\n","**Key Intuition**\n","\n","- Xavier Normal → Gaussian  \n","- Xavier Uniform → Uniform  \n","- Both achieve the **same target variance**  \n","- Suitable for **symmetric activations** (tanh, sigmoid)  \n","- Not ideal for ReLU (use He instead)\n","\n","---"]},{"cell_type":"markdown","id":"c61b7dab","metadata":{"papermill":{"duration":0.003079,"end_time":"2026-02-08T05:39:33.263319","exception":false,"start_time":"2026-02-08T05:39:33.26024","status":"completed"},"tags":[]},"source":["## When Xavier Works Well\n","\n","Xavier initialization is most effective with **symmetric activation functions**:\n","\n","Suitable activations:\n","- `sigmoid`  \n","- `tanh`  \n","\n","Why it works:\n","- These activations produce outputs centered around zero  \n","- Symmetric output ensures variance propagation assumptions hold  \n","- Forward and backward signals remain stable across layers\n"]},{"cell_type":"markdown","id":"44310c12","metadata":{"papermill":{"duration":0.003096,"end_time":"2026-02-08T05:39:33.269463","exception":false,"start_time":"2026-02-08T05:39:33.266367","status":"completed"},"tags":[]},"source":["## Important Caveat\n","\n","CNNs almost always use **ReLU activations**, not `tanh`.\n","\n","Problem:\n","- ReLU sets all negative activations to zero  \n","- This halves the effective variance of the signal  \n","- Xavier initialization assumes symmetric activations, so it underestimates the needed variance\n","\n","Consequence:\n","> Learning is slow, unstable, or may fail to converge in deep ReLU networks\n","\n","For ReLU-based CNNs, use **He initialization** instead of Xavier\n"]},{"cell_type":"markdown","id":"b3b93a67","metadata":{"papermill":{"duration":0.002984,"end_time":"2026-02-08T05:39:33.275566","exception":false,"start_time":"2026-02-08T05:39:33.272582","status":"completed"},"tags":[]},"source":["## Xavier Initialization in PyTorch\n","\n","```python\n","# Uniform Xavier\n","nn.init.xavier_uniform_(conv.weight)\n","\n","# Normal (Gaussian) Xavier\n","nn.init.xavier_normal_(conv.weight)\n","```\n","\n","Notes:\n","\n","* Each weight is randomly initialized using the Xavier formulas\n","* Works well for symmetric activations like `tanh` or `sigmoid`\n","* Keeps variance of activations and gradients roughly constant across layers"]},{"cell_type":"markdown","id":"7b50176d","metadata":{"papermill":{"duration":0.003051,"end_time":"2026-02-08T05:39:33.28175","exception":false,"start_time":"2026-02-08T05:39:33.278699","status":"completed"},"tags":[]},"source":["# He (Kaiming) Initialization\n"]},{"cell_type":"markdown","id":"2bb646ee","metadata":{"papermill":{"duration":0.003025,"end_time":"2026-02-08T05:39:33.288598","exception":false,"start_time":"2026-02-08T05:39:33.285573","status":"completed"},"tags":[]},"source":["## Core Idea\n","\n","**He Initialization** (also called Kaiming Initialization) is a weight initialization method designed specifically for ReLU-based neural networks, where weights are initialized so that activation variance remains approximately constant across layers, despite ReLU zeroing out half of the inputs.\n","\n","Formally, it sets the variance of weights as:\n","$$\n","\\text{Var}(w) = \\frac{2}{n_{in}}\n","$$\n","\n","**Why He Initialization Exists**\n","\n","ReLU behaves as:\n","$$\n","\\text{ReLU}(x) = \\max(0, x)\n","$$\n","\n","This causes:\n","- ~50% of activations → exactly zero\n","- Output variance → reduced by ~½ after each ReLU\n","\n","Without correction:\n","- Activations shrink layer by layer\n","- Gradients weaken\n","- Deep CNNs fail to train effectively\n","\n","He initialization explicitly compensates for this variance loss.\n","\n","\n","For a neuron:\n","$$\n","z = \\sum_{i=1}^{n_{in}} w_i x_i\n","$$\n","\n","If:\n","- Inputs $x_i$ have variance 1\n","- Weights are zero-mean and independent\n","\n","Then:\n","$$\n","\\text{Var}(z) = n_{in} \\cdot \\text{Var}(w)\n","$$\n","\n","After ReLU:\n","$$\n","\\text{Var}(\\text{ReLU}(z)) \\approx \\frac{1}{2}\\text{Var}(z)\n","$$\n","\n","To keep variance ≈ 1:\n","$$\n","\\frac{1}{2} \\cdot n_{in} \\cdot \\text{Var}(w) = 1\n","\\Rightarrow \\boxed{\\text{Var}(w) = \\frac{2}{n_{in}}}\n","$$\n","\n","\n","Practical Meaning of $n_{in}$:\n","\n","- **Fully Connected layer**: number of input features  \n","- **Convolution layer**:\n","\n","$$\n","n_{in} = k_h \\times k_w \\times c_{in}\n","$$\n","\n"]},{"cell_type":"markdown","id":"870db47b","metadata":{"papermill":{"duration":0.003064,"end_time":"2026-02-08T05:39:33.294731","exception":false,"start_time":"2026-02-08T05:39:33.291667","status":"completed"},"tags":[]},"source":["## Does He Initialization Use Random or Fixed Values?\n"," \n","**Random values, not fixed constants.**  \n","He initialization defines how weights are randomly sampled, not a single value.\n","\n","### What He Initialization Actually Does\n","\n","He initialization controls the distribution of weights:\n","\n","- Mean = 0  \n","- Variance = $\\dfrac{2}{n_{in}}$\n","\n","Each weight is random, but its scale is carefully chosen so that signal does not vanish after ReLU.\n","\n","\n","### Common He Initialization Variants\n","\n","#### 1️. He Normal (Most Common)\n","Weights are sampled from a normal distribution:\n","$$\n","w \\sim \\mathcal{N}\\left(0,\\; \\frac{2}{n_{in}}\\right)\n","$$\n","\n","Interpretation:\n","- Bell-shaped distribution\n","- Most values are small\n","- Occasional larger values allowed\n","\n","\n","#### 2️. He Uniform\n","Weights are sampled from a uniform distribution:\n","$$\n","w \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{in}}},\\; \\sqrt{\\frac{6}{n_{in}}}\\right)\n","$$\n","\n","Interpretation:\n","- Every value in the range is equally likely\n","- Strict upper and lower bounds\n","\n","### Critical Clarification\n","\n","| Question | Answer |\n","|--------|-------|\n","Are weights fixed? | No |\n","Are all weights identical? | No |\n","Are weights random each run? | Yes |\n","Is randomness controlled? | Yes |\n","\n","If weights were fixed:\n","- Neurons would learn identical features\n","- Training would completely fail\n","\n","Randomness is required.  \n","He initialization makes it mathematically stable.\n","\n","\n","> **He initialization = random weights scaled correctly so ReLU doesn’t destroy gradient flow.**\n"]},{"cell_type":"markdown","id":"5386bca9","metadata":{"papermill":{"duration":0.003068,"end_time":"2026-02-08T05:39:33.300976","exception":false,"start_time":"2026-02-08T05:39:33.297908","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\">Understanding the Math Behind He Initialization</p>\n","\n","#### 1️. He Normal\n","\n","Rule:\n","$$\n","w \\sim \\mathcal{N}\\left(0,\\; \\frac{2}{n_{in}}\\right)\n","$$\n","\n","Step-by-step:\n","\n","1. **Neuron output**:\n","$$\n","z = \\sum_{i=1}^{n_{in}} w_i x_i\n","$$\n","\n","2. **Variance of output** (before ReLU):\n","$$\n","\\text{Var}(z) = n_{in} \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","$$\n","\n","3. **Effect of ReLU**:\n","- ReLU zeros out ~50% of inputs  \n","- Variance after ReLU:\n","$$\n","\\text{Var}(\\text{ReLU}(z)) \\approx \\frac{1}{2} \\text{Var}(z)\n","$$\n","\n","4. **To keep signal variance stable**, want:\n","$$\n","\\text{Var}(\\text{ReLU}(z)) \\approx \\text{Var}(x)\n","$$\n","\n","5. **Solve for Var(w)**:\n","$$\n","\\frac{1}{2} \\cdot n_{in} \\cdot \\text{Var}(w) = 1\n","\\quad \\Rightarrow \\quad\n","\\boxed{\\text{Var}(w) = \\frac{2}{n_{in}}}\n","$$\n","\n","> Each weight is then sampled from a Gaussian with mean 0 and variance $2/n_{in}$.\n","\n","\n","### 2️. He Uniform\n","\n","Rule:\n","$$\n","w \\sim \\mathcal{U}\\Big(-\\sqrt{\\frac{6}{n_{in}}},\\; \\sqrt{\\frac{6}{n_{in}}}\\Big)\n","$$\n","\n","Step-by-step:\n","\n","1. Variance of uniform distribution $U[a,b]$:\n","$$\n","\\text{Var}(U[a,b]) = \\frac{(b-a)^2}{12}\n","$$\n","\n","2. Plug in bounds: $a=-r, b=r$  \n","$$\n","\\text{Var}(w) = \\frac{(r - (-r))^2}{12} = \\frac{(2r)^2}{12} = \\frac{4 r^2}{12} = \\frac{r^2}{3}\n","$$\n","\n","3. Set this equal to desired He variance $2/n_{in}$:\n","$$\n","\\frac{r^2}{3} = \\frac{2}{n_{in}} \\quad \\Rightarrow \\quad r^2 = \\frac{6}{n_{in}} \\quad \\Rightarrow \\quad \\boxed{r = \\sqrt{\\frac{6}{n_{in}}}}\n","$$\n","\n","> Each weight is then sampled uniformly in $[-r, r]$, which produces the same target variance as He Normal.\n","\n","\n","**Key Intuition**\n","\n","- Both Normal and Uniform produce random weights with variance 2/n_in \n","- Randomness breaks symmetry  \n","- Variance scaling compensates for ReLU killing half the signal\n","\n","---"]},{"cell_type":"markdown","id":"643b98ff","metadata":{"papermill":{"duration":0.002977,"end_time":"2026-02-08T05:39:33.307042","exception":false,"start_time":"2026-02-08T05:39:33.304065","status":"completed"},"tags":[]},"source":["## Why He Initialization Is Optimal for CNNs\n","\n","CNN Characteristics\n","- Very deep stacks of convolutional layers  \n","- ReLU activations in almost every layer  \n","- High fan-in (many input connections per neuron/filter)\n","\n","Why He Works\n","- **Preserves forward signal** → prevents activations from shrinking  \n","- **Preserves backward gradients** → avoids vanishing/exploding gradients  \n","- **Enables learning immediately** → no slow “warm-up” as with Xavier + ReLU\n","\n","> Bottom line: For deep, ReLU-based CNNs, **He initialization is almost always the default choice**.\n"]},{"cell_type":"markdown","id":"6939a100","metadata":{"papermill":{"duration":0.003115,"end_time":"2026-02-08T05:39:33.313306","exception":false,"start_time":"2026-02-08T05:39:33.310191","status":"completed"},"tags":[]},"source":["## He Initialization in PyTorch\n","\n","```python\n","nn.init.kaiming_normal_(\n","    conv.weight,\n","    mode=\"fan_in\",\n","    nonlinearity=\"relu\"\n",")\n","```\n","\n","or\n","\n","```python\n","nn.init.kaiming_uniform_(\n","    conv.weight,\n","    nonlinearity=\"relu\"\n",")\n","```\n","\n","**Default choice for CNNs**\n"]},{"cell_type":"markdown","id":"3d72589c","metadata":{"papermill":{"duration":0.003036,"end_time":"2026-02-08T05:39:33.319372","exception":false,"start_time":"2026-02-08T05:39:33.316336","status":"completed"},"tags":[]},"source":["# CNN-Specific Detail: `fan_in` in Conv Layers\n","\n","For a `Conv2D` layer:\n","\n","$$\n","\\text{fan\\_in} = C_{in} \\times K_h \\times K_w\n","$$\n","\n","Where:  \n","- $C_{in}$ → number of input channels  \n","- $K_h, K_w$ → kernel height and width\n","\n","Notes:\n","\n","- PyTorch automatically computes `fan_in` when you use `nn.init.kaiming_*` or `nn.init.xavier_*`  \n","- If you manually initialize weights, you **must respect fan_in**, otherwise:\n","  - Forward activations explode or vanish  \n","  - Backward gradients explode or vanish  \n","  - Variance assumptions in Xavier/He formulas break  \n","\n","> Correct fan_in calculation is **critical for deep CNN stability**\n"]},{"cell_type":"markdown","id":"29674a78","metadata":{"papermill":{"duration":0.003102,"end_time":"2026-02-08T05:39:33.325667","exception":false,"start_time":"2026-02-08T05:39:33.322565","status":"completed"},"tags":[]},"source":["# Why fan_in Matters in Convolutional Layers\n","\n","### What fan_in represents\n","- fan_in = number of inputs contributing to a single neuron/output unit\n","- For Conv2D: \n","$$\n","\\text{fan\\_in} = C_{in} \\times K_h \\times K_w\n","$$\n","- Each output pixel is computed as:\n","$$\n","z = \\sum_{c=1}^{C_{in}} \\sum_{i=1}^{K_h} \\sum_{j=1}^{K_w} w_{c,i,j} \\cdot x_{c,i,j}\n","$$\n","- So `fan_in` counts all weight-input products summed for one output pixel\n","\n","\n","### Why it affects weight initialization\n","Variance of neuron output:\n","$$\n","\\text{Var}(z) = \\text{fan\\_in} \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","$$\n","\n","- Larger fan_in → more summed terms → output variance **increases**  \n","- Smaller fan_in → output variance **decreases**  \n","\n","**Xavier/He formulas assume you know fan_in** to set $\\text{Var}(w)$ correctly:\n","- Too small variance → activations shrink → slow learning  \n","- Too large variance → activations explode → unstable gradients\n","\n","\n","### PyTorch convenience\n","- Functions like `nn.init.kaiming_normal_` automatically compute fan_in for Conv layers  \n","- If you manually compute weights, you must use the correct fan_in for the\n","\n","formula:\n","$$\n","\\text{Var}(w) = \\frac{2}{\\text{fan\\_in}} \\quad \\text{(He for ReLU)}\n","$$"]},{"cell_type":"markdown","id":"1f161559","metadata":{"papermill":{"duration":0.003183,"end_time":"2026-02-08T05:39:33.332019","exception":false,"start_time":"2026-02-08T05:39:33.328836","status":"completed"},"tags":[]},"source":["# Why `mode=\"fan_in\"` in `nn.init.kaiming_normal_` Even Though PyTorch Calculates `fan_in`\n","\n","### What `mode` does\n","- `mode` tells PyTorch how to scale the variance of the weights:\n","  - `\"fan_in\"` → scales weights based on **number of input connections**  \n","  - `\"fan_out\"` → scales weights based on **number of output connections**  \n","  - `\"fan_avg\"` → uses average of `fan_in` and `fan_out`  \n","\n","- Scaling formula (He initialization):\n","\n","$$\n","\\text{Var}(w) = \\frac{2}{\\text{fan\\_in}} \\quad \\text{(for ReLU)}\n","$$\n","\n","### Automatic calculation vs scaling decision\n","- PyTorch does compute fan_in automatically for Conv/Linear layers  \n","- But it needs your guidance on which one to use for scaling via `mode`  \n","- `\"fan_in\"` is the default and correct for ReLU, because variance of outputs depends on inputs summed \n","\n","> If you chose `\"fan_out\"` instead:\n","> - Forward activations variance could shrink/explode  \n","> - Backprop gradients could become unstable"]},{"cell_type":"markdown","id":"b0fc8876","metadata":{"papermill":{"duration":0.003083,"end_time":"2026-02-08T05:39:33.338312","exception":false,"start_time":"2026-02-08T05:39:33.335229","status":"completed"},"tags":[]},"source":["# Dead ReLU: Silent Model Death\n"]},{"cell_type":"markdown","id":"68281b09","metadata":{"papermill":{"duration":0.003057,"end_time":"2026-02-08T05:39:33.344456","exception":false,"start_time":"2026-02-08T05:39:33.341399","status":"completed"},"tags":[]},"source":["## What Is a Dead ReLU?\n","\n","A ReLU neuron is considered **dead** when it gets stuck outputting **only zero**, forever.\n","\n","This happens if:\n","- Pre-activation $z < 0$ for all inputs\n","- ReLU outputs: $ \\text{ReLU}(z) = 0 $\n","- Gradient through ReLU: $ \\frac{\\partial \\text{ReLU}}{\\partial z} = 0 $\n","\n","Consequences\n","- Output is **always zero**\n","- Gradient is **always zero**\n","- Weights **never update again**\n","\n","> Once a ReLU is dead, it cannot recover through gradient descent.  \n","> This is why bad initialization or too large learning rates are dangerous in CNNs."]},{"cell_type":"markdown","id":"93537ea8","metadata":{"papermill":{"duration":0.003028,"end_time":"2026-02-08T05:39:33.350632","exception":false,"start_time":"2026-02-08T05:39:33.347604","status":"completed"},"tags":[]},"source":["## Why Dead ReLU Happens\n","\n","A ReLU neuron becomes dead when its **pre-activation is pushed permanently negative**.\n","\n","Main Causes:\n","\n","- **Poor initialization**\n","  - Large negative bias shifts activations below zero\n","- **Excessive learning rate**\n","  - One bad update can push weights into a permanently negative regime\n","- **Deep ReLU stacks without variance control**\n","  - Activations shrink or drift negative layer by layer\n","\n","After It Happens:\n","\n","- ReLU outputs remain **exactly zero**\n","- Gradients through the neuron are **zero**\n","- Weights **stop updating**\n","\n","> Once dead, the neuron contributes nothing to the network forever, wasted capacity.\n"]},{"cell_type":"markdown","id":"01ef0df7","metadata":{"papermill":{"duration":0.003061,"end_time":"2026-02-08T05:39:33.356769","exception":false,"start_time":"2026-02-08T05:39:33.353708","status":"completed"},"tags":[]},"source":["## Mathematical Reality of Dead ReLU\n","\n","For the ReLU activation:\n","$$\n","\\frac{d}{dx}\\,\\text{ReLU}(x)=\n","\\begin{cases}\n","1 & \\text{if } x>0 \\\\\n","0 & \\text{if } x\\le 0\n","\\end{cases}\n","$$\n","\n","What this means:\n","\n","- If the pre-activation $x$ is **positive**, gradients flow normally\n","- If $x \\le 0$, the gradient is **exactly zero**\n","\n","Dead ReLU condition:\n","\n","If a neuron’s pre-activation satisfies:\n","$$\n","x \\le 0 \\quad \\text{for all inputs}\n","$$\n","\n","Then:\n","- Gradient = **0**\n","- Weight updates = **0**\n","- Learning for that neuron **halts permanently**\n","\n","> This is not a numerical issue, it is a hard mathematical cutoff.  \n","> Once stuck in the zero-gradient region, gradient descent has no signal to recover."]},{"cell_type":"markdown","id":"34cbe6c6","metadata":{"papermill":{"duration":0.00311,"end_time":"2026-02-08T05:39:33.363026","exception":false,"start_time":"2026-02-08T05:39:33.359916","status":"completed"},"tags":[]},"source":["## How He Initialization Reduces Dead ReLU\n","\n","He initialization is specifically designed for ReLU-based networks.\n","\n","What He Initialization Does:\n","\n","- **Increases activation variance**  \n","  - Compensates for ReLU zeroing ~50% of activations\n","- **Reduces probability of all-negative pre-activations**\n","  - Neurons are more likely to see positive inputs early\n","- **Keeps neurons alive during early training**\n","  - Gradients can flow from the very first steps\n","\n","Important Reality Check:\n","\n","- He initialization does not eliminate dead ReLUs\n","- It minimizes the risk, especially in deep CNNs\n","- Combined with reasonable learning rates and BatchNorm, it is industry best practice\n","\n","> He initialization doesn’t guarantee survival, it gives ReLU neurons the best statistical chance to stay alive.\n"]},{"cell_type":"markdown","id":"01c5914e","metadata":{"papermill":{"duration":0.003024,"end_time":"2026-02-08T05:39:33.369235","exception":false,"start_time":"2026-02-08T05:39:33.366211","status":"completed"},"tags":[]},"source":["# Empirical Comparison: What You’d Observe\n"]},{"cell_type":"markdown","id":"76ca9218","metadata":{"papermill":{"duration":0.003035,"end_time":"2026-02-08T05:39:33.375423","exception":false,"start_time":"2026-02-08T05:39:33.372388","status":"completed"},"tags":[]},"source":["## Experimental Setup (Conceptual)\n","\n","To isolate the effect of weight initialization, we keep everything else identical.\n","\n","Fixed Components:\n","\n","- Same **CNN architecture**\n","- Same **training dataset**\n","- Same **optimizer and learning rate**\n","- Same **batch size and training schedule**\n","\n","Single Variable Changed:\n","\n","- **Weight initialization method** only  \n","  (e.g., Random / Xavier / He)\n","\n","Purpose:\n","\n","- Ensure any difference in:\n","  - convergence speed\n","  - training stability\n","  - final accuracy  \n","\n","is caused only by initialization, not confounding factors.\n","\n","> This controlled setup reveals how initialization alone can make or break learning in deep CNNs.\n"]},{"cell_type":"markdown","id":"2dd7a71d","metadata":{"papermill":{"duration":0.00314,"end_time":"2026-02-08T05:39:33.381708","exception":false,"start_time":"2026-02-08T05:39:33.378568","status":"completed"},"tags":[]},"source":["## Expected Behavior\n","\n","When everything except weight initialization is held constant, the following patterns consistently appear:\n","\n","| Initialization | Training Speed | Stability | Final Accuracy |\n","|---------------|----------------|-----------|----------------|\n","| Random Normal | Very slow | Unstable | Low |\n","| Xavier        | Medium    | Stable    | Medium |\n","| He            | Fast      | Stable    | High |\n","\n","Interpretation:\n","- **Random Normal**\n","  - Poor variance control\n","  - Gradients vanish or explode\n","- **Xavier**\n","  - Balanced variance\n","  - Works, but underpowered for ReLU\n","- **He**\n","  - Variance matched to ReLU behavior\n","  - Learning starts immediately and scales deep\n","\n","> In modern CNNs with ReLU, He initialization is the practical default, not a theoretical luxury.\n"]},{"cell_type":"markdown","id":"f6cc879d","metadata":{"papermill":{"duration":0.003076,"end_time":"2026-02-08T05:39:33.388019","exception":false,"start_time":"2026-02-08T05:39:33.384943","status":"completed"},"tags":[]},"source":["## Why These Differences Appear\n","\n","The observed behavior comes directly from variance propagation and ReLU effects:\n","\n","- **He Initialization**  \n","  - Sets weights so that forward activations and backward gradients have the right scale for ReLU  \n","  - Signal flows immediately → fast, stable learning\n","\n","- **Xavier Initialization**  \n","  - Assumes symmetric activations (tanh/sigmoid)  \n","  - Underestimates variance for ReLU → weaker signals → slower convergence\n","\n","- **Random Initialization**  \n","  - No variance control → activations either explode or vanish  \n","  - Gradients become chaotic → training fails\n","\n","\n","> Weight initialization literally decides whether optimization has a chance to succeed.  \n","> Bad initialization = slow learning, dead neurons, or complete training collapse.\n"]},{"cell_type":"markdown","id":"be86119f","metadata":{"papermill":{"duration":0.003205,"end_time":"2026-02-08T05:39:33.394395","exception":false,"start_time":"2026-02-08T05:39:33.39119","status":"completed"},"tags":[]},"source":["# Production Rules You Actually Use\n"]},{"cell_type":"markdown","id":"48a5eb57","metadata":{"papermill":{"duration":0.003143,"end_time":"2026-02-08T05:39:33.400817","exception":false,"start_time":"2026-02-08T05:39:33.397674","status":"completed"},"tags":[]},"source":["## CNN Initialization Rules\n","\n","- **ReLU / Leaky ReLU** → use **He initialization**  \n","- **Tanh / Sigmoid** → use **Xavier initialization**  \n","- **Deep CNNs** → avoid random/unscaled initialization  \n","- **Training stalls early** → always **check initialization first**  \n","\n","> Correct initialization is the first line of defense against dead neurons, vanishing/exploding gradients, and slow learning.\n"]},{"cell_type":"markdown","id":"c21864b2","metadata":{"papermill":{"duration":0.003074,"end_time":"2026-02-08T05:39:33.406993","exception":false,"start_time":"2026-02-08T05:39:33.403919","status":"completed"},"tags":[]},"source":["## Common Beginner Mistakes\n","\n","- Treating initialization as optional\n","- Using Xavier with ReLU CNNs\n","- Blaming optimizer prematurely\n","- Over-tuning learning rate instead\n"]},{"cell_type":"markdown","id":"06cf2723","metadata":{"papermill":{"duration":0.003088,"end_time":"2026-02-08T05:39:33.413345","exception":false,"start_time":"2026-02-08T05:39:33.410257","status":"completed"},"tags":[]},"source":["# Key Takeaways from Day 35\n","\n","- Initialization controls gradient flow\n","- Bad init causes silent failure\n","- Xavier is not universal\n","- He is default for ReLU CNNs\n","- Dead ReLU is permanent damage\n","\n",">Most people debug loss curves.<br>\n",">Professionals debug variance propagation."]},{"cell_type":"markdown","id":"1daa6a7b","metadata":{"papermill":{"duration":0.003067,"end_time":"2026-02-08T05:39:33.419413","exception":false,"start_time":"2026-02-08T05:39:33.416346","status":"completed"},"tags":[]},"source":["---\n","\n","<p style=\"text-align:center; color:skyblue; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":6.510803,"end_time":"2026-02-08T05:39:36.590296","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-08T05:39:30.079493","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}