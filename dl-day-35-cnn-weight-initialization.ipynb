{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-35-cnn-weight-initialization?scriptVersionId=296345802\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"6f31e036","metadata":{"papermill":{"duration":0.004695,"end_time":"2026-02-07T08:12:20.159598","exception":false,"start_time":"2026-02-07T08:12:20.154903","status":"completed"},"tags":[]},"source":["# Day 35: CNN Weight Initialization\n","*Xavier vs He · Dead ReLU · Empirical Behavior*\n","\n","Welocme to Day 35!\n","\n","Today is not about *choosing an initializer*.\n","\n","It’s about understanding:\n","- Why some CNNs never learn\n","- Why loss curves look “flat but not broken”\n","- Why optimizers get blamed unfairly\n","\n","By the end of today:\n","\n","✔ You’ll diagnose learning failure in minutes  \n","✔ You’ll pick initialization without guessing  \n","✔ You’ll recognize Dead ReLU instantly  \n","\n","If you found this notebook helpful, your **<b style=\"color:skyblue;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"3179a80e","metadata":{"papermill":{"duration":0.003816,"end_time":"2026-02-07T08:12:20.167382","exception":false,"start_time":"2026-02-07T08:12:20.163566","status":"completed"},"tags":[]},"source":["# Why Initialization Matters\n","\n","Training deep CNNs critically depends on **stable gradient flow**.\n","\n","Weight initialization directly controls:\n","- **Activation scale** during the forward pass\n","- **Gradient scale** during backpropagation\n","\n","### What goes wrong with poor initialization\n","- **Weights too small** → activations shrink layer by layer → gradients vanish  \n","- **Weights too large** → activations blow up → gradients explode  \n","\n","Neither case usually causes runtime errors.\n","\n","> Bad initialization doesn’t crash the model.  \n","> It silently prevents effective learning."]},{"cell_type":"markdown","id":"fe48a681","metadata":{"papermill":{"duration":0.003705,"end_time":"2026-02-07T08:12:20.174971","exception":false,"start_time":"2026-02-07T08:12:20.171266","status":"completed"},"tags":[]},"source":["# What Failure Looks Like in Practice\n","\n","### Common symptoms\n","- **Training loss decreases extremely slowly or plateaus early**  \n","- **Accuracy remains near random guessing**  \n","- **Changing the optimizer has little to no effect**  \n","- **Lowering the learning rate worsens training**  \n","\n","Interpretation\n","> The model is mathematically incapable of propagating signal.\n","\n","Poor initialization causes activations or gradients to shrink or explode across layers, so learning signals never reach earlier layers. Optimization fails **silently**, even though the training loop appears normal.\n"]},{"cell_type":"markdown","id":"1da79713","metadata":{"papermill":{"duration":0.003805,"end_time":"2026-02-07T08:12:20.182487","exception":false,"start_time":"2026-02-07T08:12:20.178682","status":"completed"},"tags":[]},"source":["# The Core Problem: Variance Propagation\n","\n","Variance propagation describes how the statistical spread (variance) of activations or gradients changes as signals pass through successive layers of a neural network.\n","\n","Formally, for a layer:\n","$$\n","z^{(l)} = \\sum_{i=1}^{n} w_i^{(l)} x_i^{(l)}\n","$$\n","\n","the output variance depends on the input variance:\n","$$\n","\\boxed{\\text{Var}\\!\\left(z^{(l)}\\right)\n","= n \\cdot \\text{Var}\\!\\left(w^{(l)}\\right)\n","\\cdot \\text{Var}\\!\\left(x^{(l)}\\right)}\n","$$\n","\n","Consider a single neuron in a neural network:\n","\n","$$\n","z = \\sum_{i=1}^{n} w_i x_i\n","$$\n","\n","- $z$ → pre-activation output of the neuron (input to the non-linearity)  \n","- $n$ → number of input connections (fan-in)  \n","- $w_i$ → weight associated with the $i$-th input  \n","- $x_i$ → $i$-th input activation  \n","\n","This is a weighted sum of inputs before applying an activation function.\n","\n","\n","### Variance of the neuron output\n","\n","Assuming:\n","- inputs $x_i$ are independent and identically distributed  \n","- weights $w_i$ are independent of inputs  \n","- both have zero mean  \n","\n","the variance of $z$ becomes:\n","\n","$$\n","\\text{Var}(z) = n \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","$$\n","\n","- $\\text{Var}(z)$ → variance of the neuron’s output  \n","- $n$ → number of summed terms (fan-in)  \n","- $\\text{Var}(w)$ → variance of the weight distribution  \n","- $\\text{Var}(x)$ → variance of the input activations  \n","\n","### Why this matters in deep networks\n","\n","As signals propagate through layers:\n","\n","- If $\\text{Var}(z)$ **increases layer by layer** → activations and gradients explode  \n","- If $\\text{Var}(z)$ **decreases layer by layer** → activations and gradients vanish  \n","\n","Both cases make learning ineffective.\n","\n","### Key Insight\n","> Proper initialization chooses $\\text{Var}(w)$ such that  \n","> $\\text{Var}(z)$ remains **approximately constant across layers**.\n","\n","This is the mathematical foundation behind **Xavier and He initialization**."]},{"cell_type":"markdown","id":"54dfab3f","metadata":{"papermill":{"duration":0.003658,"end_time":"2026-02-07T08:12:20.190053","exception":false,"start_time":"2026-02-07T08:12:20.186395","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\"> Where Does  Var(z)  Come From?</p>\n","\n","Start with the neuron equation:\n","\n","$$\n","z = \\sum_{i=1}^{n} w_i x_i\n","$$\n","\n","This is a sum of random variables.\n","\n","\n","**Step 1: Variance of a sum**\n","\n","A basic probability rule:\n","\n","If random variables are independent,\n","\n","$$\n","\\text{Var}\\!\\left(\\sum_{i=1}^{n} y_i\\right)\n","= \\sum_{i=1}^{n} \\text{Var}(y_i)\n","$$\n","\n","So we apply this to:\n","\n","$$\n","z = w_i x_i\n","$$\n","\n","Then:\n","\n","$$\n","\\text{Var}(z) = \\sum_{i=1}^{n} \\text{Var}(w_i x_i)\n","$$\n","\n","\n","**Step 2: Variance of a product**\n","\n","Another key rule (under independence and zero mean):\n","\n","$$\n","\\text{Var}(w_i x_i)\n","= \\text{Var}(w_i)\\,\\text{Var}(x_i)\n","$$\n","\n","Why this holds:\n","- $w_i$ and $x_i$ are independent  \n","- $\\mathbb{E}[w_i] = 0$, $\\mathbb{E}[x_i] = 0$  \n","\n","So each term contributes:\n","\n","$$\n","\\text{Var}(w_i x_i) = \\text{Var}(w)\\,\\text{Var}(x)\n","$$\n","\n","\n","**Step 3: Sum all contributions**\n","\n","Since every term has the same variance:\n","\n","$$\n","\\text{Var}(z)\n","= \\sum_{i=1}^{n} \\text{Var}(w)\\,\\text{Var}(x)\n","$$\n","\n","$$\n","\\boxed{\n","\\text{Var}(z) = n \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","}\n","$$\n","\n","\n","### What this means\n","\n","- Each input contributes **a little variance**\n","- Adding $n$ such contributions multiplies variance by $n$\n","\n","> More connections = more variance unless weights are scaled down\n","\n","\n","### Why this breaks deep networks\n","\n","Across layers:\n","\n","$$\n","\\text{Var}(x^{(l+1)}) = n^{(l)} \\text{Var}(w^{(l)}) \\text{Var}(x^{(l)})\n","$$\n","\n","Repeat this 50 times → explosion or collapse.\n","\n","---\n"]},{"cell_type":"markdown","id":"f1fa88de","metadata":{"papermill":{"duration":0.004624,"end_time":"2026-02-07T08:12:20.198348","exception":false,"start_time":"2026-02-07T08:12:20.193724","status":"completed"},"tags":[]},"source":["# Xavier (Glorot) Initialization\n"]},{"cell_type":"markdown","id":"a022b4ad","metadata":{"papermill":{"duration":0.004146,"end_time":"2026-02-07T08:12:20.206538","exception":false,"start_time":"2026-02-07T08:12:20.202392","status":"completed"},"tags":[]},"source":["## Core idea\n","\n","Xavier initialization is a weight initialization strategy designed to keep the **variance of activations and gradients approximately constant across layers** in deep neural networks.\n","\n","Its objective is to prevent:\n","- **Vanishing signals** (variance shrinking with depth)\n","- **Exploding signals** (variance growing with depth)\n","\n","during **both the forward and backward pass**.\n","\n","For a neuron:\n","$$\n","z = \\sum_{i=1}^{n_{in}} w_i x_i\n","$$\n","\n","Assuming:\n","- inputs $x_i$ are i.i.d. with zero mean and variance $\\text{Var}(x)$  \n","- weights $w_i$ are i.i.d. with zero mean and variance $\\text{Var}(w)$  \n","- weights and inputs are independent  \n","\n","the output variance becomes:\n","$$\n","\\text{Var}(z) = n_{in} \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","$$\n","\n","To keep signal magnitude stable across layers, we want:\n","$$\n","\\text{Var}(z) \\approx \\text{Var}(x)\n","$$\n","\n","This leads to:\n","$$\n","n_{in} \\cdot \\text{Var}(w) \\approx 1\n","$$\n","\n","\n","### Why $n_{out}$ also appears\n","\n","Backpropagation imposes a **similar constraint** on gradient variance, which depends on $n_{out}$ (fan-out).\n","\n","To balance **both forward and backward variance**, Xavier initialization chooses:\n","\n","$$\n","\\boxed{\n","\\text{Var}(w) = \\frac{2}{n_{in} + n_{out}}\n","}\n","$$\n","\n","where:\n","- $n_{in}$ → number of input connections (fan-in)  \n","- $n_{out}$ → number of output connections (fan-out)  \n","- $\\text{Var}(w)$ → variance of the weight distribution  \n","\n","This choice ensures:\n","- activations neither explode nor vanish in the forward pass  \n","- gradients remain well-scaled in the backward pass  \n","\n","### Practical forms\n","\n","Xavier initialization doesn’t set all weights to the same number.  \n","Instead, **each weight is chosen randomly** from a carefully controlled distribution to preserve variance.\n","\n","#### 1️. Uniform Xavier\n","\n","$$\n","w \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{in}+n_{out}}},\\;\n","\\sqrt{\\frac{6}{n_{in}+n_{out}}}\\right)\n","$$\n","\n","- Each weight is drawn **independently at random** from the interval  \n","  $$[-\\sqrt{6/(n_{in}+n_{out})}, \\sqrt{6/(n_{in}+n_{out})}]$$\n","- All values in this range are **equally likely**\n","- Example:  \n","  If $n_{in}=128$, $n_{out}=64$, then $\\sqrt{6/192} \\approx 0.176$  \n","  → $w \\in [-0.176, 0.176]$ randomly\n","\n","\n","#### 2️. Normal (Gaussian) Xavier\n","\n","$$\n","w \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in}+n_{out}}\\right)\n","$$\n","\n","- Each weight is drawn independently from a **bell-shaped curve**  \n","  centered at 0 with variance $2/(n_{in}+n_{out})$\n","- Most weights are near 0, rare large values\n","- Example:  \n","  With the same $n_{in}$ and $n_{out}$, $\\text{Var}(w) \\approx 0.0104$, $\\sigma \\approx 0.102$  \n","  → most weights lie roughly in $[-0.3, 0.3]$\n","\n","#### Why randomness matters\n","\n","- Prevents neurons from being identical (symmetry problem)  \n","- Breaks correlation while controlling scale for stable signal propagation  \n","\n","#### Why these ranges are chosen\n","\n","- Ensures forward and backward variance is approximately constant:  \n","$$\n","\\text{Var}(z^{(l)}) \\approx \\text{Var}(z^{(l-1)})\n","$$\n"]},{"cell_type":"markdown","id":"8fdc69e0","metadata":{"papermill":{"duration":0.003854,"end_time":"2026-02-07T08:12:20.214331","exception":false,"start_time":"2026-02-07T08:12:20.210477","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\">Understanding the Math Behind Xavier Initialization</p>\n","\n","Xavier is designed for symmetric activations (tanh, sigmoid), assuming no variance is lost after the activation.\n","\n","### 1. Xavier Uniform\n","\n","Rule:\n","$$\n","w \\sim \\mathcal{U}\\Big(-\\sqrt{\\frac{6}{n_{in}+n_{out}}},\\; \\sqrt{\\frac{6}{n_{in}+n_{out}}}\\Big)\n","$$\n","\n","Step-by-step:\n","\n","1. Variance of uniform distribution $U[a,b]$:\n","\n","\n","$$\n","\\text{Var}(U[a,b]) = \\frac{(b-a)^2}{12}\n","$$\n","\n","3. Plug in bounds: $a=-r, b=r$:\n","\n","$$\n","\\text{Var}(U[-r,r]) = \\frac{(r - (-r))^2}{12} = \\frac{4 r^2}{12} = \\frac{r^2}{3}\n","$$\n","\n","3. Set this equal to desired Xavier variance:\n","$$\n","\\frac{r^2}{3} = \\frac{2}{n_{in}+n_{out}} \\quad \\Rightarrow \\quad \\boxed{r = \\sqrt{\\frac{6}{n_{in}+n_{out}}}}\n","$$\n","\n","> Each weight is sampled **uniformly in [-r, r]**, producing the correct variance.\n","\n","\n","### 2. Xavier Normal\n","\n","Rule:\n","$$\n","w \\sim \\mathcal{N}\\left(0,\\; \\frac{2}{n_{in}+n_{out}}\\right)\n","$$\n","\n","Step-by-step:\n","\n","1. **Neuron output**:\n","$$\n","z = \\sum_{i=1}^{n_{in}} w_i x_i\n","$$\n","\n","2. **Variance of output**:\n","$$\n","\\text{Var}(z) = n_{in} \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","$$\n","\n","3. **Forward + backward balance**:  \n","\n","We want both forward and backward variance to remain stable.  \n","- Forward: $n_{in} \\cdot \\text{Var}(w) \\approx 1$  \n","- Backward: $n_{out} \\cdot \\text{Var}(w) \\approx 1$\n","\n","4. **Compromise formula**:\n","\n","We take the average (harmonic mean) of the two constraints:\n","\n","$$\n","\\text{Var}(w) = \\frac{1}{2}\\Big(\\frac{1}{n_{in}} + \\frac{1}{n_{out}}\\Big)^{-1} \\quad \\approx \\frac{2}{n_{in}+n_{out}}\n","$$\n","\n","$$\n","\\boxed{\\text{Var}(w) = \\frac{2}{n_{in}+n_{out}}}\n","$$\n","- That’s why 2 appears in the numerator  \n","- It balances forward and backward variance in one formula\n","\n","> Each weight is sampled from a Gaussian with mean 0 and variance $\\frac{2}{n_{in}+n_{out}}$\n","\n","\n","**Key Intuition**\n","\n","- Xavier Normal → Gaussian  \n","- Xavier Uniform → Uniform  \n","- Both achieve the **same target variance**  \n","- Suitable for **symmetric activations** (tanh, sigmoid)  \n","- Not ideal for ReLU (use He instead)\n","\n","---"]},{"cell_type":"markdown","id":"c29cf358","metadata":{"papermill":{"duration":0.003542,"end_time":"2026-02-07T08:12:20.221571","exception":false,"start_time":"2026-02-07T08:12:20.218029","status":"completed"},"tags":[]},"source":["## When Xavier Works Well\n","\n","Xavier initialization is most effective with **symmetric activation functions**:\n","\n","Suitable activations:\n","- `sigmoid`  \n","- `tanh`  \n","\n","Why it works:\n","- These activations produce outputs centered around zero  \n","- Symmetric output ensures variance propagation assumptions hold  \n","- Forward and backward signals remain stable across layers\n"]},{"cell_type":"markdown","id":"34a990d2","metadata":{"papermill":{"duration":0.003687,"end_time":"2026-02-07T08:12:20.228886","exception":false,"start_time":"2026-02-07T08:12:20.225199","status":"completed"},"tags":[]},"source":["## Important Caveat\n","\n","CNNs almost always use **ReLU activations**, not `tanh`.\n","\n","Problem:\n","- ReLU sets all negative activations to zero  \n","- This halves the effective variance of the signal  \n","- Xavier initialization assumes symmetric activations, so it underestimates the needed variance\n","\n","Consequence:\n","> Learning is slow, unstable, or may fail to converge in deep ReLU networks\n","\n","For ReLU-based CNNs, use **He initialization** instead of Xavier\n"]},{"cell_type":"markdown","id":"64e49da5","metadata":{"papermill":{"duration":0.003693,"end_time":"2026-02-07T08:12:20.236287","exception":false,"start_time":"2026-02-07T08:12:20.232594","status":"completed"},"tags":[]},"source":["## Xavier Initialization in PyTorch\n","\n","```python\n","# Uniform Xavier\n","nn.init.xavier_uniform_(conv.weight)\n","\n","# Normal (Gaussian) Xavier\n","nn.init.xavier_normal_(conv.weight)\n","```\n","\n","Notes:\n","\n","* Each weight is randomly initialized using the Xavier formulas\n","* Works well for symmetric activations like `tanh` or `sigmoid`\n","* Keeps variance of activations and gradients roughly constant across layers"]},{"cell_type":"markdown","id":"841e86a5","metadata":{"papermill":{"duration":0.00393,"end_time":"2026-02-07T08:12:20.244438","exception":false,"start_time":"2026-02-07T08:12:20.240508","status":"completed"},"tags":[]},"source":["# He (Kaiming) Initialization\n"]},{"cell_type":"markdown","id":"886a95a8","metadata":{"papermill":{"duration":0.003731,"end_time":"2026-02-07T08:12:20.252017","exception":false,"start_time":"2026-02-07T08:12:20.248286","status":"completed"},"tags":[]},"source":["## Core Idea\n","\n","**He Initialization** (also called Kaiming Initialization) is a weight initialization method designed specifically for ReLU-based neural networks, where weights are initialized so that activation variance remains approximately constant across layers, despite ReLU zeroing out half of the inputs.\n","\n","Formally, it sets the variance of weights as:\n","$$\n","\\text{Var}(w) = \\frac{2}{n_{in}}\n","$$\n","\n","**Why He Initialization Exists**\n","\n","ReLU behaves as:\n","$$\n","\\text{ReLU}(x) = \\max(0, x)\n","$$\n","\n","This causes:\n","- ~50% of activations → exactly zero\n","- Output variance → reduced by ~½ after each ReLU\n","\n","Without correction:\n","- Activations shrink layer by layer\n","- Gradients weaken\n","- Deep CNNs fail to train effectively\n","\n","He initialization explicitly compensates for this variance loss.\n","\n","\n","For a neuron:\n","$$\n","z = \\sum_{i=1}^{n_{in}} w_i x_i\n","$$\n","\n","If:\n","- Inputs $x_i$ have variance 1\n","- Weights are zero-mean and independent\n","\n","Then:\n","$$\n","\\text{Var}(z) = n_{in} \\cdot \\text{Var}(w)\n","$$\n","\n","After ReLU:\n","$$\n","\\text{Var}(\\text{ReLU}(z)) \\approx \\frac{1}{2}\\text{Var}(z)\n","$$\n","\n","To keep variance ≈ 1:\n","$$\n","\\frac{1}{2} \\cdot n_{in} \\cdot \\text{Var}(w) = 1\n","\\Rightarrow \\boxed{\\text{Var}(w) = \\frac{2}{n_{in}}}\n","$$\n","\n","\n","Practical Meaning of $n_{in}$:\n","\n","- **Fully Connected layer**: number of input features  \n","- **Convolution layer**:\n","\n","$$\n","n_{in} = k_h \\times k_w \\times c_{in}\n","$$\n","\n"]},{"cell_type":"markdown","id":"8ab7a2fb","metadata":{"papermill":{"duration":0.003786,"end_time":"2026-02-07T08:12:20.25955","exception":false,"start_time":"2026-02-07T08:12:20.255764","status":"completed"},"tags":[]},"source":["## Does He Initialization Use Random or Fixed Values?\n"," \n","**Random values, not fixed constants.**  \n","He initialization defines how weights are randomly sampled, not a single value.\n","\n","### What He Initialization Actually Does\n","\n","He initialization controls the distribution of weights:\n","\n","- Mean = 0  \n","- Variance = $\\dfrac{2}{n_{in}}$\n","\n","Each weight is random, but its scale is carefully chosen so that signal does not vanish after ReLU.\n","\n","\n","### Common He Initialization Variants\n","\n","#### 1️. He Normal (Most Common)\n","Weights are sampled from a normal distribution:\n","$$\n","w \\sim \\mathcal{N}\\left(0,\\; \\frac{2}{n_{in}}\\right)\n","$$\n","\n","Interpretation:\n","- Bell-shaped distribution\n","- Most values are small\n","- Occasional larger values allowed\n","\n","\n","#### 2️. He Uniform\n","Weights are sampled from a uniform distribution:\n","$$\n","w \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{in}}},\\; \\sqrt{\\frac{6}{n_{in}}}\\right)\n","$$\n","\n","Interpretation:\n","- Every value in the range is equally likely\n","- Strict upper and lower bounds\n","\n","### Critical Clarification\n","\n","| Question | Answer |\n","|--------|-------|\n","Are weights fixed? | No |\n","Are all weights identical? | No |\n","Are weights random each run? | Yes |\n","Is randomness controlled? | Yes |\n","\n","If weights were fixed:\n","- Neurons would learn identical features\n","- Training would completely fail\n","\n","Randomness is required.  \n","He initialization makes it mathematically stable.\n","\n","\n","> **He initialization = random weights scaled correctly so ReLU doesn’t destroy gradient flow.**\n"]},{"cell_type":"markdown","id":"6dc89856","metadata":{"papermill":{"duration":0.003713,"end_time":"2026-02-07T08:12:20.267011","exception":false,"start_time":"2026-02-07T08:12:20.263298","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\">Understanding the Math Behind He Initialization</p>\n","\n","#### 1️. He Normal\n","\n","Rule:\n","$$\n","w \\sim \\mathcal{N}\\left(0,\\; \\frac{2}{n_{in}}\\right)\n","$$\n","\n","Step-by-step:\n","\n","1. **Neuron output**:\n","$$\n","z = \\sum_{i=1}^{n_{in}} w_i x_i\n","$$\n","\n","2. **Variance of output** (before ReLU):\n","$$\n","\\text{Var}(z) = n_{in} \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","$$\n","\n","3. **Effect of ReLU**:\n","- ReLU zeros out ~50% of inputs  \n","- Variance after ReLU:\n","$$\n","\\text{Var}(\\text{ReLU}(z)) \\approx \\frac{1}{2} \\text{Var}(z)\n","$$\n","\n","4. **To keep signal variance stable**, want:\n","$$\n","\\text{Var}(\\text{ReLU}(z)) \\approx \\text{Var}(x)\n","$$\n","\n","5. **Solve for Var(w)**:\n","$$\n","\\frac{1}{2} \\cdot n_{in} \\cdot \\text{Var}(w) = 1\n","\\quad \\Rightarrow \\quad\n","\\boxed{\\text{Var}(w) = \\frac{2}{n_{in}}}\n","$$\n","\n","> Each weight is then sampled from a Gaussian with mean 0 and variance $2/n_{in}$.\n","\n","\n","### 2️. He Uniform\n","\n","Rule:\n","$$\n","w \\sim \\mathcal{U}\\Big(-\\sqrt{\\frac{6}{n_{in}}},\\; \\sqrt{\\frac{6}{n_{in}}}\\Big)\n","$$\n","\n","Step-by-step:\n","\n","1. Variance of uniform distribution $U[a,b]$:\n","$$\n","\\text{Var}(U[a,b]) = \\frac{(b-a)^2}{12}\n","$$\n","\n","2. Plug in bounds: $a=-r, b=r$  \n","$$\n","\\text{Var}(w) = \\frac{(r - (-r))^2}{12} = \\frac{(2r)^2}{12} = \\frac{4 r^2}{12} = \\frac{r^2}{3}\n","$$\n","\n","3. Set this equal to desired He variance $2/n_{in}$:\n","$$\n","\\frac{r^2}{3} = \\frac{2}{n_{in}} \\quad \\Rightarrow \\quad r^2 = \\frac{6}{n_{in}} \\quad \\Rightarrow \\quad \\boxed{r = \\sqrt{\\frac{6}{n_{in}}}}\n","$$\n","\n","> Each weight is then sampled uniformly in $[-r, r]$, which produces the same target variance as He Normal.\n","\n","\n","**Key Intuition**\n","\n","- Both Normal and Uniform produce random weights with variance 2/n_in \n","- Randomness breaks symmetry  \n","- Variance scaling compensates for ReLU killing half the signal\n","\n","---"]},{"cell_type":"markdown","id":"988a1515","metadata":{"papermill":{"duration":0.003696,"end_time":"2026-02-07T08:12:20.274653","exception":false,"start_time":"2026-02-07T08:12:20.270957","status":"completed"},"tags":[]},"source":["## Why He Initialization Is Optimal for CNNs\n","\n","CNN Characteristics\n","- Very deep stacks of convolutional layers  \n","- ReLU activations in almost every layer  \n","- High fan-in (many input connections per neuron/filter)\n","\n","Why He Works\n","- **Preserves forward signal** → prevents activations from shrinking  \n","- **Preserves backward gradients** → avoids vanishing/exploding gradients  \n","- **Enables learning immediately** → no slow “warm-up” as with Xavier + ReLU\n","\n","> Bottom line: For deep, ReLU-based CNNs, **He initialization is almost always the default choice**.\n"]},{"cell_type":"markdown","id":"7a344e8a","metadata":{"papermill":{"duration":0.004371,"end_time":"2026-02-07T08:12:20.282684","exception":false,"start_time":"2026-02-07T08:12:20.278313","status":"completed"},"tags":[]},"source":["## He Initialization in PyTorch\n","\n","```python\n","nn.init.kaiming_normal_(\n","    conv.weight,\n","    mode=\"fan_in\",\n","    nonlinearity=\"relu\"\n",")\n","```\n","\n","or\n","\n","```python\n","nn.init.kaiming_uniform_(\n","    conv.weight,\n","    nonlinearity=\"relu\"\n",")\n","```\n","\n","**Default choice for CNNs**\n"]},{"cell_type":"markdown","id":"9de19de5","metadata":{"papermill":{"duration":0.003689,"end_time":"2026-02-07T08:12:20.29015","exception":false,"start_time":"2026-02-07T08:12:20.286461","status":"completed"},"tags":[]},"source":["# CNN-Specific Detail: `fan_in` in Conv Layers\n","\n","For a `Conv2D` layer:\n","\n","$$\n","\\text{fan\\_in} = C_{in} \\times K_h \\times K_w\n","$$\n","\n","Where:  \n","- $C_{in}$ → number of input channels  \n","- $K_h, K_w$ → kernel height and width\n","\n","Notes:\n","\n","- PyTorch automatically computes `fan_in` when you use `nn.init.kaiming_*` or `nn.init.xavier_*`  \n","- If you manually initialize weights, you **must respect fan_in**, otherwise:\n","  - Forward activations explode or vanish  \n","  - Backward gradients explode or vanish  \n","  - Variance assumptions in Xavier/He formulas break  \n","\n","> Correct fan_in calculation is **critical for deep CNN stability**\n"]},{"cell_type":"markdown","id":"6c3dd53d","metadata":{"papermill":{"duration":0.003684,"end_time":"2026-02-07T08:12:20.297571","exception":false,"start_time":"2026-02-07T08:12:20.293887","status":"completed"},"tags":[]},"source":["# Why fan_in Matters in Convolutional Layers\n","\n","### What fan_in represents\n","- fan_in = number of inputs contributing to a single neuron/output unit\n","- For Conv2D: \n","$$\n","\\text{fan\\_in} = C_{in} \\times K_h \\times K_w\n","$$\n","- Each output pixel is computed as:\n","$$\n","z = \\sum_{c=1}^{C_{in}} \\sum_{i=1}^{K_h} \\sum_{j=1}^{K_w} w_{c,i,j} \\cdot x_{c,i,j}\n","$$\n","- So `fan_in` counts all weight-input products summed for one output pixel\n","\n","\n","### Why it affects weight initialization\n","Variance of neuron output:\n","$$\n","\\text{Var}(z) = \\text{fan\\_in} \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)\n","$$\n","\n","- Larger fan_in → more summed terms → output variance **increases**  \n","- Smaller fan_in → output variance **decreases**  \n","\n","**Xavier/He formulas assume you know fan_in** to set $\\text{Var}(w)$ correctly:\n","- Too small variance → activations shrink → slow learning  \n","- Too large variance → activations explode → unstable gradients\n","\n","\n","### PyTorch convenience\n","- Functions like `nn.init.kaiming_normal_` automatically compute fan_in for Conv layers  \n","- If you manually compute weights, you must use the correct fan_in for the\n","\n","formula:\n","$$\n","\\text{Var}(w) = \\frac{2}{\\text{fan\\_in}} \\quad \\text{(He for ReLU)}\n","$$"]},{"cell_type":"markdown","id":"ad88f981","metadata":{"papermill":{"duration":0.003817,"end_time":"2026-02-07T08:12:20.305341","exception":false,"start_time":"2026-02-07T08:12:20.301524","status":"completed"},"tags":[]},"source":["# Why `mode=\"fan_in\"` in `nn.init.kaiming_normal_` Even Though PyTorch Calculates `fan_in`\n","\n","### What `mode` does\n","- `mode` tells PyTorch how to scale the variance of the weights:\n","  - `\"fan_in\"` → scales weights based on **number of input connections**  \n","  - `\"fan_out\"` → scales weights based on **number of output connections**  \n","  - `\"fan_avg\"` → uses average of `fan_in` and `fan_out`  \n","\n","- Scaling formula (He initialization):\n","\n","$$\n","\\text{Var}(w) = \\frac{2}{\\text{fan\\_in}} \\quad \\text{(for ReLU)}\n","$$\n","\n","### Automatic calculation vs scaling decision\n","- PyTorch does compute fan_in automatically for Conv/Linear layers  \n","- But it needs your guidance on which one to use for scaling via `mode`  \n","- `\"fan_in\"` is the default and correct for ReLU, because variance of outputs depends on inputs summed \n","\n","> If you chose `\"fan_out\"` instead:\n","> - Forward activations variance could shrink/explode  \n","> - Backprop gradients could become unstable"]},{"cell_type":"markdown","id":"c2b35b88","metadata":{"papermill":{"duration":0.003641,"end_time":"2026-02-07T08:12:20.31275","exception":false,"start_time":"2026-02-07T08:12:20.309109","status":"completed"},"tags":[]},"source":["# Dead ReLU: Silent Model Death\n"]},{"cell_type":"markdown","id":"210410c2","metadata":{"papermill":{"duration":0.004931,"end_time":"2026-02-07T08:12:20.321797","exception":false,"start_time":"2026-02-07T08:12:20.316866","status":"completed"},"tags":[]},"source":["To be continue..."]},{"cell_type":"markdown","id":"a79de7fc","metadata":{"papermill":{"duration":0.003892,"end_time":"2026-02-07T08:12:20.329664","exception":false,"start_time":"2026-02-07T08:12:20.325772","status":"completed"},"tags":[]},"source":["---\n","\n","<p style=\"text-align:center; color:skyblue; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":4.096043,"end_time":"2026-02-07T08:12:20.653554","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-07T08:12:16.557511","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}