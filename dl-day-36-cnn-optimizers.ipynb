{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-36-cnn-optimizers?scriptVersionId=296674935\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"a25417c7","metadata":{"papermill":{"duration":0.005209,"end_time":"2026-02-09T07:01:50.223261","exception":false,"start_time":"2026-02-09T07:01:50.218052","status":"completed"},"tags":[]},"source":["# Day 36: CNN Optimizers\n","*SGD · RMSProp · Adam · AdamW · Convergence Tradeoffs*\n","\n","Welcome to Day 36! Not just “pick an optimizer.”\n","\n","Today you’ll understand:\n","\n","- Why some CNNs converge fast but overfit  \n","- Why some slow optimizers generalize better  \n","- How optimizer interacts with LR, weight decay, and batch size  \n","- Practical strategies to switch optimizers mid-training\n","\n","If you found this notebook helpful, your **<b style=\"color:skyblue;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"4395d47e","metadata":{"papermill":{"duration":0.00425,"end_time":"2026-02-09T07:01:50.231798","exception":false,"start_time":"2026-02-09T07:01:50.227548","status":"completed"},"tags":[]},"source":["# What an Optimizer Does\n","\n","An optimizer’s job is to update the network’s weights $\\theta$ to minimize the loss $L(\\theta)$:\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\cdot f(\\nabla_\\theta L)\n","$$\n","\n","Where:  \n","- $\\alpha$ → learning rate (step size)  \n","- $f(\\cdot)$ → optimizer-specific gradient transformation (e.g., momentum, adaptive scaling)\n","\n","> The choice of optimizer affects not just training speed, but also convergence stability and generalization quality.\n"]},{"cell_type":"markdown","id":"eeaa20a0","metadata":{"papermill":{"duration":0.004188,"end_time":"2026-02-09T07:01:50.240645","exception":false,"start_time":"2026-02-09T07:01:50.236457","status":"completed"},"tags":[]},"source":["# SGD (Stochastic Gradient Descent)\n"]},{"cell_type":"markdown","id":"337f2484","metadata":{"papermill":{"duration":0.004293,"end_time":"2026-02-09T07:01:50.249096","exception":false,"start_time":"2026-02-09T07:01:50.244803","status":"completed"},"tags":[]},"source":["## SGD Intuition\n","\n","SGD is a first-order optimization algorithm that updates weights in the direction of the negative gradient of the loss:\n","\n","$$\n","\\boxed{\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta L}\n","$$\n","\n","- $\\theta_t$ → current weights  \n","- $\\nabla_\\theta L$ → gradient of loss w.r.t weights  \n","- $\\alpha$ → learning rate  \n","\n","> Idea: move weights opposite to the slope to reduce loss.\n","\n","\n","### Momentum Extension\n","\n","Momentum helps accelerate convergence along consistent gradient directions:\n","\n","$$\n","v_{t+1} = \\mu v_t - \\alpha \\nabla_\\theta L\n","$$\n","$$\n","\\theta_{t+1} = \\theta_t + v_{t+1}\n","$$\n","\n","Where:  \n","- $v_t$ → accumulated velocity  \n","- $\\mu$ → momentum factor (0–1)  \n","- $\\alpha$ → learning rate  \n","\n","**Mathematical Breakdown**:\n","\n","1. $v_{t+1}$ combines previous direction $v_t$ with current gradient $-\\alpha \\nabla_\\theta L$  \n","2. Update $\\theta$ by adding velocity → smooths zig-zag steps in steep directions  \n","3. Gradients in consistent directions get amplified, oscillations dampened\n","\n","\n","### Pros\n","\n","- Excellent generalization on unseen data  \n","- Deterministic with fixed LR and batch  \n","- Momentum accelerates learning in shallow or medium-depth CNNs\n","\n","### Cons\n","- Sensitive to learning rate\n","- Slower than adaptive optimizers (Adam, RMSProp)  \n","- May stall in very deep CNNs without proper initialization or LR schedule\n"]},{"cell_type":"markdown","id":"67c64271","metadata":{"papermill":{"duration":0.004978,"end_time":"2026-02-09T07:01:50.260011","exception":false,"start_time":"2026-02-09T07:01:50.255033","status":"completed"},"tags":[]},"source":["## Momentum in SGD\n","\n","### 1️. Motivation\n","\n","Plain SGD update:\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta L\n","$$\n","\n","- Moves weights directly opposite the gradient\n","- Problem: in steep, narrow valleys (common in deep CNNs), SGD can oscillate back and forth, slowing convergence\n","- Momentum is designed to smooth updates and accelerate along consistent directions\n","\n","### 2️. Momentum Update Rule\n","\n","Introduce a velocity vector $v_t$:\n","\n","$$\n","v_{t+1} = \\mu v_t - \\alpha \\nabla_\\theta L\n","$$\n","$$\n","\\theta_{t+1} = \\theta_t + v_{t+1}\n","$$\n","\n","Where:\n","- $v_t$ → accumulated “motion” from past gradients  \n","- $\\mu$ → momentum factor (0.9 is typical)  \n","- $\\alpha$ → learning rate\n","\n","### 3️. Intuition\n","- Think of $v_t$ as velocity of a ball rolling down a slope\n","- Gradient acts like a force pushing the ball\n","- Momentum remembers past updates → builds speed in directions where gradient is consistently pointing\n","- Reduces oscillations across steep directions, increases movement along gentle slopes\n","\n","### 4️. Step-by-Step Example\n","\n","Suppose $\\mu = 0.9$, $\\alpha = 0.01$:\n","\n","1. **First step**:  \n","$$\n","v_1 = 0 - 0.01 \\nabla_\\theta L_1 = -0.01 \\nabla_\\theta L_1\n","$$\n","$$\n","\\theta_1 = \\theta_0 + v_1\n","$$\n","\n","2. **Second step**:  \n","$$\n","v_2 = 0.9 v_1 - 0.01 \\nabla_\\theta L_2\n","$$\n","- 90% of previous “velocity” carried over  \n","- Current gradient contributes a small push  \n","$$\n","\\theta_2 = \\theta_1 + v_2\n","$$\n","\n","3. **Over time:**\n","- Updates accumulate along consistent directions  \n","- Updates cancel out in oscillating directions → smoother path\n","\n","\n","### 5️. Key Effects\n","\n","- Faster convergence along shallow directions  \n","- Reduced zig-zag along steep directions  \n","- Can escape small local minima more easily  \n","- Requires tuning momentum factor $\\mu$ carefully\n","\n","\n","> Momentum adds a memory effect to SGD. It’s like giving the optimizer “inertia,” so it doesn’t react only to the current gradient, but also to recent trends.\n"]},{"cell_type":"markdown","id":"933425a6","metadata":{"papermill":{"duration":0.004532,"end_time":"2026-02-09T07:01:50.269115","exception":false,"start_time":"2026-02-09T07:01:50.264583","status":"completed"},"tags":[]},"source":["``` python\n","# PyTorch SGD with momentum\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","```"]},{"cell_type":"markdown","id":"2ba6be01","metadata":{"papermill":{"duration":0.004015,"end_time":"2026-02-09T07:01:50.277313","exception":false,"start_time":"2026-02-09T07:01:50.273298","status":"completed"},"tags":[]},"source":["## Silent Failures in SGD\n","\n","Even with correct architecture and initialization, SGD can fail silently if hyperparameters are off:\n","\n","- **Learning rate too high**  \n","  - Gradients explode, loss may jump unpredictably  \n","  - Training appears unstable, but sometimes no NaNs appear  \n","\n","- **Learning rate too low**  \n","  - Updates are tiny → training progresses extremely slowly  \n","  - Loss decreases almost imperceptibly, giving false impression of stagnation  \n","\n","- **Momentum misconfigured**  \n","  - Too high → overshoots minima, oscillates along valleys  \n","  - Too low → slow convergence, little benefit over plain SGD  \n","\n","> SGD’s failure modes are often quiet, unlike crashes, making careful hyperparameter tuning essential."]},{"cell_type":"markdown","id":"429f1590","metadata":{"papermill":{"duration":0.004061,"end_time":"2026-02-09T07:01:50.285489","exception":false,"start_time":"2026-02-09T07:01:50.281428","status":"completed"},"tags":[]},"source":["# RMSProp (Root Mean Square Propagation)"]},{"cell_type":"markdown","id":"fbad569d","metadata":{"papermill":{"duration":0.003994,"end_time":"2026-02-09T07:01:50.293497","exception":false,"start_time":"2026-02-09T07:01:50.289503","status":"completed"},"tags":[]},"source":["## RMSProp Intuition\n","\n","RMSProp (Root Mean Square Propagation) is an adaptive learning rate optimizer*.\n","\n","<p style=\"color:orange;\">“Adaptive learning rate optimizer” is just a fancy way of saying the optimizer automatically changes the step size for each weight during training, instead of using the same fixed learning rate for all weights.\n","\n","- **Idea:** scale each weight’s update based on recent gradient magnitudes \n","- **Purpose:** avoid vanishing/exploding updates, especially in non-stationary or steep loss landscapes \n","\n","RMSProp is essentially momentum for the second moment (squared gradients).\n","\n","### RMSProp Update Equations\n","\n","1. **Compute moving average of squared gradients**:\n","\n","$$\n","\\boxed{v_t = \\beta v_{t-1} + (1-\\beta) (\\nabla_\\theta L_t)^2}\n","$$\n","\n","Where:  \n","- $v_t$ → running average of squared gradients (elementwise)  \n","- $\\beta$ → decay factor (typical 0.9)  \n","- $(\\nabla_\\theta L_t)^2$ → elementwise square of current gradient  \n","\n","2. **Update weights:**\n","\n","$$\n","\\boxed{\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t + \\epsilon}} \\nabla_\\theta L_t}\n","$$\n","\n","- $\\alpha$ → base learning rate  \n","- $\\epsilon$ → tiny number to prevent division by zero  \n","\n","\n","**Intuition**\n","\n","- RMSProp reduces step size for weights with large recent gradients \n","- RMSProp increases step size for weights with small recent gradients\n","- Acts like per-parameter adaptive learning rates\n","\n","**Analogy:**  \n","- You’re hiking in a landscape where some slopes are very steep and others shallow  \n","- RMSProp slows down on steep slopes, speeds up on shallow slopes, so you don’t overshoot or crawl unnecessarily\n","\n","\n","### Tiny Example\n","\n","Suppose one weight $\\theta$ with gradients: 0.2, 0.1, -0.05  \n","Set: $\\alpha = 0.1$, $\\beta = 0.9$, $\\epsilon = 1e-8$\n","\n","**Step 1**:  \n","\n","$$\n","v_1 = 0.9*0 + 0.1*(0.2^2) = 0.004\n","$$\n","$$\n","\\theta_1 = \\theta_0 - 0.1 / \\sqrt{0.004 + 1e-8} * 0.2\n","$$\n","$$\n","\\sqrt{0.004} \\approx 0.0632\n","$$\n","$$\n","\\Delta \\theta \\approx 0.1 / 0.0632 * 0.2 \\approx 0.316\n","$$\n","\n","- Notice: gradient scaled by recent magnitude  \n","\n","**Step 2**:  \n","$$\n","v_2 = 0.9 * 0.004 + 0.1 * 0.1^2 = 0.00361\n","$$\n","$$\n","\\theta_2 = \\theta_1 - 0.1 / \\sqrt{0.00361} * 0.1\n","$$\n","- Step size grows when recent gradients are small\n","\n","\n","### Pros\n","- Works well for non-stationary data (gradients change distribution over time)  \n","- Prevents exploding or vanishing updates by scaling each weight adaptively  \n","\n","### Cons\n","- Rarely used in modern CNNs; Adam/AdamW usually preferred  \n","- Can be less predictable than Adam because it only adapts learning rate, not momentum on gradients"]},{"cell_type":"markdown","id":"0549cbb4","metadata":{"papermill":{"duration":0.003896,"end_time":"2026-02-09T07:01:50.301406","exception":false,"start_time":"2026-02-09T07:01:50.29751","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\">RMSProp Update Equations: Detailed Breakdown</p>\n","\n","### 1️. The Core Equations\n","\n","1. Moving average of squared gradients:\n","$$\n","\\boxed{v_t = \\beta v_{t-1} + (1-\\beta) (\\nabla_\\theta L_t)^2}\n","$$\n","\n","2. Weight update:\n","$$\n","\\boxed{\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t + \\epsilon}} \\nabla_\\theta L_t}\n","$$\n","\n","\n","### 2️. Term-by-Term Explanation\n","\n","#### a) $v_t = \\beta v_{t-1} + (1-\\beta)(\\nabla_\\theta L_t)^2$\n","\n","- $v_t$ → running average of squared gradients\n","- $\\beta$ → decay factor (typically 0.9) → controls how much history matters\n","- $(\\nabla_\\theta L_t)^2$ → current squared gradient  \n","- **Purpose:** if a gradient is consistently large, $v_t$ grows → future updates are scaled down  \n","- **Intuition:** RMSProp tracks how steep the slope is for this parameter\n","\n","\n","#### b) $\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t + \\epsilon}} \\nabla_\\theta L_t$\n","\n","- $\\nabla_\\theta L_t$ → standard gradient  \n","- $\\sqrt{v_t + \\epsilon}$ → adaptive scaling based on past gradient magnitude  \n","  - Large $v_t$ → divide by bigger number → smaller step  \n","  - Small $v_t$ → divide by smaller number → larger step  \n","- $\\epsilon$ → tiny constant (e.g., $1e-8$) → avoids division by zero  \n","\n","> This ensures all weights move at roughly comparable effective speeds, even if gradients vary a lot across parameters.\n","\n","### 3️. Step-by-Step Intuition\n","\n","1. **Compute squared gradient** → measures “steepness” for this parameter  \n","2. **Combine with past history** using $\\beta$ → smooths noisy gradients  \n","3. **Scale current gradient by inverse sqrt** → large gradients shrink, small gradients grow  \n","4. **Update weight** → move in negative gradient direction, but scaled adaptively\n","\n","\n","### 4️. Numeric Mini Example\n","\n","Suppose a single weight, $\\alpha = 0.1$, $\\beta = 0.9$, $\\epsilon = 1e-8$\n","\n","| Step | Grad | $v_t$ (squared avg) | Update $\\Delta \\theta$ |\n","|------|------|--------------------|------------------------|\n","| 1    | 0.2  | 0.004              | $-0.1 / \\sqrt{0.004} * 0.2 \\approx -0.316$ |\n","| 2    | 0.1  | 0.00361            | $-0.1 / \\sqrt{0.00361} * 0.1 \\approx -0.052$ |\n","| 3    | -0.05| 0.00325            | $-0.1 / \\sqrt{0.00325} * (-0.05) \\approx +0.028$ |\n","\n","- Notice how the update step changes automatically depending on recent gradient magnitudes \n","- RMSProp prevents “overshooting” in steep directions and “slowness” in shallow ones\n","\n","### Key Takeaways\n","\n","- RMSProp = adaptive step size per weight\n","- Large gradients → smaller step  \n","- Small gradients → larger step  \n","- Smooths training and handles non-stationary loss surfaces\n","\n","---"]},{"cell_type":"markdown","id":"30799572","metadata":{"papermill":{"duration":0.00397,"end_time":"2026-02-09T07:01:50.309296","exception":false,"start_time":"2026-02-09T07:01:50.305326","status":"completed"},"tags":[]},"source":["### <p style=\"text-align:center; color:orange; font-size:18px;\">Decay Factor in RMSProp</p>\n","\n","\n","In RMSProp, the **decay factor** $\\beta$ controls how much past gradients influence the running average:\n","\n","$$\n","v_t = \\beta v_{t-1} + (1-\\beta)(\\nabla_\\theta L_t)^2\n","$$\n","\n","- $\\beta \\in [0,1)$  \n","- Also called **momentum for second moment** or **smoothing factor**\n","- In statistics, **first moment** = mean  \n","- **Second moment** = mean of **squared values** → measures **magnitude or variance**  \n","- In RMSProp, $v_t$ is a running average of **squared gradients**:\n","\n","$$\n","v_t = \\beta v_{t-1} + (1-\\beta)(\\nabla_\\theta L_t)^2\n","$$\n","\n","So $v_t$ is tracking the second moment of gradients.  \n","\n","### Role\n","\n","- **Large $\\beta$ (e.g., 0.9)** → keeps long-term memory  \n","  - Running average changes gradually → smoother, more stable  \n","- **Small $\\beta$ (e.g., 0.5)** → more weight on recent gradients \n","  - Running average reacts quickly → can be noisy  \n","\n","> Think of $\\beta$ as controlling how fast you “forget the past”.\n","\n","\n","### Analogy\n","\n","- Measure temperature every minute  \n","- Running average = previous average × $\\beta$ + current reading × $(1-\\beta)$  \n","- **High $\\beta$** → average changes slowly, remembers past hours  \n","- **Low $\\beta$** → average follows recent readings closely  \n","\n","Same principle applies to RMSProp: $\\beta$ smooths gradient magnitude history.\n","\n","\n","### Tiny Example\n","\n","- $\\beta = 0.9$, $v_{t-1} = 0.04$, current squared gradient = 0.01  \n","\n","$$\n","v_t = 0.9*0.04 + 0.1*0.01 = 0.037\n","$$\n","\n","- Most weight (0.9) comes from past → “decays slowly”  \n","- Small weight (0.1) comes from current → “partial update”\n","\n","- If $\\beta = 0.5$:\n","\n","$$\n","v_t = 0.5*0.04 + 0.5*0.01 = 0.025\n","$$\n","\n","- Running average reacts faster to new gradient  \n","\n","\n","Decay factor $\\beta$ = how much history you remember in your running average.\n","\n","---"]},{"cell_type":"markdown","id":"537af2f3","metadata":{"papermill":{"duration":0.003919,"end_time":"2026-02-09T07:01:50.3172","exception":false,"start_time":"2026-02-09T07:01:50.313281","status":"completed"},"tags":[]},"source":["```python\n","# Create an RMSprop optimizer for the model\n","# model.parameters(): all trainable weights of the network\n","# lr=1e-3: base learning rate\n","# alpha=0.99: decay factor for the running average of squared gradients\n","#              (controls how much past gradients influence the adaptive scaling)\n","optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3, alpha=0.99)\n","```\n"]},{"cell_type":"markdown","id":"ef981b69","metadata":{"papermill":{"duration":0.00475,"end_time":"2026-02-09T07:01:50.326297","exception":false,"start_time":"2026-02-09T07:01:50.321547","status":"completed"},"tags":[]},"source":["# Adam (Adaptive Moment Estimation)\n"]},{"cell_type":"markdown","id":"e0d00fba","metadata":{"papermill":{"duration":0.003942,"end_time":"2026-02-09T07:01:50.334278","exception":false,"start_time":"2026-02-09T07:01:50.330336","status":"completed"},"tags":[]},"source":["## Adam Intuition\n","\n","Adam  is an adaptive gradient optimizer that combines:\n","\n","1. **Momentum** → keeps a moving average of past gradients  \n","2. **RMSProp** → scales updates by recent gradient magnitude (adaptive learning rate)\n","\n","Purpose:\n","- Fast convergence\n","- Handles noisy or sparse gradients\n","- Works well \"out-of-the-box\" without careful LR tuning\n","\n","\n","### Adam Update Equations\n","\n","1. **Compute moving averages** of gradient and squared gradient:\n","\n","$$\n","m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla_\\theta L_t\n","$$\n","\n","- $m_t$ → “momentum” term (first moment estimate)  \n","- $\\beta_1$ → decay factor for momentum (typical 0.9)  \n","- $\\nabla_\\theta L_t$ → gradient at step $t$\n","\n","$$\n","v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla_\\theta L_t)^2\n","$$\n","\n","- $v_t$ → “variance” term (second moment estimate)  \n","- $\\beta_2$ → decay factor for squared gradients (typical 0.999)  \n","- $(\\nabla_\\theta L_t)^2$ → elementwise square of gradient  \n","\n","2. **Bias correction** (optional but common):\n","\n","$$\n","\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\n","$$\n","\n","- Corrects the fact that $m_0$ and $v_0$ start at 0 → avoids underestimation early\n","\n","3. **Parameter update**:\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n","$$\n","\n","- $\\alpha$ → base learning rate  \n","- $\\epsilon$ → tiny number (e.g., $1e-8$) to avoid division by zero  \n","- Intuition: divide momentum by RMS (adaptive scaling)\n","\n","\n","**Intuition**\n","\n","- $m_t$ → keeps track of trend in gradients (like momentum)  \n","- $v_t$ → keeps track of how big gradients are \n","- $\\hat{m}_t / \\sqrt{\\hat{v}_t}$ → scales step proportionally to signal / noise  \n","- Large gradients → step reduced  \n","- Small gradients → step amplified  \n","\n","> Adam dynamically adjusts the effective learning rate for each weight\n","\n","### Tiny Example\n","\n","Suppose one weight $\\theta$ with gradients over 3 steps:\n","\n","| Step | Grad ($\\nabla_\\theta L$) |\n","|------|---------------------------|\n","| 1    | 0.2                       |\n","| 2    | 0.1                       |\n","| 3    | -0.05                     |\n","\n","Set $\\alpha=0.1$, $\\beta_1=0.9$, $\\beta_2=0.999$, $\\epsilon=1e-8$  \n","\n","**Step 1**:  \n","\n","$$\n","m_1 = 0.9*0 + 0.1*0.2 = 0.02\n","$$\n","$$\n","v_1 = 0.999*0 + 0.001*0.2^2 = 0.00004\n","$$\n","$$\n","\\hat{m}_1 = 0.02 / (1-0.9^1) = 0.2\n","$$\n","$$\n","\\hat{v}_1 = 0.00004 / (1-0.999^1) \\approx 0.04\n","$$\n","$$\n","\\theta_1 = \\theta_0 - 0.1 * 0.2 / \\sqrt{0.04} \\approx \\theta_0 - 0.1\n","$$\n","\n","**Step 2**: similar, $m_2$, $v_2$, compute bias-corrected estimates, update $\\theta_2$  \n","\n","**Step 3**: $m_3$, $v_3$, update $\\theta_3$  \n","\n","> Notice how updates scale automatically based on both trend and magnitude of gradients.\n","\n","\n","### **Pros**:\n","- Fast convergence  \n","- Works well with sparse gradients  \n","- Often “plug-and-play” without LR tuning\n","\n","### **Cons**:\n","- Can overfit (fast convergence → less implicit regularization)  \n","- Sometimes worse final accuracy than SGD+momentum  \n","- Implicit weight decay may interfere with explicit regularization\n","\n","\n","> Adam is a smart, adaptive SGD, it tracks both direction and scale of gradients, making it very effective, but you must still monitor overfitting and final generalization.\n"]},{"cell_type":"markdown","id":"0e1b67ac","metadata":{"papermill":{"duration":0.004392,"end_time":"2026-02-09T07:01:50.342647","exception":false,"start_time":"2026-02-09T07:01:50.338255","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\">Adam Optimizer: Core Update Equations Breakdown</p>\n","\n","Adam combines **momentum (first moment)** with **RMSProp (second moment)** for adaptive, stable updates. Let’s go step by step.\n","\n","\n","#### Step 1. Compute Gradient\n","For a weight $\\theta$ at time step $t$:\n","\n","$$\n","g_t = \\nabla_\\theta L_t\n","$$\n","\n","- $g_t$ is the current gradient\n","- Represents how much the loss changes with respect to $\\theta$  \n","\n","\n","#### Step 2: First Moment (Momentum)\n","\n","$$\n","m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n","$$\n","\n","- $m_t$ = running average of past gradients (momentum)  \n","- $\\beta_1$ = decay factor for first moment (typical 0.9)  \n","- $(1-\\beta_1)$ = fraction of current gradient included  \n","- **Intuition:** smooths the gradient, capturing the direction of descent  \n","\n","> Like a “moving average” of gradients, it prevents oscillation in steep or noisy directions.\n","\n","#### Step 3: Second Moment (RMSProp)\n","\n","$$\n","v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n","$$\n","\n","- $v_t$ = running average of squared gradients \n","- $\\beta_2$ = decay factor for second moment (typical 0.999)  \n","- Tracks magnitude of gradients → adaptive scaling of updates  \n","- Squared gradient ensures we measure size only, ignoring direction\n","- **Intuition:** prevents steps from being too big on steep slopes and too small on flat slopes.\n","\n","#### Step 4: Bias Correction\n","\n","- At initialization: $m_0 = 0$, $v_0 = 0$  \n","- First few steps: $m_t$ and $v_t$ are biased toward zero because they are computed as:\n","\n","$$\n","m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n","$$\n","\n","- Step 1: $m_1 = (1-\\beta_1) g_1$ → much smaller than actual gradient magnitude  \n","- Step 2: $m_2 = \\beta_1 m_1 + (1-\\beta_1) g_2$ → still smaller than “true average”  \n","\n","> Without correction, early updates are tiny, slowing learning.\n","\n","Bias correction divides by $(1 - \\beta_1^t)$:\n","\n","$$\n","\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad\n","\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n","$$\n","\n","- $t$ = current time step  \n","- $(1 - \\beta^t)$ starts small → compensates for the initial “zero bias”  \n","\n","**Example:**  \n","\n","- $\\beta_1 = 0.9$, $t=1$  \n","- $1 - \\beta_1^1 = 0.1$ → divide by 0.1 → scale up $m_1$ 10×  \n","- Now $\\hat{m}_1 \\approx g_1$ → update is correct magnitude\n","\n","**Intuition**\n","\n","- Without correction: first updates are too small, learning is slow  \n","- With correction: first updates match the scale we expect, then later updates naturally settle  \n","- Bias correction is only significant in **early steps**  \n","- After many steps ($t \\to \\infty$), $\\beta^t \\to 0$, correction factor → 1  \n","- Then $\\hat{m}_t \\approx m_t$, $\\hat{v}_t \\approx v_t$  \n","\n","> Ensures Adam starts with correct step sizes immediately, preventing slow initial training.\n","\n","#### Step 5: Parameter Update\n","\n","$$\n","\\boxed{\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}}\n","$$\n","\n","- $\\alpha$ = learning rate  \n","- $\\hat{m}_t / \\sqrt{\\hat{v}_t}$ → **direction (momentum)** / **magnitude scaling (RMSProp)**  \n","- $\\epsilon$ = small number to avoid division by zero\n","- Intuition: move weight along a smoothed direction with adaptive step size.\n","\n","#### Summary of Roles\n","\n","| Term          | Role |\n","|---------------|------|\n","| $g_t$         | current gradient (signal) |\n","| $m_t, \\hat{m}_t$ | momentum → smooths direction |\n","| $v_t, \\hat{v}_t$ | second moment → adaptive step size |\n","| $\\alpha$      | base learning rate |\n","| $\\epsilon$    | numerical stability |\n","\n","Adam combines directional smoothing and per-parameter scaling, making it robust and fast for most deep learning tasks.\n","\n","---"]},{"cell_type":"markdown","id":"d92d0b35","metadata":{"papermill":{"duration":0.004011,"end_time":"2026-02-09T07:01:50.350886","exception":false,"start_time":"2026-02-09T07:01:50.346875","status":"completed"},"tags":[]},"source":["```python\n","\n","# PyTorch Adam\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","```"]},{"cell_type":"markdown","id":"6e99d066","metadata":{"papermill":{"duration":0.004771,"end_time":"2026-02-09T07:01:50.359704","exception":false,"start_time":"2026-02-09T07:01:50.354933","status":"completed"},"tags":[]},"source":["## Adam Failure Modes\n","\n","- **Quick loss drop** → may give a false sense of convergence, but validation may still be poor  \n","- **Overfits small datasets** rapidly due to aggressive adaptive steps  \n","- **Incorrect weight decay / regularization** → can harm generalization\n","\n","> Adam is fast and adaptive, but does not automatically prevent overfitting.\n"]},{"cell_type":"markdown","id":"60512b1a","metadata":{"papermill":{"duration":0.00429,"end_time":"2026-02-09T07:01:50.368167","exception":false,"start_time":"2026-02-09T07:01:50.363877","status":"completed"},"tags":[]},"source":["# AdamW (Adam with Decoupled Weight Decay)\n"]},{"cell_type":"markdown","id":"d3b4b31b","metadata":{"papermill":{"duration":0.004177,"end_time":"2026-02-09T07:01:50.376511","exception":false,"start_time":"2026-02-09T07:01:50.372334","status":"completed"},"tags":[]},"source":["## AdamW Intuition\n","\n","AdamW improves on Adam by properly applying **L2 regularization** while keeping Adam’s adaptive updates intact. Let’s break this down from scratch.\n","\n","\n","### What is L2 Regularization?\n","\n","L2 regularization is a weight shrinkage technique that prevents overfitting:\n","\n","$$\n","\\boxed{L_{\\text{total}} = L_{\\text{loss}} + \\lambda \\sum_i \\theta_i^2}\n","$$\n","\n","- $L_{\\text{total}}$: **Total Objective**\n","- $L_{\\text{loss}}$ → prediction error\n","- $\\theta_i$ → a single weight\n","- $\\theta_i^2$ → how large that weight is\n","- $\\sum$ → total size of all weights\n","- $\\lambda$: **Regularization Strength** → how strict we are\n","    * Controls **how hard we punish large weights**\n","    * Hyperparameter (you choose it)\n","\n","| $\\lambda$ value | Effect            |\n","| --------------- | ----------------- |\n","| $0$             | No regularization |\n","| Small           | Gentle control    |\n","| Large           | Strong shrinkage  |\n","\n","Typical values:\n","\n","```text\n","1e-4, 5e-4, 1e-3\n","```\n","\n","### Standard Adam\n","\n","Original Adam update equation:\n","\n","$$\n","\\boxed{\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}}\n","$$\n","\n","- $\\hat{m}_t$ → bias-corrected first moment (momentum)  \n","- $\\hat{v}_t$ → bias-corrected second moment (RMS scaling)  \n","- $\\alpha$ → learning rate  \n","\n",">No explicit weight decay / L2 term.<br>\n",">The optimizer only tries to minimize data loss.\n","\n","### Adam With Naive L2 (Not Recommended)\n","\n","Sometimes people add L2 regularization inside Adam gradient:\n","\n","$$\n","\\nabla L_{\\text{total}} = \\nabla L_{\\text{loss}} + \\lambda \\theta\n","$$\n","\n","Then update:\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n","$$\n","\n","Where $\\hat{m}_t$ now includes $\\lambda \\theta$.\n","\n","\n","**How it is done?**\n","\n","**Step 1: Start With L2-Augmented Loss**\n","\n","Total loss with L2:\n","\n","$$\n","L_{\\text{total}} = L_{\\text{loss}} + \\frac{\\lambda}{2} \\sum_i \\theta_i^2\n","$$\n","\n","- $L_{\\text{loss}}$ → data loss  \n","- $\\lambda \\theta_i^2 / 2$ → L2 regularization term  \n","\n","> Factor 1/2 is optional but convenient (derivative becomes clean)\n","\n","\n","**Step 2: Compute Gradient**\n","\n","Take derivative w.r.t. $\\theta_i$:\n","\n","$$\n","\\nabla_{\\theta_i} L_{\\text{total}} \n","= \\underbrace{\\nabla_{\\theta_i} L_{\\text{loss}}}_{\\text{data gradient}}\n","+ \\underbrace{\\lambda \\theta_i}_{\\text{L2 gradient}}\n","$$\n","\n","Interpretation:\n","\n","- $\\nabla_{\\theta_i} L_{\\text{loss}}$ → tells optimizer how to reduce prediction error  \n","- $\\lambda \\theta_i$ → tells optimizer “shrink weight toward zero”\n","\n","**Step 3: Adam’s Moment Estimators**\n","\n","Adam computes moving averages:\n","\n","1️. First moment (momentum-like):\n","\n","$$\n","m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla_{\\theta_i} L_{\\text{total}}\n","$$\n","\n","2️. Second moment (RMS scaling):\n","\n","$$\n","v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla_{\\theta_i} L_{\\text{total}})^2\n","$$\n","\n","Then bias-corrected:\n","\n","$$\n","\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad\n","\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\n","$$\n","\n","**Step 4: Standard Adam Update**\n","\n","Adam update uses adaptive scaling:\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n","$$\n","\n","Where:\n","\n","- $\\hat{m}_t$ already contains $\\lambda \\theta_i$ if L2 was added inside gradient\n","- $\\hat{v}_t$ also depends on this gradient (squared)\n","\n","**Step 5: Expand $\\hat{m}_t$ With L2 Term (Naive Approach)**\n","\n","If we expand first moment:\n","\n","$$\n","\\hat{m}_t = \\text{moment of } (\\nabla_{\\theta_i} L_{\\text{loss}} + \\lambda \\theta_i)\n","$$\n","\n","Then update becomes:\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\text{moment of } (\\nabla L_{\\text{loss}} + \\lambda \\theta)}{\\sqrt{\\text{RMS of } (\\nabla L_{\\text{loss}} + \\lambda \\theta)^2} + \\epsilon}\n","$$\n","\n","**Problem:**  \n","The $\\lambda \\theta$ part is rescaled by RMS, not a fixed shrink → inconsistent L2\n","\n","**Step 6: Why This Is Problematic**\n","\n","1. L2 weight decay is meant to shrink weights independently \n","2. Standard Adam rescales every gradient by $\\sqrt{\\hat{v}_t}$  \n","3. So $\\lambda \\theta$ is no longer exactly proportional → decay varies per weight per step  \n","4. Result → unpredictable regularization → worse generalization\n","\n","**Step 7: AdamW Fix (Decoupled)**\n","\n","Instead, apply weight decay separately:\n","\n","$$\n","\\theta_{t+1} = \\theta_t \n","- \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \n","- \\alpha \\lambda \\theta_t\n","$$\n","\n","- Gradient step (fit data)  \n","- Decoupled shrink step (L2 independent)  \n","\n","This guarantees consistent L2 regularization.\n","\n","\n","### AdamW: “Decoupled” Weight Decay\n","\n","AdamW separates weight decay from the adaptive gradient:\n","\n","$$\n","\\theta_{t+1} = \\theta_t \n","- \\underbrace{\\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}}_{\\text{gradient step}}\n","- \\underbrace{\\alpha \\lambda \\theta_t}_{\\text{weight decay step}}\n","$$\n","\n","- Gradient step → fits the data  \n","- Weight decay step → shrinks all weights independently  \n","\n","- Now L2 regularization behaves consistently  \n","- Works better in practice for CNNs / Transformers\n","\n","### Why “Decoupled”?\n","\n","- Standard Adam: gradient + L2 combined → adaptive scaling changes decay  \n","- AdamW: gradient step and decay step are separate → predictable, stable L2 effect  \n","\n","> Think of it as two separate levers:\n","> 1. “Move weights to reduce loss”  \n","> 2. “Shrink weights a little”  \n","> They do not interfere\n","\n","\n","### Core AdamW Update Equations\n","\n","1. **Gradient**:\n","\n","$$\n","g_t = \\nabla_\\theta L_t\n","$$\n","\n","2. **First moment (momentum)**:\n","\n","$$\n","m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n","$$\n","\n","3. **Second moment (RMS of gradients)**:\n","\n","$$\n","v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n","$$\n","\n","4. **Bias correction**:\n","\n","$$\n","\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad\n","\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n","$$\n","\n","5. **Parameter update with decoupled weight decay**:\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} - \\alpha \\lambda \\theta_t\n","$$\n","\n","- First term = **adaptive Adam step**  \n","- Second term = **direct weight shrinkage (L2 regularization)**  \n","\n","\n","### Tiny Example\n","\n","Suppose:\n","\n","- Weight: $\\theta_t = 0.5$  \n","- Gradient: $g_t = 0.2$  \n","- $m_t = 0.18$, $v_t = 0.04$  \n","- $\\alpha = 0.01$, $\\lambda = 0.1$, $\\epsilon = 1e-8$\n","\n","**Step 1: Adaptive step:**\n","\n","$$\n","\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\approx 0.9\n","$$\n","\n","**Step 2: Gradient-based update:**\n","\n","$$\n","\\Delta \\theta_{\\text{adam}} = \\alpha \\cdot 0.9 = 0.009\n","$$\n","\n","**Step 3: Weight decay update:**\n","\n","$$\n","\\Delta \\theta_{\\text{decay}} = \\alpha \\cdot \\lambda \\cdot \\theta_t = 0.01 * 0.1 * 0.5 = 0.0005\n","$$\n","\n","**Step 4: Total update:**\n","\n","$$\n","\\theta_{t+1} = 0.5 - 0.009 - 0.0005 = 0.4905\n","$$\n","\n","> Both adaptive step + weight shrinkage applied independently → proper regularization."]},{"cell_type":"markdown","id":"c0bcff89","metadata":{"papermill":{"duration":0.00397,"end_time":"2026-02-09T07:01:50.384632","exception":false,"start_time":"2026-02-09T07:01:50.380662","status":"completed"},"tags":[]},"source":["```python\n","# PyTorch AdamW\n","optimizer = torch.optim.AdamW(\n","    model.parameters(),  #   Tells optimizer which parameters to update\n","    lr=1e-3,             #   Learning rate α = 0.001 (step size for weight updates)\n","    betas=(0.9, 0.999),  #   (β1, β2) for Adam moments:\n","                         #   β1 = 0.9 → momentum for first moment (moving average of gradients)\n","                         #   β2 = 0.999 → momentum for second moment (RMS scaling)\n","    weight_decay=0.01    #   Decoupled L2 regularization (λ)\n","                         #   Shrinks weights independently of gradient step\n","                         #   Ensures consistent weight decay → better generalization\n",")\n","\n"]},{"cell_type":"markdown","id":"b5ef9e2d","metadata":{"papermill":{"duration":0.004759,"end_time":"2026-02-09T07:01:50.393509","exception":false,"start_time":"2026-02-09T07:01:50.38875","status":"completed"},"tags":[]},"source":["```python\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n","```\n"]},{"cell_type":"markdown","id":"704dae1e","metadata":{"papermill":{"duration":0.003992,"end_time":"2026-02-09T07:01:50.40167","exception":false,"start_time":"2026-02-09T07:01:50.397678","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\"> What Is Regularization? (Optional) </p>\n","\n","**Regularization = a rule that stops the model from becoming too complex.**\n","\n","Without regularization, a neural network will:\n","- Memorize training data\n","- Use very large weights\n","- Perform badly on new (unseen) data\n","\n","Regularization says:\n","\n","> “Fit the data, **but don’t overreact**.”\n","\n","It adds discipline to learning.\n","\n","### Why Models Overfit Without Regularization\n","\n","Imagine fitting points on a graph.\n","\n","- No regularization → curve bends wildly to hit every point\n","- With regularization → smoother curve that generalizes\n","\n","Neural networks behave the same way:\n","- Large weights → sharp, fragile decision boundaries\n","- Small weights → smooth, stable behavior\n","\n","\n","### What L2 Regularization Specifically Does\n","\n","**L2 regularization penalizes large weights.**\n","\n","It does not:\n","- Remove neurons\n","- Change architecture\n","- Randomly drop connections\n","\n","It only says:\n","\n","> “Big weights are expensive. Keep them small unless truly necessary.”\n","\n","### Intuition Before Math\n","\n","Think of each weight as a volume knob.\n","\n","- Large weight → loud reaction to input\n","- Small weight → calm reaction\n","\n","L2 regularization:\n","- Turns all knobs slightly down\n","- Prevents any single weight from dominating\n","\n","### L2 Regularization - Now the Math\n","\n","$$\n","\\boxed{L_{\\text{total}} = L_{\\text{loss}} + \\lambda \\sum_i \\theta_i^2}\n","$$\n","\n","- $L_{\\text{loss}}$ → prediction error\n","- $\\theta_i$ → a single weight\n","- $\\theta_i^2$ → how large that weight is\n","- $\\sum$ → total size of all weights\n","- $\\lambda$ → how strict we are\n","\n","So this means:\n","\n","> “Minimize prediction error and keep weights small.”\n","\n","### How L2 Changes Learning\n","\n","Gradient update becomes:\n","\n","$$\n","\\theta \\leftarrow \\theta - \\alpha (\\text{data gradient} + \\lambda \\theta)\n","$$\n","\n","This adds a pull toward zero on every weight.\n","\n","Result:\n","- Weights shrink slowly\n","- Model becomes smoother\n","- Overfitting reduces\n","\n","### Manual Example\n","\n","- One weight: $\\theta$\n","- One data point\n","- Simple squared error loss\n","- Learning rate: $\\alpha = 0.1$\n","\n","We compare:\n","1. Training **without** L2 regularization  \n","2. Training **with** L2 regularization  \n","\n","**Case 1: Without L2 Regularization**\n","\n","- Current weight: $\\theta = 10$\n","- Gradient from data loss:\n","$$\n","\\frac{\\partial L_{\\text{loss}}}{\\partial \\theta} = 2\n","$$\n","\n","Update rule:\n","$$\n","\\theta_{\\text{new}} = \\theta - \\alpha \\cdot \\nabla L\n","$$\n","\n","Plug in numbers:\n","$$\n","\\theta_{\\text{new}} = 10 - 0.1 \\times 2 = 9.8\n","$$\n","\n","Interpretation:\n","\n","- Weight changes only because of data\n","- If data encourages large weights → nothing stops it\n","- Overfitting risk remains\n","\n","**Case 2: With L2 Regularization**\n","\n","Add L2 term:\n","$$\n","L_{\\text{total}} = L_{\\text{loss}} + \\lambda \\theta^2\n","$$\n","\n","Let:\n","- $\\lambda = 0.5$\n","\n","Gradient of L2 term:\n","$$\n","\\frac{\\partial}{\\partial \\theta}(\\lambda \\theta^2) = 2\\lambda \\theta\n","$$\n","\n","Total gradient:\n","$$\n","\\nabla L_{\\text{total}} = 2 + 2(0.5)(10) = 2 + 10 = 12\n","$$\n","\n","Update:\n","\n","$$\n","\\theta_{\\text{new}} = 10 - 0.1 \\times 12 = 8.8\n","$$\n","\n","Compare the Two Updates:\n","\n","| Scenario | New Weight |\n","|--------|-----------|\n","| No L2 | 9.8 |\n","| With L2 | 8.8 |\n","\n"," What Just Happened?\n","\n","- Data wanted to reduce weight by **0.2**\n","- L2 added an **extra pull toward zero**\n","- Net effect: **stronger shrinkage**\n","\n","This happens every update step.\n","\n","\n","### What $\\lambda$ Controls\n","\n","| $\\lambda$ | Effect |\n","|---------|-------|\n","| $0$ | No regularization |\n","| Small | Gentle shrink |\n","| Large | Strong shrink |\n","\n","Too large:\n","- Underfitting  \n","\n","Too small:\n","- Overfitting \n","\n","### What Regularization Is NOT\n","\n","❌ Not about training speed  \n","❌ Not about optimizer choice  \n","❌ Not about accuracy on training set  \n","\n","✔ About **generalization**<br>\n","✔ About **stability**<br>\n","✔ About **robust learning**\n","\n","### Big Picture\n","\n","| Concept | Meaning |\n","|------|--------|\n","| Loss | Fit the data |\n","| Regularization | Control complexity |\n","| L2 | Penalize large weights |\n","| AdamW | Applies L2 correctly |\n","\n","> Overfitting is not a bug  \n","> It’s the default behavior without regularization\n","\n","\n","---"]},{"cell_type":"markdown","id":"bb48df5b","metadata":{"papermill":{"duration":0.004079,"end_time":"2026-02-09T07:01:50.409825","exception":false,"start_time":"2026-02-09T07:01:50.405746","status":"completed"},"tags":[]},"source":["# Optimizer Comparison: Real-World Perspective\n","\n","| Optimizer       | Convergence | Generalization | Notes                                  |\n","|-----------------|------------|----------------|---------------------------------------|\n","| SGD             | Slow       | Excellent      | Default choice for vision / CNN tasks |\n","| SGD + Momentum  | Medium     | Excellent      | Accelerates deep CNN / ResNet training|\n","| RMSProp         | Medium     | Moderate       | Mostly used in RNNs / non-stationary data |\n","| Adam            | Fast       | Moderate       | Works well for prototypes, small datasets |\n","| AdamW           | Fast       | Good           | Modern CNNs & Transformers; decoupled weight decay ensures stable L2 regularization |\n","\n"]},{"cell_type":"markdown","id":"779d1aef","metadata":{"papermill":{"duration":0.003995,"end_time":"2026-02-09T07:01:50.417855","exception":false,"start_time":"2026-02-09T07:01:50.41386","status":"completed"},"tags":[]},"source":["#  Practical Trade-Offs\n","\n","- **SGD** → slow but stable, best final performance  \n","- **Adam** → fast prototyping, may overfit  \n","- **AdamW** → fast + better generalization  \n","- **RMSProp** → specialized cases  \n","\n","> Choice depends on: model depth, dataset size, training schedule"]},{"cell_type":"markdown","id":"1d02bbae","metadata":{"papermill":{"duration":0.003932,"end_time":"2026-02-09T07:01:50.425799","exception":false,"start_time":"2026-02-09T07:01:50.421867","status":"completed"},"tags":[]},"source":["# Practical CNN Tuning Workflow\n","\n","1. Start with **Adam** (LR=1e-3)\n","2. Monitor training vs validation curves\n","3. If overfitting → switch to **AdamW**\n","4. For final production model → **SGD + momentum**\n","5. Adjust learning rate, weight decay, batch size iteratively\n","6. Consider LR schedulers (Cosine, Step, OneCycle) for best results\n"]},{"cell_type":"markdown","id":"989cf567","metadata":{"papermill":{"duration":0.003978,"end_time":"2026-02-09T07:01:50.433868","exception":false,"start_time":"2026-02-09T07:01:50.42989","status":"completed"},"tags":[]},"source":["# Key Takeaways from Day 36\n","\n","- Optimizer affects **speed**, **stability**, **generalization**  \n","- **Adam / AdamW** → fast prototyping  \n","- **SGD + momentum** → best for production CNNs  \n","- Weight decay in Adam must be decoupled (AdamW)  \n","- Always monitor curves, not just final accuracy  \n","- Silent failures often appear as “flat” or “diverging” loss curves\n"]},{"cell_type":"markdown","id":"ee40fad4","metadata":{"papermill":{"duration":0.003892,"end_time":"2026-02-09T07:01:50.44174","exception":false,"start_time":"2026-02-09T07:01:50.437848","status":"completed"},"tags":[]},"source":["---\n","<p style=\"text-align:center; color:skyblue; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":4.239026,"end_time":"2026-02-09T07:01:50.866238","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-09T07:01:46.627212","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}