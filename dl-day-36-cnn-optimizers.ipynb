{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-36-cnn-optimizers?scriptVersionId=296516268\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"311c92be","metadata":{"papermill":{"duration":0.004648,"end_time":"2026-02-08T09:49:40.453309","exception":false,"start_time":"2026-02-08T09:49:40.448661","status":"completed"},"tags":[]},"source":["# Day 36: CNN Optimizers\n","*SGD · RMSProp · Adam · AdamW · Convergence Tradeoffs*\n","\n","Welcome to Day 36! Not just “pick an optimizer.”\n","\n","Today you’ll understand:\n","\n","- Why some CNNs converge fast but overfit  \n","- Why some slow optimizers generalize better  \n","- How optimizer interacts with LR, weight decay, and batch size  \n","- Practical strategies to switch optimizers mid-training\n","\n","If you found this notebook helpful, your **<b style=\"color:skyblue;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"2f0a9ff9","metadata":{"papermill":{"duration":0.003571,"end_time":"2026-02-08T09:49:40.460837","exception":false,"start_time":"2026-02-08T09:49:40.457266","status":"completed"},"tags":[]},"source":["# What an Optimizer Does\n","\n","An optimizer’s job is to update the network’s weights $\\theta$ to minimize the loss $L(\\theta)$:\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\cdot f(\\nabla_\\theta L)\n","$$\n","\n","Where:  \n","- $\\alpha$ → learning rate (step size)  \n","- $f(\\cdot)$ → optimizer-specific gradient transformation (e.g., momentum, adaptive scaling)\n","\n","> The choice of optimizer affects not just training speed, but also convergence stability and generalization quality.\n"]},{"cell_type":"markdown","id":"1f60b0d8","metadata":{"papermill":{"duration":0.003186,"end_time":"2026-02-08T09:49:40.467495","exception":false,"start_time":"2026-02-08T09:49:40.464309","status":"completed"},"tags":[]},"source":["# SGD (Stochastic Gradient Descent)\n"]},{"cell_type":"markdown","id":"df295f62","metadata":{"papermill":{"duration":0.003085,"end_time":"2026-02-08T09:49:40.473802","exception":false,"start_time":"2026-02-08T09:49:40.470717","status":"completed"},"tags":[]},"source":["## SGD Intuition\n","\n","SGD is a first-order optimization algorithm that updates weights in the direction of the negative gradient of the loss:\n","\n","$$\n","\\boxed{\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta L}\n","$$\n","\n","- $\\theta_t$ → current weights  \n","- $\\nabla_\\theta L$ → gradient of loss w.r.t weights  \n","- $\\alpha$ → learning rate  \n","\n","> Idea: move weights opposite to the slope to reduce loss.\n","\n","\n","### Momentum Extension\n","\n","Momentum helps accelerate convergence along consistent gradient directions:\n","\n","$$\n","v_{t+1} = \\mu v_t - \\alpha \\nabla_\\theta L\n","$$\n","$$\n","\\theta_{t+1} = \\theta_t + v_{t+1}\n","$$\n","\n","Where:  \n","- $v_t$ → accumulated velocity  \n","- $\\mu$ → momentum factor (0–1)  \n","- $\\alpha$ → learning rate  \n","\n","**Mathematical Breakdown**:\n","\n","1. $v_{t+1}$ combines previous direction $v_t$ with current gradient $-\\alpha \\nabla_\\theta L$  \n","2. Update $\\theta$ by adding velocity → smooths zig-zag steps in steep directions  \n","3. Gradients in consistent directions get amplified, oscillations dampened\n","\n","\n","### Pros\n","\n","- Excellent generalization on unseen data  \n","- Deterministic with fixed LR and batch  \n","- Momentum accelerates learning in shallow or medium-depth CNNs\n","\n","### Cons\n","- Sensitive to learning rate\n","- Slower than adaptive optimizers (Adam, RMSProp)  \n","- May stall in very deep CNNs without proper initialization or LR schedule\n"]},{"cell_type":"markdown","id":"21d382e3","metadata":{"papermill":{"duration":0.003177,"end_time":"2026-02-08T09:49:40.480347","exception":false,"start_time":"2026-02-08T09:49:40.47717","status":"completed"},"tags":[]},"source":["## Momentum in SGD\n","\n","### 1️. Motivation\n","\n","Plain SGD update:\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta L\n","$$\n","\n","- Moves weights directly opposite the gradient\n","- Problem: in steep, narrow valleys (common in deep CNNs), SGD can oscillate back and forth, slowing convergence\n","- Momentum is designed to smooth updates and accelerate along consistent directions\n","\n","### 2️. Momentum Update Rule\n","\n","Introduce a velocity vector $v_t$:\n","\n","$$\n","v_{t+1} = \\mu v_t - \\alpha \\nabla_\\theta L\n","$$\n","$$\n","\\theta_{t+1} = \\theta_t + v_{t+1}\n","$$\n","\n","Where:\n","- $v_t$ → accumulated “motion” from past gradients  \n","- $\\mu$ → momentum factor (0.9 is typical)  \n","- $\\alpha$ → learning rate\n","\n","### 3️. Intuition\n","- Think of $v_t$ as velocity of a ball rolling down a slope\n","- Gradient acts like a force pushing the ball\n","- Momentum remembers past updates → builds speed in directions where gradient is consistently pointing\n","- Reduces oscillations across steep directions, increases movement along gentle slopes\n","\n","### 4️. Step-by-Step Example\n","\n","Suppose $\\mu = 0.9$, $\\alpha = 0.01$:\n","\n","1. **First step**:  \n","$$\n","v_1 = 0 - 0.01 \\nabla_\\theta L_1 = -0.01 \\nabla_\\theta L_1\n","$$\n","$$\n","\\theta_1 = \\theta_0 + v_1\n","$$\n","\n","2. **Second step**:  \n","$$\n","v_2 = 0.9 v_1 - 0.01 \\nabla_\\theta L_2\n","$$\n","- 90% of previous “velocity” carried over  \n","- Current gradient contributes a small push  \n","$$\n","\\theta_2 = \\theta_1 + v_2\n","$$\n","\n","3. **Over time:**\n","- Updates accumulate along consistent directions  \n","- Updates cancel out in oscillating directions → smoother path\n","\n","\n","### 5️. Key Effects\n","\n","- Faster convergence along shallow directions  \n","- Reduced zig-zag along steep directions  \n","- Can escape small local minima more easily  \n","- Requires tuning momentum factor $\\mu$ carefully\n","\n","\n","> Momentum adds a memory effect to SGD. It’s like giving the optimizer “inertia,” so it doesn’t react only to the current gradient, but also to recent trends.\n"]},{"cell_type":"markdown","id":"a75b2d61","metadata":{"papermill":{"duration":0.003114,"end_time":"2026-02-08T09:49:40.486678","exception":false,"start_time":"2026-02-08T09:49:40.483564","status":"completed"},"tags":[]},"source":["``` python\n","# PyTorch SGD with momentum\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","```"]},{"cell_type":"markdown","id":"8980dbef","metadata":{"papermill":{"duration":0.003183,"end_time":"2026-02-08T09:49:40.493134","exception":false,"start_time":"2026-02-08T09:49:40.489951","status":"completed"},"tags":[]},"source":["## Silent Failures in SGD\n","\n","Even with correct architecture and initialization, SGD can fail silently if hyperparameters are off:\n","\n","- **Learning rate too high**  \n","  - Gradients explode, loss may jump unpredictably  \n","  - Training appears unstable, but sometimes no NaNs appear  \n","\n","- **Learning rate too low**  \n","  - Updates are tiny → training progresses extremely slowly  \n","  - Loss decreases almost imperceptibly, giving false impression of stagnation  \n","\n","- **Momentum misconfigured**  \n","  - Too high → overshoots minima, oscillates along valleys  \n","  - Too low → slow convergence, little benefit over plain SGD  \n","\n","> SGD’s failure modes are often quiet, unlike crashes, making careful hyperparameter tuning essential."]},{"cell_type":"markdown","id":"455ae011","metadata":{"papermill":{"duration":0.00322,"end_time":"2026-02-08T09:49:40.499612","exception":false,"start_time":"2026-02-08T09:49:40.496392","status":"completed"},"tags":[]},"source":["# RMSProp (Root Mean Square Propagation)"]},{"cell_type":"markdown","id":"380e88a6","metadata":{"papermill":{"duration":0.00314,"end_time":"2026-02-08T09:49:40.50601","exception":false,"start_time":"2026-02-08T09:49:40.50287","status":"completed"},"tags":[]},"source":["## RMSProp Intuition\n","\n","RMSProp (Root Mean Square Propagation) is an adaptive learning rate optimizr*.\n","\n","<p style=\"color:orange;\">“Adaptive learning rate optimizer” is just a fancy way of saying the optimizer automatically changes the step size for each weight during training, instead of using the same fixed learning rate for all weights.\n","\n","- **Idea:** scale each weight’s update based on recent gradient magnitudes \n","- **Purpose:** avoid vanishing/exploding updates, especially in non-stationary or steep loss landscapes \n","\n","RMSProp is essentially momentum for the second moment (squared gradients).\n","\n","### RMSProp Update Equations\n","\n","1. **Compute moving average of squared gradients**:\n","\n","$$\n","\\boxed{v_t = \\beta v_{t-1} + (1-\\beta) (\\nabla_\\theta L_t)^2}\n","$$\n","\n","Where:  \n","- $v_t$ → running average of squared gradients (elementwise)  \n","- $\\beta$ → decay factor (typical 0.9)  \n","- $(\\nabla_\\theta L_t)^2$ → elementwise square of current gradient  \n","\n","2. **Update weights:**\n","\n","$$\n","\\boxed{\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t + \\epsilon}} \\nabla_\\theta L_t}\n","$$\n","\n","- $\\alpha$ → base learning rate  \n","- $\\epsilon$ → tiny number to prevent division by zero  \n","\n","\n","**Intuition**\n","\n","- RMSProp reduces step size for weights with large recent gradients \n","- RMSProp increases step size for weights with small recent gradients\n","- Acts like per-parameter adaptive learning rates\n","\n","**Analogy:**  \n","- You’re hiking in a landscape where some slopes are very steep and others shallow  \n","- RMSProp slows down on steep slopes, speeds up on shallow slopes, so you don’t overshoot or crawl unnecessarily\n","\n","\n","### Tiny Example\n","\n","Suppose one weight $\\theta$ with gradients: 0.2, 0.1, -0.05  \n","Set: $\\alpha = 0.1$, $\\beta = 0.9$, $\\epsilon = 1e-8$\n","\n","**Step 1**:  \n","\n","$$\n","v_1 = 0.9*0 + 0.1*(0.2^2) = 0.004\n","$$\n","$$\n","\\theta_1 = \\theta_0 - 0.1 / \\sqrt{0.004 + 1e-8} * 0.2\n","$$\n","$$\n","\\sqrt{0.004} \\approx 0.0632\n","$$\n","$$\n","\\Delta \\theta \\approx 0.1 / 0.0632 * 0.2 \\approx 0.316\n","$$\n","\n","- Notice: gradient scaled by recent magnitude  \n","\n","**Step 2**:  \n","$$\n","v_2 = 0.9 * 0.004 + 0.1 * 0.1^2 = 0.00361\n","$$\n","$$\n","\\theta_2 = \\theta_1 - 0.1 / \\sqrt{0.00361} * 0.1\n","$$\n","- Step size grows when recent gradients are small\n","\n","\n","### Pros\n","- Works well for non-stationary data (gradients change distribution over time)  \n","- Prevents exploding or vanishing updates by scaling each weight adaptively  \n","\n","### Cons\n","- Rarely used in modern CNNs; Adam/AdamW usually preferred  \n","- Can be less predictable than Adam because it only adapts learning rate, not momentum on gradients"]},{"cell_type":"markdown","id":"98cbd367","metadata":{"papermill":{"duration":0.0032,"end_time":"2026-02-08T09:49:40.512467","exception":false,"start_time":"2026-02-08T09:49:40.509267","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\">RMSProp Update Equations: Detailed Breakdown</p>\n","\n","### 1️. The Core Equations\n","\n","1. Moving average of squared gradients:\n","$$\n","\\boxed{v_t = \\beta v_{t-1} + (1-\\beta) (\\nabla_\\theta L_t)^2}\n","$$\n","\n","2. Weight update:\n","$$\n","\\boxed{\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t + \\epsilon}} \\nabla_\\theta L_t}\n","$$\n","\n","\n","### 2️. Term-by-Term Explanation\n","\n","#### a) $v_t = \\beta v_{t-1} + (1-\\beta)(\\nabla_\\theta L_t)^2$\n","\n","- $v_t$ → running average of squared gradients\n","- $\\beta$ → decay factor (typically 0.9) → controls how much history matters\n","- $(\\nabla_\\theta L_t)^2$ → current squared gradient  \n","- **Purpose:** if a gradient is consistently large, $v_t$ grows → future updates are scaled down  \n","- **Intuition:** RMSProp tracks how steep the slope is for this parameter\n","\n","\n","#### b) $\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t + \\epsilon}} \\nabla_\\theta L_t$\n","\n","- $\\nabla_\\theta L_t$ → standard gradient  \n","- $\\sqrt{v_t + \\epsilon}$ → adaptive scaling based on past gradient magnitude  \n","  - Large $v_t$ → divide by bigger number → smaller step  \n","  - Small $v_t$ → divide by smaller number → larger step  \n","- $\\epsilon$ → tiny constant (e.g., $1e-8$) → avoids division by zero  \n","\n","> This ensures all weights move at roughly comparable effective speeds, even if gradients vary a lot across parameters.\n","\n","### 3️. Step-by-Step Intuition\n","\n","1. **Compute squared gradient** → measures “steepness” for this parameter  \n","2. **Combine with past history** using $\\beta$ → smooths noisy gradients  \n","3. **Scale current gradient by inverse sqrt** → large gradients shrink, small gradients grow  \n","4. **Update weight** → move in negative gradient direction, but scaled adaptively\n","\n","\n","### 4️. Numeric Mini Example\n","\n","Suppose a single weight, $\\alpha = 0.1$, $\\beta = 0.9$, $\\epsilon = 1e-8$\n","\n","| Step | Grad | $v_t$ (squared avg) | Update $\\Delta \\theta$ |\n","|------|------|--------------------|------------------------|\n","| 1    | 0.2  | 0.004              | $-0.1 / \\sqrt{0.004} * 0.2 \\approx -0.316$ |\n","| 2    | 0.1  | 0.00361            | $-0.1 / \\sqrt{0.00361} * 0.1 \\approx -0.052$ |\n","| 3    | -0.05| 0.00325            | $-0.1 / \\sqrt{0.00325} * (-0.05) \\approx +0.028$ |\n","\n","- Notice how the update step changes automatically depending on recent gradient magnitudes \n","- RMSProp prevents “overshooting” in steep directions and “slowness” in shallow ones\n","\n","### Key Takeaways\n","\n","- RMSProp = adaptive step size per weight\n","- Large gradients → smaller step  \n","- Small gradients → larger step  \n","- Smooths training and handles non-stationary loss surfaces\n","\n","---"]},{"cell_type":"markdown","id":"d6d1b638","metadata":{"papermill":{"duration":0.0031,"end_time":"2026-02-08T09:49:40.518878","exception":false,"start_time":"2026-02-08T09:49:40.515778","status":"completed"},"tags":[]},"source":["### <p style=\"text-align:center; color:orange; font-size:18px;\">Decay Factor in RMSProp</p>\n","\n","\n","In RMSProp, the **decay factor** $\\beta$ controls how much past gradients influence the running average:\n","\n","$$\n","v_t = \\beta v_{t-1} + (1-\\beta)(\\nabla_\\theta L_t)^2\n","$$\n","\n","- $\\beta \\in [0,1)$  \n","- Also called **momentum for second moment** or **smoothing factor**\n","- In statistics, **first moment** = mean  \n","- **Second moment** = mean of **squared values** → measures **magnitude or variance**  \n","- In RMSProp, $v_t$ is a running average of **squared gradients**:\n","\n","$$\n","v_t = \\beta v_{t-1} + (1-\\beta)(\\nabla_\\theta L_t)^2\n","$$\n","\n","So $v_t$ is tracking the second moment of gradients.  \n","\n","### Role\n","\n","- **Large $\\beta$ (e.g., 0.9)** → keeps long-term memory  \n","  - Running average changes gradually → smoother, more stable  \n","- **Small $\\beta$ (e.g., 0.5)** → more weight on recent gradients \n","  - Running average reacts quickly → can be noisy  \n","\n","> Think of $\\beta$ as controlling how fast you “forget the past”.\n","\n","\n","### Analogy\n","\n","- Measure temperature every minute  \n","- Running average = previous average × $\\beta$ + current reading × $(1-\\beta)$  \n","- **High $\\beta$** → average changes slowly, remembers past hours  \n","- **Low $\\beta$** → average follows recent readings closely  \n","\n","Same principle applies to RMSProp: $\\beta$ smooths gradient magnitude history.\n","\n","\n","### Tiny Example\n","\n","- $\\beta = 0.9$, $v_{t-1} = 0.04$, current squared gradient = 0.01  \n","\n","$$\n","v_t = 0.9*0.04 + 0.1*0.01 = 0.037\n","$$\n","\n","- Most weight (0.9) comes from past → “decays slowly”  \n","- Small weight (0.1) comes from current → “partial update”\n","\n","- If $\\beta = 0.5$:\n","\n","$$\n","v_t = 0.5*0.04 + 0.5*0.01 = 0.025\n","$$\n","\n","- Running average reacts faster to new gradient  \n","\n","\n","Decay factor $\\beta$ = how much history you remember in your running average.\n","\n","---"]},{"cell_type":"markdown","id":"21eeacea","metadata":{"papermill":{"duration":0.003193,"end_time":"2026-02-08T09:49:40.525222","exception":false,"start_time":"2026-02-08T09:49:40.522029","status":"completed"},"tags":[]},"source":["```python\n","# Create an RMSprop optimizer for the model\n","# model.parameters(): all trainable weights of the network\n","# lr=1e-3: base learning rate\n","# alpha=0.99: decay factor for the running average of squared gradients\n","#              (controls how much past gradients influence the adaptive scaling)\n","optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3, alpha=0.99)\n","```\n"]},{"cell_type":"markdown","id":"79d24c9c","metadata":{"papermill":{"duration":0.003112,"end_time":"2026-02-08T09:49:40.531683","exception":false,"start_time":"2026-02-08T09:49:40.528571","status":"completed"},"tags":[]},"source":["# Adam (Adaptive Moment Estimation)\n"]},{"cell_type":"markdown","id":"4bd7b20a","metadata":{"papermill":{"duration":0.003113,"end_time":"2026-02-08T09:49:40.538","exception":false,"start_time":"2026-02-08T09:49:40.534887","status":"completed"},"tags":[]},"source":["## Adam Intuition\n","\n","Adam  is an adaptive gradient optimizer that combines:\n","\n","1. **Momentum** → keeps a moving average of past gradients  \n","2. **RMSProp** → scales updates by recent gradient magnitude (adaptive learning rate)\n","\n","Purpose:\n","- Fast convergence\n","- Handles noisy or sparse gradients\n","- Works well \"out-of-the-box\" without careful LR tuning\n","\n","\n","### Adam Update Equations\n","\n","1. **Compute moving averages** of gradient and squared gradient:\n","\n","$$\n","m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla_\\theta L_t\n","$$\n","\n","- $m_t$ → “momentum” term (first moment estimate)  \n","- $\\beta_1$ → decay factor for momentum (typical 0.9)  \n","- $\\nabla_\\theta L_t$ → gradient at step $t$\n","\n","$$\n","v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla_\\theta L_t)^2\n","$$\n","\n","- $v_t$ → “variance” term (second moment estimate)  \n","- $\\beta_2$ → decay factor for squared gradients (typical 0.999)  \n","- $(\\nabla_\\theta L_t)^2$ → elementwise square of gradient  \n","\n","2. **Bias correction** (optional but common):\n","\n","$$\n","\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\n","$$\n","\n","- Corrects the fact that $m_0$ and $v_0$ start at 0 → avoids underestimation early\n","\n","3. **Parameter update**:\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n","$$\n","\n","- $\\alpha$ → base learning rate  \n","- $\\epsilon$ → tiny number (e.g., $1e-8$) to avoid division by zero  \n","- Intuition: divide momentum by RMS (adaptive scaling)\n","\n","\n","**Intuition**\n","\n","- $m_t$ → keeps track of trend in gradients (like momentum)  \n","- $v_t$ → keeps track of how big gradients are \n","- $\\hat{m}_t / \\sqrt{\\hat{v}_t}$ → scales step proportionally to signal / noise  \n","- Large gradients → step reduced  \n","- Small gradients → step amplified  \n","\n","> Adam dynamically adjusts the effective learning rate for each weight\n","\n","### Tiny Example\n","\n","Suppose one weight $\\theta$ with gradients over 3 steps:\n","\n","| Step | Grad ($\\nabla_\\theta L$) |\n","|------|---------------------------|\n","| 1    | 0.2                       |\n","| 2    | 0.1                       |\n","| 3    | -0.05                     |\n","\n","Set $\\alpha=0.1$, $\\beta_1=0.9$, $\\beta_2=0.999$, $\\epsilon=1e-8$  \n","\n","**Step 1**:  \n","\n","$$\n","m_1 = 0.9*0 + 0.1*0.2 = 0.02\n","$$\n","$$\n","v_1 = 0.999*0 + 0.001*0.2^2 = 0.00004\n","$$\n","$$\n","\\hat{m}_1 = 0.02 / (1-0.9^1) = 0.2\n","$$\n","$$\n","\\hat{v}_1 = 0.00004 / (1-0.999^1) \\approx 0.04\n","$$\n","$$\n","\\theta_1 = \\theta_0 - 0.1 * 0.2 / \\sqrt{0.04} \\approx \\theta_0 - 0.1\n","$$\n","\n","**Step 2**: similar, $m_2$, $v_2$, compute bias-corrected estimates, update $\\theta_2$  \n","\n","**Step 3**: $m_3$, $v_3$, update $\\theta_3$  \n","\n","> Notice how updates scale automatically based on both trend and magnitude of gradients.\n","\n","\n","### **Pros**:\n","- Fast convergence  \n","- Works well with sparse gradients  \n","- Often “plug-and-play” without LR tuning\n","\n","### **Cons**:\n","- Can overfit (fast convergence → less implicit regularization)  \n","- Sometimes worse final accuracy than SGD+momentum  \n","- Implicit weight decay may interfere with explicit regularization\n","\n","\n","> Adam is a smart, adaptive SGD, it tracks both direction and scale of gradients, making it very effective, but you must still monitor overfitting and final generalization.\n"]},{"cell_type":"markdown","id":"c187a0f2","metadata":{"papermill":{"duration":0.00318,"end_time":"2026-02-08T09:49:40.544467","exception":false,"start_time":"2026-02-08T09:49:40.541287","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\">Adam Optimizer: Core Update Equations Breakdown</p>\n","\n","Adam combines **momentum (first moment)** with **RMSProp (second moment)** for adaptive, stable updates. Let’s go step by step.\n","\n","\n","#### Step 1. Compute Gradient\n","For a weight $\\theta$ at time step $t$:\n","\n","$$\n","g_t = \\nabla_\\theta L_t\n","$$\n","\n","- $g_t$ is the current gradient\n","- Represents how much the loss changes with respect to $\\theta$  \n","\n","\n","#### Step 2: First Moment (Momentum)\n","\n","$$\n","m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n","$$\n","\n","- $m_t$ = running average of past gradients (momentum)  \n","- $\\beta_1$ = decay factor for first moment (typical 0.9)  \n","- $(1-\\beta_1)$ = fraction of current gradient included  \n","- **Intuition:** smooths the gradient, capturing the direction of descent  \n","\n","> Like a “moving average” of gradients, it prevents oscillation in steep or noisy directions.\n","\n","#### Step 3: Second Moment (RMSProp)\n","\n","$$\n","v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n","$$\n","\n","- $v_t$ = running average of squared gradients \n","- $\\beta_2$ = decay factor for second moment (typical 0.999)  \n","- Tracks magnitude of gradients → adaptive scaling of updates  \n","- Squared gradient ensures we measure size only, ignoring direction\n","- **Intuition:** prevents steps from being too big on steep slopes and too small on flat slopes.\n","\n","#### Step 4: Bias Correction\n","\n","- At initialization: $m_0 = 0$, $v_0 = 0$  \n","- First few steps: $m_t$ and $v_t$ are biased toward zero because they are computed as:\n","\n","$$\n","m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n","$$\n","\n","- Step 1: $m_1 = (1-\\beta_1) g_1$ → much smaller than actual gradient magnitude  \n","- Step 2: $m_2 = \\beta_1 m_1 + (1-\\beta_1) g_2$ → still smaller than “true average”  \n","\n","> Without correction, early updates are tiny, slowing learning.\n","\n","Bias correction divides by $(1 - \\beta_1^t)$:\n","\n","$$\n","\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad\n","\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n","$$\n","\n","- $t$ = current time step  \n","- $(1 - \\beta^t)$ starts small → compensates for the initial “zero bias”  \n","\n","**Example:**  \n","\n","- $\\beta_1 = 0.9$, $t=1$  \n","- $1 - \\beta_1^1 = 0.1$ → divide by 0.1 → scale up $m_1$ 10×  \n","- Now $\\hat{m}_1 \\approx g_1$ → update is correct magnitude\n","\n","**Intuition**\n","\n","- Without correction: first updates are too small, learning is slow  \n","- With correction: first updates match the scale we expect, then later updates naturally settle  \n","- Bias correction is only significant in **early steps**  \n","- After many steps ($t \\to \\infty$), $\\beta^t \\to 0$, correction factor → 1  \n","- Then $\\hat{m}_t \\approx m_t$, $\\hat{v}_t \\approx v_t$  \n","\n","> Ensures Adam starts with correct step sizes immediately, preventing slow initial training.\n","\n","#### Step 5: Parameter Update\n","\n","$$\n","\\boxed{\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}}\n","$$\n","\n","- $\\alpha$ = learning rate  \n","- $\\hat{m}_t / \\sqrt{\\hat{v}_t}$ → **direction (momentum)** / **magnitude scaling (RMSProp)**  \n","- $\\epsilon$ = small number to avoid division by zero\n","- Intuition: move weight along a smoothed direction with adaptive step size.\n","\n","#### Summary of Roles\n","\n","| Term          | Role |\n","|---------------|------|\n","| $g_t$         | current gradient (signal) |\n","| $m_t, \\hat{m}_t$ | momentum → smooths direction |\n","| $v_t, \\hat{v}_t$ | second moment → adaptive step size |\n","| $\\alpha$      | base learning rate |\n","| $\\epsilon$    | numerical stability |\n","\n","Adam combines directional smoothing and per-parameter scaling, making it robust and fast for most deep learning tasks.\n","\n","---"]},{"cell_type":"markdown","id":"989ce0e1","metadata":{"papermill":{"duration":0.003102,"end_time":"2026-02-08T09:49:40.550808","exception":false,"start_time":"2026-02-08T09:49:40.547706","status":"completed"},"tags":[]},"source":["```python\n","\n","# PyTorch Adam\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","```"]},{"cell_type":"markdown","id":"5c3583e6","metadata":{"papermill":{"duration":0.003168,"end_time":"2026-02-08T09:49:40.557199","exception":false,"start_time":"2026-02-08T09:49:40.554031","status":"completed"},"tags":[]},"source":["## Adam Failure Modes\n","\n","- **Quick loss drop** → may give a false sense of convergence, but validation may still be poor  \n","- **Overfits small datasets** rapidly due to aggressive adaptive steps  \n","- **Incorrect weight decay / regularization** → can harm generalization\n","\n","> Adam is fast and adaptive, but does not automatically prevent overfitting.\n"]},{"cell_type":"markdown","id":"256a397f","metadata":{"papermill":{"duration":0.003384,"end_time":"2026-02-08T09:49:40.564584","exception":false,"start_time":"2026-02-08T09:49:40.5612","status":"completed"},"tags":[]},"source":["# AdamW (Adam with Decoupled Weight Decay)\n"]},{"cell_type":"markdown","id":"322ce378","metadata":{"papermill":{"duration":0.003126,"end_time":"2026-02-08T09:49:40.570936","exception":false,"start_time":"2026-02-08T09:49:40.56781","status":"completed"},"tags":[]},"source":["To be continue..."]},{"cell_type":"markdown","id":"56cb8adc","metadata":{"papermill":{"duration":0.003142,"end_time":"2026-02-08T09:49:40.57727","exception":false,"start_time":"2026-02-08T09:49:40.574128","status":"completed"},"tags":[]},"source":["---\n","<p style=\"text-align:center; color:skyblue; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":3.620531,"end_time":"2026-02-08T09:49:40.89989","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-08T09:49:37.279359","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}