{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-38-cnn-data-augmentation?scriptVersionId=299792234\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"b3fd50ea","metadata":{"papermill":{"duration":0.005319,"end_time":"2026-02-24T15:41:50.422918","exception":false,"start_time":"2026-02-24T15:41:50.417599","status":"completed"},"tags":[]},"source":["# Day 38: CNN Data Augmentatiom\n","\n","Welcome to Day 38!\n","\n","Today You'll Learn\n","\n","1. What data augmentation is\n","2. Why CNNs critically need augmentation\n","3. Geometric transformations (with math intuition)\n","4. Color & intensity transformations\n","5. Under-augmentation vs over-augmentation\n","6. How to design a professional augmentation pipeline\n","7. Domain-specific best practices\n","8. Theoretical insight: augmentation as manifold smoothing\n","\n","If you found this notebook helpful, your **<b style=\"color:skyblue;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"f0a56ba0","metadata":{"papermill":{"duration":0.004288,"end_time":"2026-02-24T15:41:50.431687","exception":false,"start_time":"2026-02-24T15:41:50.427399","status":"completed"},"tags":[]},"source":["# Data Augmentation\n","\n","Data augmentation means:\n","\n","> Create new training examples by slightly modifying existing ones  \n","> without changing their label.\n","\n","Data augmentation is a technique that artificially increases dataset diversity\n","by applying label-preserving transformations to training samples.\n","\n","Formally:\n","\n","Given dataset:\n","\n","$$D = {(xáµ¢, yáµ¢)} \\text{ for } i = 1 \\text{ to } N$$\n","\n","We apply a transformation T such that:\n","\n","$$y(x) = y(T(x))$$\n","\n","The label must remain unchanged.\n","\n","**Goal:**\n","\n","Increase effective dataset size without collecting new data:\n","\n","$$\n","N â†’N_{effective} \\gg N\n","$$\n"]},{"cell_type":"markdown","id":"0942e502","metadata":{"papermill":{"duration":0.005387,"end_time":"2026-02-24T15:41:50.441043","exception":false,"start_time":"2026-02-24T15:41:50.435656","status":"completed"},"tags":[]},"source":["## Simple Example\n","\n","You have this image:\n","\n","Cat ðŸ± â†’ label = \"cat\"\n","\n","Now you:\n","\n","- Flip it horizontally  \n","- Rotate it slightly  \n","- Change brightness  \n","- Crop a little  \n","\n","Itâ€™s still a cat.\n","\n","So now instead of 1 image,\n","you effectively have many variations of the same image.\n","\n","That is data augmentation."]},{"cell_type":"markdown","id":"04d04b74","metadata":{"papermill":{"duration":0.00388,"end_time":"2026-02-24T15:41:50.448838","exception":false,"start_time":"2026-02-24T15:41:50.444958","status":"completed"},"tags":[]},"source":["## Why Do This?\n","\n","Because models donâ€™t just memorize pixels.\n","\n","They must learn patterns.\n","\n","If your dataset is small:\n","\n","- Model overfits\n","- Model memorizes instead of generalizing\n","\n","Augmentation makes the model see:\n","\n","- Cats in different positions\n","- Different lighting\n","- Slight distortions\n","\n","So it learns the concept of â€œcatâ€,\n","not just specific pixels."]},{"cell_type":"markdown","id":"4336ed57","metadata":{"papermill":{"duration":0.003926,"end_time":"2026-02-24T15:41:50.456673","exception":false,"start_time":"2026-02-24T15:41:50.452747","status":"completed"},"tags":[]},"source":["## What â€œLabel-Preservingâ€ Means\n","\n","If original sample is:\n","\n","Image of 3 â†’ label = 3\n","\n","After rotating slightly:\n","\n","Still 3.\n","\n","But if you rotate 180Â°:\n","\n","It might look like something else.\n","\n","That would break the rule.\n","\n","So transformation must not change the true class.\n","\n","Thatâ€™s what this means:\n","\n","$$\n","y(x) = y(T(x))\n","$$\n","\n","The label before and after transformation stays the same."]},{"cell_type":"markdown","id":"160abdaa","metadata":{"papermill":{"duration":0.003944,"end_time":"2026-02-24T15:41:50.464521","exception":false,"start_time":"2026-02-24T15:41:50.460577","status":"completed"},"tags":[]},"source":["## What â€œIncrease Effective Dataset Sizeâ€ Means\n","\n","Suppose:\n","\n","You have 1,000 images.\n","\n","With augmentation during training:\n","\n","Each epoch, images look slightly different.\n","\n","So the model might effectively see:\n","\n","10,000+ variations.\n","\n","You didnâ€™t collect new data.\n","\n","But the model experiences more diversity.\n","\n","Thatâ€™s why:\n","\n","$$\n","N_{effective} \\gg N\n","$$\n"]},{"cell_type":"markdown","id":"a4ab50ad","metadata":{"papermill":{"duration":0.003913,"end_time":"2026-02-24T15:41:50.47235","exception":false,"start_time":"2026-02-24T15:41:50.468437","status":"completed"},"tags":[]},"source":["## Real-World Intuition\n","\n","Without augmentation:\n","Model learns:\n","> â€œDogs look exactly like these 1,000 pictures.â€\n","\n","With augmentation:\n","Model learns:\n","> â€œDogs can appear in many positions, angles, and lighting conditions.â€\n","\n","That improves generalization.\n","\n","**One-Line Definition (Simple):**\n","\n","> Data augmentation is creating modified versions of training data that keep the same label, so the model sees more variety and generalizes better.\n"]},{"cell_type":"markdown","id":"e9567d72","metadata":{"papermill":{"duration":0.004014,"end_time":"2026-02-24T15:41:50.480333","exception":false,"start_time":"2026-02-24T15:41:50.476319","status":"completed"},"tags":[]},"source":["# Geometric Transformations\n","\n","Geometric augmentations modify the **spatial arrangement** of pixels while keeping the object class unchanged.\n","\n","They do not alter what the object is, they alter how it is positioned."]},{"cell_type":"markdown","id":"164e7db2","metadata":{"papermill":{"duration":0.003853,"end_time":"2026-02-24T15:41:50.488067","exception":false,"start_time":"2026-02-24T15:41:50.484214","status":"completed"},"tags":[]},"source":["## Horizontal Flip\n","\n","A horizontal flip mirrors the image across the vertical axis.\n","\n","If image width is $W$, a pixel at:\n","\n","$$\n","(x, y)\n","$$\n","\n","becomes:\n","\n","$$\n","(W - x, y)\n","$$\n","\n","So left becomes right, and right becomes left.\n","\n","### Visual Intuition\n","\n","Original image: object facing right  \n","After flip: object facing left  \n","\n","The object identity remains the same.\n","\n","### When It Is Good\n","\n","Use horizontal flip when orientation does **not define the class**.\n","\n","Strong use cases:\n","\n","- Natural images  \n","- Object recognition  \n","- Animals  \n","- Vehicles  \n","- Generic object detection  \n","\n","Reason:\n","\n","Most real-world objects can appear facing either direction.\n","\n","Flipping increases orientation diversity.\n","\n","### When You Should Avoid It\n","\n","Do NOT use it when horizontal orientation carries meaning.\n","\n","Examples:\n","\n","- Text recognition (text becomes reversed)\n","- Digit classification ($6 \\leftrightarrow 9$ confusion)\n","- Medical imaging (left anatomy $\\neq$ right anatomy)\n","- Traffic signs with directional arrows\n","\n","In these cases, flipping changes semantics.\n","\n","That violates the label-preserving rule:\n","\n","$$\n","y(x) = y(T(x))\n","$$\n","\n","### PyTorch Implementation\n","\n","```python\n","transforms.RandomHorizontalFlip(p=0.5)\n","```\n","**Meaning:**\n","\n","- Each image has 50% probability of being flipped\n","- Augmentation is applied during training only"]},{"cell_type":"markdown","id":"debb0807","metadata":{"papermill":{"duration":0.003785,"end_time":"2026-02-24T15:41:50.495718","exception":false,"start_time":"2026-02-24T15:41:50.491933","status":"completed"},"tags":[]},"source":["## Random Rotation\n","\n","Rotate image by an angle $\\theta$.\n","\n","The rotation transformation is:\n","\n","$$\n","\\begin{bmatrix}\n","x' \\\\\n","y'\n","\\end{bmatrix}\n","=\n","\\begin{bmatrix}\n","\\cos\\theta & -\\sin\\theta \\\\\n","\\sin\\theta & \\cos\\theta\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","x \\\\\n","y\n","\\end{bmatrix}\n","$$\n","\n","Each pixel coordinate is rotated around the image center.\n","\n","### Purpose\n","\n","Build **rotational invariance**.\n","\n","The model learns that small orientation changes do not change class identity.\n","\n","### Safe Range\n","\n","For natural images:\n","\n","$$\n","5^\\circ \\text{ to } 20^\\circ\n","$$\n","\n","Small rotations simulate camera tilt.  \n","Large rotations may break semantics (e.g., digits or medical scans).\n","\n","\n","### PyTorch Implementation\n","\n","```python\n","transforms.RandomRotation(degrees=15)\n","```\n","\n","Images are randomly rotated within:\n","\n","$$\n","[-15^\\circ, +15^\\circ]\n","$$"]},{"cell_type":"markdown","id":"b3841ea0","metadata":{"papermill":{"duration":0.00383,"end_time":"2026-02-24T15:41:50.503593","exception":false,"start_time":"2026-02-24T15:41:50.499763","status":"completed"},"tags":[]},"source":["## Random Resized Crop\n","\n","Randomly:\n","\n","1. Select a region of the image\n","2. Crop it\n","3. Resize it to a fixed resolution\n","\n","If the original image is $H \\times W$,\n","a sub-region is sampled with scale:\n","\n","$$\n","s \\in [s_{min}, s_{max}]\n","$$\n","\n","Then resized (e.g., to $224 \\times 224$).\n","\n","### Effect\n","\n","* Forces focus on partial features\n","* Encourages scale robustness\n","* Reduces reliance on full global context\n","\n","The model must recognize objects even if partially visible.\n","\n","### PyTorch Implementation\n","\n","```python\n","transforms.RandomResizedCrop(224, scale=(0.8, 1.0))\n","```\n","\n","**Meaning:**\n","\n","* Crop between $80%$ and $100%$ of original area\n","* Resize to $224 \\times 224$"]},{"cell_type":"markdown","id":"683d8c31","metadata":{"papermill":{"duration":0.003946,"end_time":"2026-02-24T15:41:50.511391","exception":false,"start_time":"2026-02-24T15:41:50.507445","status":"completed"},"tags":[]},"source":["Suppose original image is:\n","\n","400 Ã— 400\n","\n","And you use:\n","\n","```python\n","transforms.RandomResizedCrop(224, scale=(0.8, 1.0))\n","```\n","\n","**Step 1: Pick Random Area**\n","\n","Scale = (0.8, 1.0)\n","\n","This means:\n","\n","Crop between:\n","\n","$$\n","80% \\text{ and } 100%\n","$$\n","\n","of the original image area.\n","\n","If original area is:\n","\n","$$\n","400 \\times 400 = 160000\n","$$\n","\n","Then crop area will be between:\n","\n","$$\n","128000 \\text{ and } 160000\n","$$\n","\n","So maybe it selects a 360 Ã— 360 region.<br>\n","Or maybe 380 Ã— 380.<br>\n","Or maybe full image.\n","\n","**Step 2: Crop That Region**\n","\n","It cuts out that selected box.\n","\n","Now you have something smaller than original.\n","\n","**Step 3: Resize to 224 Ã— 224**\n","\n","No matter what was cropped,\n","it gets resized to:\n","\n","$$\n","224 \\times 224\n","$$\n","\n","So final input size stays consistent.\n","\n","### Why This Helps\n","\n","### Without It\n","\n","Model always sees:\n","\n","* Entire object\n","* Centered\n","* Same scale\n","\n","It may memorize layout.\n","\n","### With RandomResizedCrop\n","\n","Sometimes:\n","\n","* Only part of object is visible\n","* Object appears zoomed in\n","* Object appears slightly zoomed out\n","\n","Now model must learn:\n","\n","> â€œEven if I only see part of the dog, itâ€™s still a dog.â€\n","\n","This builds:\n","\n","* Scale robustness\n","* Partial feature recognition\n","* Better generalization\n","\n","\n","### Simple Visual Intuition\n","\n","Original:\n","\n","[     Full Dog Image     ]\n","\n","\n","After random crop:\n","\n","\n","[   Only Dog Face   ]\n","\n","\n","Resized back to full size.\n","\n","Model learns to recognize from partial cues.\n","\n","\n","\n","\n","RandomResizedCrop =\n","\n","> Random zoom + random crop + resize back to fixed size."]},{"cell_type":"markdown","id":"4d71c4fe","metadata":{"execution":{"iopub.execute_input":"2026-02-23T03:30:40.016259Z","iopub.status.busy":"2026-02-23T03:30:40.015238Z","iopub.status.idle":"2026-02-23T03:30:40.02781Z","shell.execute_reply":"2026-02-23T03:30:40.026628Z","shell.execute_reply.started":"2026-02-23T03:30:40.016219Z"},"papermill":{"duration":0.004007,"end_time":"2026-02-24T15:41:50.519875","exception":false,"start_time":"2026-02-24T15:41:50.515868","status":"completed"},"tags":[]},"source":["## Random Affine (Translation)\n","\n","It just means:\n","\n","> Move the image slightly left/right or up/down.\n","\n","Nothing else changes.\n","\n","### Simple Example\n","\n","Original image:\n","\n","[Dog is centered]\n","\n","After translation:\n","\n","[Dog shifted left]\n","\n","Or:\n","\n","[Dog shifted right]\n","\n","Still a dog.\n","\n","Same label.\n","\n","### Math Formula\n","\n","Translate image coordinates:\n","\n","$$\n","(x, y) \\rightarrow (x + \\Delta x, y + \\Delta y)\n","$$\n","\n","Where:\n","\n","$$\n","\\Delta x, \\Delta y\n","$$\n","\n","are small horizontal and vertical shifts.\n","\n","This just says:\n","\n","Each pixel moves by a small amount.\n","\n","- $\\Delta x$ = how much we move left/right  \n","- $\\Delta y$ = how much we move up/down  \n","\n","If $\\Delta x = 10$, every pixel shifts 10 pixels to the right.\n","\n","### PyTorch Implementation\n","\n","```python\n","transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))\n","```\n","\n","Meaning:\n","\n","* `degrees=0` â†’ no rotation\n","* `translate=(0.1, 0.1)` â†’ can move image up to: $10%$ of width and height.\n","\n","If image is 200 Ã— 200:\n","\n","* Max horizontal shift = 20 pixels\n","* Max vertical shift = 20 pixels\n","\n","Each image is randomly shifted within that range.\n","\n","### Why Do This?\n","\n","Without translation:\n","\n","Model may learn:\n","\n","> â€œDog is always in center.â€\n","\n","Thatâ€™s dangerous.\n","\n","In real life:\n","\n","* Objects appear anywhere in frame.\n","* They are not perfectly centered.\n","\n","Translation teaches:\n","\n","> Position does NOT define the class.\n","\n","This builds **location invariance**\n","\n","### Real Benefit\n","\n","If you donâ€™t use translation:\n","\n","Model might fail when object appears near edge.\n","\n","If you use translation:\n","\n","Model becomes robust to placement changes.\n","\n","Random translation =\n","\n","> Slightly move the image so the model doesnâ€™t depend on object position."]},{"cell_type":"markdown","id":"aa557c3e","metadata":{"papermill":{"duration":0.003875,"end_time":"2026-02-24T15:41:50.527743","exception":false,"start_time":"2026-02-24T15:41:50.523868","status":"completed"},"tags":[]},"source":["# Color & Intensity Transformations\n","\n","These augmentations change **pixel values**, not spatial structure.\n","\n","Geometric transforms move pixels.  \n","Color transforms modify their intensity.\n","\n","They simulate:\n","\n","- Lighting variation  \n","- Camera differences  \n","- Sensor noise  \n","- Exposure shifts  \n","\n","The object stays the same.  \n","Only how it appears changes.\n","\n","\n","### Why This Matters\n","\n","In real life:\n","\n","- Same object looks different at night\n","- Different cameras produce different colors\n","- Shadows change brightness\n","- Weather affects contrast\n","\n","If your model only sees perfect lighting,\n","it will fail in the real world.\n","\n","Color augmentation teaches:\n","\n","> Class identity is independent of lighting conditions."]},{"cell_type":"markdown","id":"ff3cacc8","metadata":{"papermill":{"duration":0.003929,"end_time":"2026-02-24T15:41:50.535584","exception":false,"start_time":"2026-02-24T15:41:50.531655","status":"completed"},"tags":[]},"source":["## Color Jitter\n","\n","Color Jitter randomly changes image appearance without changing object identity.\n","\n","It perturbs:\n","\n","- **Brightness**\n","- **Contrast**\n","- **Saturation**\n","- **Hue**\n","\n","No geometry changes.  \n","Only pixel intensities change.\n","\n","### Brightness\n","\n","Makes the image lighter or darker.\n","\n","Think:\n","\n","Daylight vs cloudy vs indoor lighting.\n","\n","Mathematically (simplified):\n","\n","$$\n","x' = x \\cdot \\alpha\n","$$\n","\n","Where $\\alpha$ slightly increases or decreases intensity.\n","\n","### Contrast\n","\n","Makes dark areas darker and bright areas brighter (or flattens them).\n","\n","High contrast â†’ sharp image  \n","Low contrast â†’ washed out image  \n","\n","### Saturation\n","\n","Changes color intensity.\n","\n","High saturation â†’ vivid colors  \n","Low saturation â†’ closer to grayscale  \n","\n","### Hue\n","\n","Shifts color tones slightly.\n","\n","Red may shift toward orange.  \n","Blue may shift toward cyan.\n","\n","Small changes only, large shifts can break realism.\n","\n","\n","### Why Use It?\n","\n","Without Color Jitter:\n","\n","Model might learn:\n","\n","> â€œDogs appear only in bright outdoor lighting.â€\n","\n","Thatâ€™s fragile.\n","\n","With Color Jitter:\n","\n","Model learns:\n","\n","> â€œLighting does not define the class.â€\n","\n","It improves lighting robustness.\n","\n","\n","### PyTorch Example\n","\n","```python\n","transforms.ColorJitter(\n","    brightness=0.2,\n","    contrast=0.2,\n","    saturation=0.2,\n","    hue=0.1\n",")\n","```\n","\n","**What This Means**\n","\n","* Brightness varies by Â±20%\n","* Contrast varies by Â±20%\n","* Saturation varies by Â±20%\n","* Hue shifts slightly (Â±0.1 range)\n","\n","Each image gets random variation during training.\n","\n","\n","### When Not to Use Aggressively\n","\n","Avoid strong jitter if:\n","\n","* Color is a key signal (e.g., fruit ripeness)\n","* Medical imaging where intensity carries diagnosis info\n","* Industrial defect detection where shade matters\n","\n","> Color Jitter teaches the model to ignore lighting and color variations so it focuses on structure and shape instead.\n"]},{"cell_type":"markdown","id":"07242ed8","metadata":{"papermill":{"duration":0.004146,"end_time":"2026-02-24T15:41:50.543961","exception":false,"start_time":"2026-02-24T15:41:50.539815","status":"completed"},"tags":[]},"source":["## Gaussian Noise\n","\n","Gaussian Noise means:\n","\n","> Add small random values to every pixel.\n","\n","Mathematically:\n","\n","$$\n","\\tilde{x} = x + \\varepsilon\n","$$\n","\n","Where noise is sampled from:\n","\n","$$\n","\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\n","$$\n","\n","This means:\n","\n","- Mean = $0$ (no systematic shift)\n","- Variance = $\\sigma^2$ (controls noise strength)\n","\n","### What This Actually Means\n","\n","Each pixel gets slightly disturbed.\n","\n","If original pixel value is:\n","\n","$$\n","x = 0.6\n","$$\n","\n","And sampled noise is:\n","\n","$$\n","\\varepsilon = 0.02\n","$$\n","\n","Then:\n","\n","$$\n","\\tilde{x} = 0.62\n","$$\n","\n","Some pixels increase.  \n","Some decrease.\n","\n","Randomly.\n","\n","\n","### Visual Intuition\n","\n","Original image:\n","\n","**[Smooth]**\n","\n","After Gaussian noise:\n","\n","**[Slightly grainy like low-light camera noise]**\n","\n","Object identity remains the same.\n","\n","### Why It Helps\n","\n","**1ï¸. Improves Robustness**\n","\n","Real-world cameras produce:\n","\n","- Sensor noise\n","- Compression artifacts\n","- Low-light grain\n","\n","Noise augmentation prepares the model for that.\n","\n","**2ï¸. Acts Like Regularization**\n","\n","Noise forces the model to:\n","\n","- Not depend on exact pixel values\n","- Learn stable patterns instead of memorizing details\n","\n","Itâ€™s similar in spirit to dropout but applied to input.\n","\n","**3ï¸. Encourages Smoother Decision Boundaries**\n","\n","If small input changes donâ€™t change prediction,\n","the model becomes more stable.\n","\n","Noise training encourages this property.\n","\n","### Choosing $\\sigma$\n","\n","Small $\\sigma$ (e.g., 0.01â€“0.05):\n","\n","- Subtle noise\n","- Safe for most tasks\n","\n","Large $\\sigma$:\n","\n","- May destroy image structure\n","- Can hurt performance\n","\n","Noise must simulate realism not corruption.\n","\n","> Gaussian noise adds small random pixel disturbances so the model becomes robust to real-world imperfections and avoids overfitting to clean data.\n"]},{"cell_type":"markdown","id":"08f5a878","metadata":{"papermill":{"duration":0.004081,"end_time":"2026-02-24T15:41:50.55208","exception":false,"start_time":"2026-02-24T15:41:50.547999","status":"completed"},"tags":[]},"source":["## Random Erasing\n","\n","Random Erasing means:\n","\n","> Randomly remove (mask) a small rectangular region of the image during training.\n","\n","It replaces that region with:\n","\n","- Zeros  \n","- Random noise  \n","- Or constant value  \n","\n","The rest of the image remains unchanged.\n","\n","### What Actually Happens\n","\n","Original:\n","\n","[full image]\n","\n","After random erasing:\n","\n","[â–ˆâ–ˆâ–ˆ image]\n","\n","A small rectangular patch is erased.\n","\n","The object is partially occluded.\n","\n","### Why This Helps\n","\n","**1ï¸. Forces Redundant Feature Learning**\n","\n","Without erasing:\n","\n","Model might rely on one strong cue:\n","\n","> â€œI detect dog because of the face.â€\n","\n","With erasing:\n","\n","Sometimes the face is gone.\n","\n","Model must learn:\n","\n","- Body shape  \n","- Fur texture  \n","- Context  \n","\n","It distributes importance across multiple features.\n","\n","**2ï¸. Prevents Shortcut Learning**\n","\n","Neural networks love shortcuts.\n","\n","If one small region is highly predictive,\n","they over-rely on it.\n","\n","Random erasing removes that crutch.\n","\n","**3ï¸. Simulates Real-World Occlusion**\n","\n","In reality:\n","\n","- Objects get partially blocked\n","- Shadows hide regions\n","- Objects overlap\n","\n","Random Erasing simulates that.\n","\n","### PyTorch Example\n","\n","```python\n","transforms.RandomErasing(p=0.5)\n","```\n","\n","Meaning:\n","\n","* 50% probability of applying erasing\n","* Patch size and aspect ratio are randomly sampled\n","\n","Applied after converting image to tensor.\n","\n","### When To Be Careful\n","\n","Avoid aggressive erasing if:\n","\n","* Small objects dominate image\n","* Critical class info exists in tiny region\n","* Dataset is already very small\n","\n","Too much erasing can destroy learning signal.\n","\n","> Random Erasing hides random patches so the model learns to recognize objects even when parts are missing."]},{"cell_type":"markdown","id":"c6d8ca86","metadata":{"papermill":{"duration":0.003931,"end_time":"2026-02-24T15:41:50.560173","exception":false,"start_time":"2026-02-24T15:41:50.556242","status":"completed"},"tags":[]},"source":["# Under-Augmentation vs Over-Augmentation\n","\n","Augmentation is not â€œmore is better.â€\n","\n","It is a calibration problem.\n"]},{"cell_type":"markdown","id":"7cd3d6ef","metadata":{"papermill":{"duration":0.003896,"end_time":"2026-02-24T15:41:50.568","exception":false,"start_time":"2026-02-24T15:41:50.564104","status":"completed"},"tags":[]},"source":["## Under-Augmentation\n","\n","### Symptoms\n","\n","- High training accuracy  \n","- Low validation accuracy  \n","\n","This gap indicates: $\\text{Overfitting}$\n","\n","The model memorizes training distribution but fails to generalize.\n","\n","### Root Cause\n","\n","Your dataset does not contain enough variation.\n","\n","The model learns:\n","\n","- Background shortcuts  \n","- Specific lighting  \n","- Specific object positions  \n","- Dataset biases  \n","\n","Instead of learning robust features.\n","\n","### Fix\n","\n","Increase augmentation gradually.\n","\n","Do not randomly stack everything.\n","\n","Increase diversity in this order:\n","\n","1. Horizontal flip  \n","2. Mild rotation ($\\pm 10^\\circ$)  \n","3. RandomResizedCrop  \n","4. Light ColorJitter  \n","\n","Monitor validation improvement.\n","\n","If validation rises while training drops slightly â†’ good sign."]},{"cell_type":"markdown","id":"c93689b1","metadata":{"papermill":{"duration":0.004107,"end_time":"2026-02-24T15:41:50.576183","exception":false,"start_time":"2026-02-24T15:41:50.572076","status":"completed"},"tags":[]},"source":["## Over-Augmentation\n","\n","### Symptoms\n","\n","- Training accuracy very low  \n","- Loss does not decrease  \n","- Slow or unstable convergence  \n","\n","Sometimes:\n","\n","$$\n","\\text{Training accuracy} \\approx \\text{Validation accuracy}\n","$$\n","\n","But both are poor.\n","\n","### Root Cause\n","\n","Your transformations break realism.\n","\n","Examples:\n","\n","- Rotation of $90^\\circ$ for natural objects  \n","- Crop scale too small (object disappears)  \n","- Heavy hue shift  \n","- Large erasing patches  \n","\n","The model cannot learn consistent patterns.\n","\n","You are corrupting signal, not regularizing it.\n","\n","### Fix\n","\n","Reduce magnitude, not necessarily type.\n","\n","Instead of:\n","\n","- Rotation = $45^\\circ$ â†’ try $10^\\circ$  \n","- Crop scale = $(0.2, 1.0)$ â†’ try $(0.8, 1.0)$  \n","- Brightness = $0.8$ â†’ try $0.2$  \n","\n","Think in terms of realism:\n","\n","> Would this variation appear in the real world?\n","\n","If not, remove it.\n"]},{"cell_type":"markdown","id":"0a0f1f91","metadata":{"papermill":{"duration":0.003853,"end_time":"2026-02-24T15:41:50.584057","exception":false,"start_time":"2026-02-24T15:41:50.580204","status":"completed"},"tags":[]},"source":["# The Balance Principle\n","\n","Augmentation is distribution engineering.\n","\n","You are reshaping the training distribution:\n","\n","$$\n","P_{train}(x) \\rightarrow \\tilde{P}_{train}(x)\n","$$\n","\n","The goal is not distortion.\n","\n","The goal is alignment with the real-world distribution:\n","\n","$$\n","\\tilde{P}_{train}(x) \\approx P_{real}(x)\n","$$\n","\n","\n","### Good Augmentation\n","\n","#### Expands Distribution\n","\n","Introduces controlled variability:\n","\n","- Position  \n","- Scale  \n","- Lighting  \n","- Partial visibility  \n","\n","This reduces overfitting.\n","\n","Training becomes slightly harder.\n","Generalization improves.\n","\n","#### Preserves Label Meaning\n","\n","For every transformation $T$:\n","\n","$$\n","y(x) = y(T(x))\n","$$\n","\n","Class identity must remain unchanged.\n","\n","If semantics change, augmentation becomes label noise.\n","\n","#### Mimics Real-World Variability\n","\n","Ask:\n","\n","> Could this variation naturally occur?\n","\n","Examples:\n","\n","- Slight camera tilt â†’ Yes  \n","- Mild lighting variation â†’ Yes  \n","- 180Â° upside-down dog â†’ Usually no  \n","\n","Realism is the constraint.\n","\n","\n","### Bad Augmentation\n","\n","#### Alters Semantics\n","\n","Examples:\n","\n","- Flipping digits ($6 \\leftrightarrow 9$)  \n","- Flipping medical left/right anatomy  \n","- Excessive rotation in structured data  \n","\n","This injects incorrect supervision.\n","\n","#### Creates Unrealistic Samples\n","\n","Examples:\n","\n","- Extreme hue shifts  \n","- Heavy cropping removing object  \n","- Massive noise  \n","\n","You train on data that never occurs in reality.\n","\n","Model learns to solve artificial distortions.\n","\n","#### Causes Underfitting\n","\n","If augmentation is too strong:\n","\n","- Training accuracy stays low  \n","- Model struggles to converge  \n","\n","You increased difficulty beyond learnable signal.\n","\n","### Strategic Rule\n","\n","Good augmentation:\n","\n","- Slightly lowers training accuracy  \n","- Raises validation accuracy  \n","\n","Bad augmentation:\n","\n","- Lowers both  \n","\n","> Augmentation should make learning harder but reality-aligned. Not chaotic. Not destructive.\n","\n"]},{"cell_type":"markdown","id":"2043d23f","metadata":{"papermill":{"duration":0.003842,"end_time":"2026-02-24T15:41:50.591787","exception":false,"start_time":"2026-02-24T15:41:50.587945","status":"completed"},"tags":[]},"source":["# Practical CNN Augmentation Pipeline\n","\n","This is a realistic, production-style pipeline for image classification.\n","\n","It separates:\n","\n","- Training distribution shaping  \n","- Validation distribution evaluation  \n","\n","That separation is critical."]},{"cell_type":"markdown","id":"44a047e7","metadata":{"papermill":{"duration":0.003939,"end_time":"2026-02-24T15:41:50.59973","exception":false,"start_time":"2026-02-24T15:41:50.595791","status":"completed"},"tags":[]},"source":["## Training Pipeline\n","\n","```python\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.ColorJitter(0.2,0.2,0.2,0.05),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std),\n","    transforms.RandomErasing(p=0.3)\n","])\n","```\n","\n","Letâ€™s decode what this actually does.\n","\n","`RandomResizedCrop(224)`\n","\n","* Random zoom\n","* Random crop\n","* Output size = $224 \\times 224$\n","\n","Builds:\n","\n","* Scale robustness\n","* Partial feature learning\n","\n","\n","`RandomHorizontalFlip()`\n","\n","* 50% probability flip\n","\n","Builds:\n","\n","* Orientation invariance\n","\n","`RandomRotation(10)`\n","\n","* Rotate within:\n","\n","$$\n","[-10^\\circ, +10^\\circ]\n","$$\n","\n","Builds:\n","\n","* Mild rotational robustness\n","\n","Small angle = realistic camera tilt.\n","\n","\n","`ColorJitter(0.2, 0.2, 0.2, 0.05)`\n","\n","Random changes in:\n","\n","* Brightness (Â±20%)\n","* Contrast (Â±20%)\n","* Saturation (Â±20%)\n","* Hue (small shift)\n","\n","Builds:\n","\n","* Lighting robustness\n","* Sensor variability tolerance\n","\n","`ToTensor()`\n","\n","Converts image to tensor format:\n","\n","$$\n","H \\times W \\times C \\rightarrow C \\times H \\times W\n","$$\n","\n","\n","`Normalize(mean, std)`\n","\n","Standardizes pixel values:\n","\n","$$\n","x' = \\frac{x - \\mu}{\\sigma}\n","$$\n","\n","Why?\n","\n","* Stabilizes optimization\n","* Matches pretrained model expectations\n","\n","`RandomErasing(p=0.3)`\n","\n","30% chance to erase small patch.\n","\n","Builds:\n","\n","* Occlusion robustness\n","* Redundant feature learning\n","\n","Applied after normalization because it operates on tensors."]},{"cell_type":"markdown","id":"7bcdaa07","metadata":{"papermill":{"duration":0.003861,"end_time":"2026-02-24T15:41:50.607486","exception":false,"start_time":"2026-02-24T15:41:50.603625","status":"completed"},"tags":[]},"source":["## Validation Pipeline (No Augmentation)\n","\n","```python\n","val_transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)\n","])\n","```\n","\n","This is deterministic.\n","\n","No randomness.\n","\n","Why?\n","\n","Validation must estimate:\n","\n","$$\n","\\text{True generalization performance}\n","$$\n","\n","If you augment validation data:\n","\n","* You inject artificial noise\n","* Metrics become unstable\n","* Comparisons become meaningless\n","\n","### Why Resize(256) Then CenterCrop(224)?\n","\n","Common ImageNet-style preprocessing:\n","\n","1. Resize shorter side to 256\n","2. Center crop 224\n","\n","This preserves aspect ratio before cropping.\n","\n","More stable than direct resizing to 224.\n","\n","### Strategic Design Logic\n","\n","Training pipeline:\n","\n","> Introduce controlled chaos.\n","\n","Validation pipeline:\n","\n","> Measure clean performance."]},{"cell_type":"markdown","id":"e748ed02","metadata":{"papermill":{"duration":0.004023,"end_time":"2026-02-24T15:41:50.615422","exception":false,"start_time":"2026-02-24T15:41:50.611399","status":"completed"},"tags":[]},"source":["# Domain-Specific Guidelines\n","\n","| Domain | Recommended | Avoid |\n","|--------|------------|--------|\n","| Natural images | Flip, crop, mild rotation | Extreme rotation |\n","| Medical imaging | Small rotation | Flip if anatomy asymmetric |\n","| OCR/Text | Small rotation | Horizontal flip |\n","| Aerial images | Rotation up to 360Â° | Unrealistic color shifts |\n"]},{"cell_type":"markdown","id":"bbe110af","metadata":{"papermill":{"duration":0.004342,"end_time":"2026-02-24T15:41:50.623746","exception":false,"start_time":"2026-02-24T15:41:50.619404","status":"completed"},"tags":[]},"source":["# Augmentation as Manifold Smoothing\n","\n","Without augmentation:\n","Decision boundary becomes sharp around training points.\n","\n","With augmentation:\n","Model learns neighborhood around each sample.\n","\n","This smooths the decision boundary and improves generalization.\n","\n","Augmentation approximates the local data manifold.\n"]},{"cell_type":"markdown","id":"e8e71e17","metadata":{"papermill":{"duration":0.004301,"end_time":"2026-02-24T15:41:50.632196","exception":false,"start_time":"2026-02-24T15:41:50.627895","status":"completed"},"tags":[]},"source":["# Key Takeaways from Day 38\n","\n","- Augmentation increases effective dataset size\n","- Geometric transforms build spatial invariance\n","- Color transforms build lighting robustness\n","- Over-augmentation leads to underfitting\n","- Validation must remain clean\n","- Domain knowledge determines augmentation strength\n","- Augmentation often improves generalization more than regularization"]},{"cell_type":"markdown","id":"ceee104a","metadata":{"papermill":{"duration":0.004023,"end_time":"2026-02-24T15:41:50.640555","exception":false,"start_time":"2026-02-24T15:41:50.636532","status":"completed"},"tags":[]},"source":["---\n","\n","<p style=\"text-align:center; color:skyblue; font-size:18px;\">\n","Â© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31286,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":3.861056,"end_time":"2026-02-24T15:41:51.064325","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-24T15:41:47.203269","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}