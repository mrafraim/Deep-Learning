{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-38-cnn-data-augmentation?scriptVersionId=299449536\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"863a1809","metadata":{"papermill":{"duration":0.0035,"end_time":"2026-02-23T08:14:30.579982","exception":false,"start_time":"2026-02-23T08:14:30.576482","status":"completed"},"tags":[]},"source":["# Day 38: CNN Data Augmentatiom\n","\n","Welcome to Day 38!\n","\n","Today You'll Learn\n","\n","1. What data augmentation is\n","2. Why CNNs critically need augmentation\n","3. Geometric transformations (with math intuition)\n","4. Color & intensity transformations\n","5. Under-augmentation vs over-augmentation\n","6. How to design a professional augmentation pipeline\n","7. Domain-specific best practices\n","8. Theoretical insight: augmentation as manifold smoothing\n","\n","If you found this notebook helpful, your **<b style=\"color:skyblue;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"5b781048","metadata":{"papermill":{"duration":0.002429,"end_time":"2026-02-23T08:14:30.585069","exception":false,"start_time":"2026-02-23T08:14:30.58264","status":"completed"},"tags":[]},"source":["# Data Augmentation\n","\n","Data augmentation means:\n","\n","> Create new training examples by slightly modifying existing ones  \n","> without changing their label.\n","\n","Data augmentation is a technique that artificially increases dataset diversity\n","by applying label-preserving transformations to training samples.\n","\n","Formally:\n","\n","Given dataset:\n","\n","$$D = {(xáµ¢, yáµ¢)} \\text{ for } i = 1 \\text{ to } N$$\n","\n","We apply a transformation T such that:\n","\n","$$y(x) = y(T(x))$$\n","\n","The label must remain unchanged.\n","\n","**Goal:**\n","\n","Increase effective dataset size without collecting new data:\n","\n","$$\n","N â†’N_{effective} \\gg N\n","$$\n"]},{"cell_type":"markdown","id":"f6fbffb3","metadata":{"papermill":{"duration":0.002411,"end_time":"2026-02-23T08:14:30.589915","exception":false,"start_time":"2026-02-23T08:14:30.587504","status":"completed"},"tags":[]},"source":["## Simple Example\n","\n","You have this image:\n","\n","Cat ðŸ± â†’ label = \"cat\"\n","\n","Now you:\n","\n","- Flip it horizontally  \n","- Rotate it slightly  \n","- Change brightness  \n","- Crop a little  \n","\n","Itâ€™s still a cat.\n","\n","So now instead of 1 image,\n","you effectively have many variations of the same image.\n","\n","That is data augmentation."]},{"cell_type":"markdown","id":"cb6e19cd","metadata":{"papermill":{"duration":0.002338,"end_time":"2026-02-23T08:14:30.594592","exception":false,"start_time":"2026-02-23T08:14:30.592254","status":"completed"},"tags":[]},"source":["## Why Do This?\n","\n","Because models donâ€™t just memorize pixels.\n","\n","They must learn patterns.\n","\n","If your dataset is small:\n","\n","- Model overfits\n","- Model memorizes instead of generalizing\n","\n","Augmentation makes the model see:\n","\n","- Cats in different positions\n","- Different lighting\n","- Slight distortions\n","\n","So it learns the concept of â€œcatâ€,\n","not just specific pixels."]},{"cell_type":"markdown","id":"50609d99","metadata":{"papermill":{"duration":0.002329,"end_time":"2026-02-23T08:14:30.59926","exception":false,"start_time":"2026-02-23T08:14:30.596931","status":"completed"},"tags":[]},"source":["## What â€œLabel-Preservingâ€ Means\n","\n","If original sample is:\n","\n","Image of 3 â†’ label = 3\n","\n","After rotating slightly:\n","\n","Still 3.\n","\n","But if you rotate 180Â°:\n","\n","It might look like something else.\n","\n","That would break the rule.\n","\n","So transformation must not change the true class.\n","\n","Thatâ€™s what this means:\n","\n","$$\n","y(x) = y(T(x))\n","$$\n","\n","The label before and after transformation stays the same."]},{"cell_type":"markdown","id":"62d6b624","metadata":{"papermill":{"duration":0.002341,"end_time":"2026-02-23T08:14:30.605233","exception":false,"start_time":"2026-02-23T08:14:30.602892","status":"completed"},"tags":[]},"source":["## What â€œIncrease Effective Dataset Sizeâ€ Means\n","\n","Suppose:\n","\n","You have 1,000 images.\n","\n","With augmentation during training:\n","\n","Each epoch, images look slightly different.\n","\n","So the model might effectively see:\n","\n","10,000+ variations.\n","\n","You didnâ€™t collect new data.\n","\n","But the model experiences more diversity.\n","\n","Thatâ€™s why:\n","\n","$$\n","N_{effective} \\gg N\n","$$\n"]},{"cell_type":"markdown","id":"18daf27f","metadata":{"papermill":{"duration":0.002337,"end_time":"2026-02-23T08:14:30.609975","exception":false,"start_time":"2026-02-23T08:14:30.607638","status":"completed"},"tags":[]},"source":["## Real-World Intuition\n","\n","Without augmentation:\n","Model learns:\n","> â€œDogs look exactly like these 1,000 pictures.â€\n","\n","With augmentation:\n","Model learns:\n","> â€œDogs can appear in many positions, angles, and lighting conditions.â€\n","\n","That improves generalization.\n","\n","**One-Line Definition (Simple):**\n","\n","> Data augmentation is creating modified versions of training data that keep the same label, so the model sees more variety and generalizes better.\n"]},{"cell_type":"markdown","id":"7b8ef648","metadata":{"papermill":{"duration":0.002398,"end_time":"2026-02-23T08:14:30.615126","exception":false,"start_time":"2026-02-23T08:14:30.612728","status":"completed"},"tags":[]},"source":["# Geometric Transformations\n","\n","Geometric augmentations modify the **spatial arrangement** of pixels while keeping the object class unchanged.\n","\n","They do not alter what the object is, they alter how it is positioned."]},{"cell_type":"markdown","id":"87335abe","metadata":{"papermill":{"duration":0.002346,"end_time":"2026-02-23T08:14:30.619984","exception":false,"start_time":"2026-02-23T08:14:30.617638","status":"completed"},"tags":[]},"source":["## 3.1 Horizontal Flip\n","\n","A horizontal flip mirrors the image across the vertical axis.\n","\n","If image width is $W$, a pixel at:\n","\n","$$\n","(x, y)\n","$$\n","\n","becomes:\n","\n","$$\n","(W - x, y)\n","$$\n","\n","So left becomes right, and right becomes left.\n","\n","### Visual Intuition\n","\n","Original image: object facing right  \n","After flip: object facing left  \n","\n","The object identity remains the same.\n","\n","### When It Is Good\n","\n","Use horizontal flip when orientation does **not define the class**.\n","\n","Strong use cases:\n","\n","- Natural images  \n","- Object recognition  \n","- Animals  \n","- Vehicles  \n","- Generic object detection  \n","\n","Reason:\n","\n","Most real-world objects can appear facing either direction.\n","\n","Flipping increases orientation diversity.\n","\n","### When You Should Avoid It\n","\n","Do NOT use it when horizontal orientation carries meaning.\n","\n","Examples:\n","\n","- Text recognition (text becomes reversed)\n","- Digit classification ($6 \\leftrightarrow 9$ confusion)\n","- Medical imaging (left anatomy $\\neq$ right anatomy)\n","- Traffic signs with directional arrows\n","\n","In these cases, flipping changes semantics.\n","\n","That violates the label-preserving rule:\n","\n","$$\n","y(x) = y(T(x))\n","$$\n","\n","### PyTorch Implementation\n","\n","```python\n","transforms.RandomHorizontalFlip(p=0.5)\n","```\n","**Meaning:**\n","\n","- Each image has 50% probability of being flipped\n","- Augmentation is applied during training only"]},{"cell_type":"markdown","id":"040d5238","metadata":{"papermill":{"duration":0.002387,"end_time":"2026-02-23T08:14:30.624713","exception":false,"start_time":"2026-02-23T08:14:30.622326","status":"completed"},"tags":[]},"source":["## 3.2 Random Rotation\n","\n","Rotate image by an angle $\\theta$.\n","\n","The rotation transformation is:\n","\n","$$\n","\\begin{bmatrix}\n","x' \\\\\n","y'\n","\\end{bmatrix}\n","=\n","\\begin{bmatrix}\n","\\cos\\theta & -\\sin\\theta \\\\\n","\\sin\\theta & \\cos\\theta\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","x \\\\\n","y\n","\\end{bmatrix}\n","$$\n","\n","Each pixel coordinate is rotated around the image center.\n","\n","### Purpose\n","\n","Build **rotational invariance**.\n","\n","The model learns that small orientation changes do not change class identity.\n","\n","### Safe Range\n","\n","For natural images:\n","\n","$$\n","5^\\circ \\text{ to } 20^\\circ\n","$$\n","\n","Small rotations simulate camera tilt.  \n","Large rotations may break semantics (e.g., digits or medical scans).\n","\n","\n","### PyTorch Implementation\n","\n","```python\n","transforms.RandomRotation(degrees=15)\n","````\n","\n","Images are randomly rotated within:\n","\n","$$\n","[-15^\\circ, +15^\\circ]\n","$$"]},{"cell_type":"markdown","id":"215a691b","metadata":{"papermill":{"duration":0.002487,"end_time":"2026-02-23T08:14:30.629578","exception":false,"start_time":"2026-02-23T08:14:30.627091","status":"completed"},"tags":[]},"source":["## 3.3 Random Resized Crop\n","\n","Randomly:\n","\n","1. Select a region of the image\n","2. Crop it\n","3. Resize it to a fixed resolution\n","\n","If the original image is $H \\times W$,\n","a sub-region is sampled with scale:\n","\n","$$\n","s \\in [s_{min}, s_{max}]\n","$$\n","\n","Then resized (e.g., to $224 \\times 224$).\n","\n","### Effect\n","\n","* Forces focus on partial features\n","* Encourages scale robustness\n","* Reduces reliance on full global context\n","\n","The model must recognize objects even if partially visible.\n","\n","### PyTorch Implementation\n","\n","```python\n","transforms.RandomResizedCrop(224, scale=(0.8, 1.0))\n","```\n","\n","**Meaning:**\n","\n","* Crop between $80%$ and $100%$ of original area\n","* Resize to $224 \\times 224$"]},{"cell_type":"markdown","id":"6fdd6d89","metadata":{"papermill":{"duration":0.002493,"end_time":"2026-02-23T08:14:30.634569","exception":false,"start_time":"2026-02-23T08:14:30.632076","status":"completed"},"tags":[]},"source":["Suppose original image is:\n","\n","400 Ã— 400\n","\n","And you use:\n","\n","```python\n","transforms.RandomResizedCrop(224, scale=(0.8, 1.0))\n","````\n","\n","**Step 1: Pick Random Area**\n","\n","Scale = (0.8, 1.0)\n","\n","This means:\n","\n","Crop between:\n","\n","$$\n","80% \\text{ and } 100%\n","$$\n","\n","of the original image area.\n","\n","If original area is:\n","\n","$$\n","400 \\times 400 = 160000\n","$$\n","\n","Then crop area will be between:\n","\n","$$\n","128000 \\text{ and } 160000\n","$$\n","\n","So maybe it selects a 360 Ã— 360 region.<br>\n","Or maybe 380 Ã— 380.<br>\n","Or maybe full image.\n","\n","**Step 2: Crop That Region**\n","\n","It cuts out that selected box.\n","\n","Now you have something smaller than original.\n","\n","**Step 3: Resize to 224 Ã— 224**\n","\n","No matter what was cropped,\n","it gets resized to:\n","\n","$$\n","224 \\times 224\n","$$\n","\n","So final input size stays consistent.\n","\n","### Why This Helps\n","\n","### Without It\n","\n","Model always sees:\n","\n","* Entire object\n","* Centered\n","* Same scale\n","\n","It may memorize layout.\n","\n","### With RandomResizedCrop\n","\n","Sometimes:\n","\n","* Only part of object is visible\n","* Object appears zoomed in\n","* Object appears slightly zoomed out\n","\n","Now model must learn:\n","\n","> â€œEven if I only see part of the dog, itâ€™s still a dog.â€\n","\n","This builds:\n","\n","* Scale robustness\n","* Partial feature recognition\n","* Better generalization\n","\n","\n","### Simple Visual Intuition\n","\n","Original:\n","\n","[     Full Dog Image     ]\n","\n","\n","After random crop:\n","\n","\n","[   Only Dog Face   ]\n","\n","\n","Resized back to full size.\n","\n","Model learns to recognize from partial cues.\n","\n","\n","\n","\n","RandomResizedCrop =\n","\n","> Random zoom + random crop + resize back to fixed size."]},{"cell_type":"markdown","id":"9d0a27bb","metadata":{"execution":{"iopub.execute_input":"2026-02-23T03:30:40.016259Z","iopub.status.busy":"2026-02-23T03:30:40.015238Z","iopub.status.idle":"2026-02-23T03:30:40.02781Z","shell.execute_reply":"2026-02-23T03:30:40.026628Z","shell.execute_reply.started":"2026-02-23T03:30:40.016219Z"},"papermill":{"duration":0.002455,"end_time":"2026-02-23T08:14:30.639473","exception":false,"start_time":"2026-02-23T08:14:30.637018","status":"completed"},"tags":[]},"source":["## 3.4 Random Affine (Translation)\n","\n","It just means:\n","\n","> Move the image slightly left/right or up/down.\n","\n","Nothing else changes.\n","\n","### Simple Example\n","\n","Original image:\n","\n","[Dog is centered]\n","\n","After translation:\n","\n","[Dog shifted left]\n","\n","Or:\n","\n","[Dog shifted right]\n","\n","Still a dog.\n","\n","Same label.\n","\n","### Math Formula\n","\n","Translate image coordinates:\n","\n","$$\n","(x, y) \\rightarrow (x + \\Delta x, y + \\Delta y)\n","$$\n","\n","Where:\n","\n","$$\n","\\Delta x, \\Delta y\n","$$\n","\n","are small horizontal and vertical shifts.\n","\n","This just says:\n","\n","Each pixel moves by a small amount.\n","\n","- $\\Delta x$ = how much we move left/right  \n","- $\\Delta y$ = how much we move up/down  \n","\n","If $\\Delta x = 10$, every pixel shifts 10 pixels to the right.\n","\n","### PyTorch Implementation\n","\n","```python\n","transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))\n","```\n","\n","Meaning:\n","\n","* `degrees=0` â†’ no rotation\n","* `translate=(0.1, 0.1)` â†’ can move image up to: $10%$ of width and height.\n","\n","If image is 200 Ã— 200:\n","\n","* Max horizontal shift = 20 pixels\n","* Max vertical shift = 20 pixels\n","\n","Each image is randomly shifted within that range.\n","\n","### Why Do This?\n","\n","Without translation:\n","\n","Model may learn:\n","\n","> â€œDog is always in center.â€\n","\n","Thatâ€™s dangerous.\n","\n","In real life:\n","\n","* Objects appear anywhere in frame.\n","* They are not perfectly centered.\n","\n","Translation teaches:\n","\n","> Position does NOT define the class.\n","\n","This builds **location invariance**\n","\n","### Real Benefit\n","\n","If you donâ€™t use translation:\n","\n","Model might fail when object appears near edge.\n","\n","If you use translation:\n","\n","Model becomes robust to placement changes.\n","\n","Random translation =\n","\n","> Slightly move the image so the model doesnâ€™t depend on object position."]},{"cell_type":"markdown","id":"f677d97b","metadata":{"papermill":{"duration":0.00246,"end_time":"2026-02-23T08:14:30.644304","exception":false,"start_time":"2026-02-23T08:14:30.641844","status":"completed"},"tags":[]},"source":["# Color & Intensity Transformations\n","\n","**TO BE CONTINUE...**"]},{"cell_type":"markdown","id":"3f0ac25f","metadata":{"papermill":{"duration":0.002351,"end_time":"2026-02-23T08:14:30.649046","exception":false,"start_time":"2026-02-23T08:14:30.646695","status":"completed"},"tags":[]},"source":["---\n","\n","<p style=\"text-align:center; color:skyblue; font-size:18px;\">\n","Â© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31286,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":3.482823,"end_time":"2026-02-23T08:14:30.970315","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-23T08:14:27.487492","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}