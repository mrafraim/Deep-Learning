{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-25-lstm-gru?scriptVersionId=290137071\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"41ebe12f","metadata":{"papermill":{"duration":0.005319,"end_time":"2026-01-05T10:54:12.31844","exception":false,"start_time":"2026-01-05T10:54:12.313121","status":"completed"},"tags":[]},"source":["# Day 25: LSTM & GRU\n","\n","Wlcome to Day 25!\n","\n","Today you‚Äôll learn:\n","\n","1. Why vanilla RNNs fail on long sequences\n","2. What vanishing gradient really means (intuitively)\n","3. What an LSTM is and why it fixes RNN problems\n","4. LSTM gates: Forget, Input, Output\n","5. How information flows through an LSTM cell\n","6. What a GRU is and how it differs from LSTM\n","7. When to use RNN vs LSTM vs GRU\n","\n","> By the end of this notebook, you will understand why LSTM exists, not just how it works.\n","\n","If you found this notebook helpful, your **<b style=\"color:red;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"67c20a7e","metadata":{"papermill":{"duration":0.003965,"end_time":"2026-01-05T10:54:12.326594","exception":false,"start_time":"2026-01-05T10:54:12.322629","status":"completed"},"tags":[]},"source":["# What is a Vanilla RNN?\n","\n","A vanilla RNN is the simplest possible recurrent neural network.\n","It is the original RNN formulation - no gates, no cell state, no memory control.\n","\n","It consists of:\n","- An input\n","- A hidden state (memory)\n","- A nonlinear activation function\n","\n","That‚Äôs it."]},{"cell_type":"markdown","id":"3e11c1ce","metadata":{"papermill":{"duration":0.004079,"end_time":"2026-01-05T10:54:12.334887","exception":false,"start_time":"2026-01-05T10:54:12.330808","status":"completed"},"tags":[]},"source":["## Core idea\n","\n","A vanilla RNN processes a sequence one time step at a time, while carrying forward a single hidden state that summarizes the past.\n","\n","At each time step $t$:\n","- It reads the current input $x_t$\n","- It combines it with the previous hidden state $h_{t-1}$\n","- It produces a new hidden state $h_t$\n","\n","Mathematically:\n","\n","$$\n","h_t = \\tanh(W_{xh}x_t + W_{hh}h_{t-1} + b)\n","$$\n","\n","Where:\n","- $x_t$ ‚Üí input at time step $t$\n","- $h_{t-1}$ ‚Üí memory from the past\n","- $h_t$ ‚Üí updated memory\n","- $\\tanh$ ‚Üí squashing nonlinearity\n","- Same weights are reused at every time step"]},{"cell_type":"markdown","id":"f91db090","metadata":{"papermill":{"duration":0.003926,"end_time":"2026-01-05T10:54:12.34282","exception":false,"start_time":"2026-01-05T10:54:12.338894","status":"completed"},"tags":[]},"source":["## Why it‚Äôs called ‚Äúvanilla‚Äù\n","\n","‚ÄúVanilla‚Äù means:\n","- No gating mechanisms\n","- No selective memory\n","- No protection against gradient decay\n","- One single memory vector doing everything\n","\n","> A vanilla RNN blindly mixes past and present at every step.\n"]},{"cell_type":"markdown","id":"e636520b","metadata":{"papermill":{"duration":0.004246,"end_time":"2026-01-05T10:54:12.351561","exception":false,"start_time":"2026-01-05T10:54:12.347315","status":"completed"},"tags":[]},"source":["## Structural view\n","\n","At each time step:\n","\n","$$\n","x_t + h_{t-1} ‚Üí [Linear + tanh] ‚Üí h_t\n","$$\n","\n","Unrolled over time:\n","\n","$$\n","x‚ÇÅ ‚Üí [RNN] ‚Üí h‚ÇÅ ‚Üí [RNN] ‚Üí h‚ÇÇ ‚Üí [RNN] ‚Üí h‚ÇÉ ‚Üí ...\n","$$\n","\n","Same cell. Same weights. Growing dependency chain."]},{"cell_type":"markdown","id":"1b024949","metadata":{"papermill":{"duration":0.003947,"end_time":"2026-01-05T10:54:12.359522","exception":false,"start_time":"2026-01-05T10:54:12.355575","status":"completed"},"tags":[]},"source":["## What the hidden state really is\n","\n","The hidden state $h_t$ is:\n","- A compressed summary of everything seen so far\n","- Fixed-size, regardless of sequence length\n","- Overwritten at every time step\n","\n","This creates a fundamental tension:\n","- Short-term details vs long-term memory\n","- New input vs old information\n","\n","Vanilla RNNs have no mechanism to manage this tradeoff."]},{"cell_type":"markdown","id":"9d34d97a","metadata":{"papermill":{"duration":0.004518,"end_time":"2026-01-05T10:54:12.367924","exception":false,"start_time":"2026-01-05T10:54:12.363406","status":"completed"},"tags":[]},"source":["## What vanilla RNNs can do well\n","\n","Vanilla RNNs work when:\n","- Sequences are short\n","- Dependencies are local\n","- Patterns are simple\n","\n","Examples:\n","- Simple signal smoothing\n","- Toy language models\n","- Educational demonstrations"]},{"cell_type":"markdown","id":"d1f88e8b","metadata":{"papermill":{"duration":0.004419,"end_time":"2026-01-05T10:54:12.376426","exception":false,"start_time":"2026-01-05T10:54:12.372007","status":"completed"},"tags":[]},"source":["## What vanilla RNNs cannot do well\n","\n","They fail when:\n","- Sequences are long\n","- Important information appears far in the past\n","- Memory must be preserved precisely\n","\n","Examples they struggle with:\n","- Long sentences\n","- Long time-series forecasting\n","- Context-dependent language tasks"]},{"cell_type":"markdown","id":"a4be05ee","metadata":{"papermill":{"duration":0.004026,"end_time":"2026-01-05T10:54:12.384644","exception":false,"start_time":"2026-01-05T10:54:12.380618","status":"completed"},"tags":[]},"source":["## The critical flaw\n","\n","The vanilla RNN:\n","- Reuses the same transformation repeatedly\n","- Multiplies gradients through many time steps\n","- Has no mechanism to protect important memory\n","\n","This leads directly to:\n","- Repeated multiplication causes:\n","  - **Vanishing gradients** (‚Üí 0)\n","  - **Exploding gradients** (‚Üí ‚àû) \n","- Forgetting long-term information\n","- Cannot learn dependencies far in the past\n","\n","> LSTM exists specifically to fix this flaw."]},{"cell_type":"markdown","id":"e87fc83b","metadata":{"papermill":{"duration":0.003896,"end_time":"2026-01-05T10:54:12.392554","exception":false,"start_time":"2026-01-05T10:54:12.388658","status":"completed"},"tags":[]},"source":["# Vanishing Gradient\n","\n","Vanishing gradient is not a bug. It is a mathematical consequence of how vanilla RNNs are built.\n","\n","During training, RNNs use Backpropagation Through Time (BPTT):\n","- The RNN is unrolled across time\n","- Gradients flow backward from later time steps to earlier ones\n","\n","To update early weights, gradients must pass through many repeated transformations.\n","\n","\n","Hidden state:\n","\n","$$\n","h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t)\n","$$\n","\n","Key observation:\n","- The same weight matrix $W_{hh}$ is applied at every time step\n","- The same activation function (`tanh`) is applied repeatedly\n"]},{"cell_type":"markdown","id":"97d717cf","metadata":{"papermill":{"duration":0.004217,"end_time":"2026-01-05T10:54:12.400758","exception":false,"start_time":"2026-01-05T10:54:12.396541","status":"completed"},"tags":[]},"source":["## Gradient flow across time\n","\n","Consider the gradient of the loss $L$ with respect to an early hidden state $h_k$:\n","\n","$$\n","\\frac{\\partial L}{\\partial h_k}\n","=\n","\\prod_{t=k+1}^{T}\n","\\frac{\\partial h_t}{\\partial h_{t-1}}\n","\\cdot\n","\\frac{\\partial L}{\\partial h_T}\n","$$\n","\n","Each term contains:\n","\n","$$\n","\\frac{\\partial h_t}{\\partial h_{t-1}}\n","=\n","W_{hh}^\\top \\cdot \\tanh'(z_t)\n","$$\n","\n","So gradients are repeatedly multiplied by:\n","- The recurrent weight matrix\n","- The derivative of `tanh`"]},{"cell_type":"markdown","id":"60369e5d","metadata":{"papermill":{"duration":0.003893,"end_time":"2026-01-05T10:54:12.408667","exception":false,"start_time":"2026-01-05T10:54:12.404774","status":"completed"},"tags":[]},"source":["## Why gradients vanish \n","\n","Key facts:\n","- `tanh'(x) ‚â§ 1`\n","- Usually < 1, especially near saturation\n","- Eigenvalues of $W_{hh}$ are often < 1 for stability\n","\n","```\n","Eigenvalue tells you how much a matrix stretches a vector in a particular direction.\n","\n","In RNNs: eigenvalues of $ùëä_{‚Ñé‚Ñé}$ tell you how the hidden state (and gradients) grow or shrink over time.\n","\n","- ‚à£Œª‚à£>1 ‚Üí exponential growth (exploding)\n","- ‚à£Œª‚à£<1 ‚Üí exponential decay (vanishing)\n","- ‚à£Œª‚à£=1 ‚Üí stable (ideal for long sequences)\n","```\n","So each step multiplies the gradient by a number slightly less than 1.\n","\n","Example:\n","$$\n","0.8^{10} \\approx 0.11\n","$$\n","$$\n","0.8^{50} \\approx 0.000014\n","$$\n","\n","After many time steps:\n","\n","Gradient ‚Üí almost zero\n","\n","Early time steps receive no learning signal.\n"]},{"cell_type":"markdown","id":"31ec9b56","metadata":{"papermill":{"duration":0.004144,"end_time":"2026-01-05T10:54:12.417536","exception":false,"start_time":"2026-01-05T10:54:12.413392","status":"completed"},"tags":[]},"source":["## Exploding Gradient\n","\n","If Eigenvalues of $W_{hh} > 1$\n","\n","Then:\n","$$\n","1.2^{50} \\approx 9100\n","$$\n","\n","Result:\n","- Exploding gradients  \n","- Numerical instability  \n","- Training collapses\n","\n","So vanilla RNNs live in a narrow unstable zone:\n","- Too small ‚Üí vanishing\n","- Too large ‚Üí exploding\n"]},{"cell_type":"markdown","id":"beb51ec2","metadata":{"papermill":{"duration":0.003918,"end_time":"2026-01-05T10:54:12.425492","exception":false,"start_time":"2026-01-05T10:54:12.421574","status":"completed"},"tags":[]},"source":["## Why this Breaks Learning\n","\n","Vanilla RNNs:\n","- Learn recent inputs well\n","- Forget distant inputs completely\n","\n","They behave like:\n","> ‚ÄúShort-term memory machines‚Äù\n","\n","This is why they fail at:\n","- Long sentences\n","- Long time dependencies\n","- Context-heavy tasks\n","\n","> Vanishing gradients occur because backpropagation through many time steps repeatedly multiplies gradients by numbers less than 1, causing early information to disappear."]},{"cell_type":"markdown","id":"1c66f392","metadata":{"papermill":{"duration":0.004562,"end_time":"2026-01-05T10:54:12.433983","exception":false,"start_time":"2026-01-05T10:54:12.429421","status":"completed"},"tags":[]},"source":["---\n","<p style=\"text-align:center; font-size:18px;\"> (Optional) </p>\n","\n","### 1Ô∏è. Jacobian Definition\n","\n","When a function maps vectors:\n","\n","$$\n","\\mathbf{y} = f(\\mathbf{x}), \\quad \\mathbf{x} \\in \\mathbb{R}^n, \\mathbf{y} \\in \\mathbb{R}^m\n","$$\n","\n","the derivative is not a number. It‚Äôs a matrix of partial derivatives, called the Jacobian:\n","\n","$$\n","J = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} =\n","\\begin{bmatrix}\n","\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","\\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n} \n","\\end{bmatrix}\n","$$\n","\n","- Each row = gradient of one output w.r.t all inputs  \n","- Each column = how one input affects all outputs\n","\n","\n","**Simple Jacobian Example**\n","\n","Define:\n","\n","$$\n","\\mathbf{y} = \\tanh(\\mathbf{x}), \\quad \\mathbf{x} = \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}, \\mathbf{y} = \\begin{bmatrix}y_1\\\\y_2\\end{bmatrix}\n","$$\n","\n","$$\n","y_1 = \\tanh(x_1), \\quad y_2 = \\tanh(x_2)\n","$$\n","\n","Derivative (Jacobian):\n","\n","$$\n","\\frac{\\partial y_i}{\\partial x_j} =\n","\\begin{cases}\n","1 - \\tanh^2(x_i) & i = j \\\\\n","0 & i \\neq j\n","\\end{cases}\n","$$\n","\n","<br>\n","\n","$$\n","\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} =\n","\\begin{bmatrix}\n","1-\\tanh^2(x_1) & 0 \\\\\n","0 & 1-\\tanh^2(x_2)\n","\\end{bmatrix} = \\operatorname{diag}(1-\\tanh^2(\\mathbf{x}))\n","$$\n","\n","**Numerical example:**\n","\n","$$\n","\\mathbf{x} = \\begin{bmatrix}1\\\\2\\end{bmatrix} \\quad \\Rightarrow \\quad\n","\\tanh(1)\\approx0.761, \\quad \\tanh(2)\\approx0.964\n","$$\n","\n","$$\n","J \\approx\n","\\begin{bmatrix}\n","0.42 & 0 \\\\\n","0 & 0.07\n","\\end{bmatrix}\n","$$\n","\n","\n","### 2. Vanilla RNN Hidden State & Gradient Flow\n","\n","Hidden state:\n","\n","$$\n","h_t = \\tanh(z_t), \\quad z_t = W_{hh} h_{t-1} + W_{xh} x_t\n","$$\n","\n","Backprop:\n","\n","$$\n","\\frac{\\partial L}{\\partial h_{t-1}} =\n","\\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot \\frac{\\partial L}{\\partial h_t}, \\quad\n","\\frac{\\partial h_t}{\\partial h_{t-1}} = \\operatorname{diag}(1-\\tanh^2(z_t)) W_{hh}\n","$$\n","\n","- Each time step multiplies gradient by the Jacobian  \n","- Diagonal entries <1 ‚Üí gradient shrinks ‚Üí vanishing gradient\n","\n","\n","**Numerical Example (2D, 2 time steps)**\n","\n","Weights:\n","\n","$$\n","W_{hh} = \\begin{bmatrix}0.5 & 0 \\\\ 0 & 0.5\\end{bmatrix},\\quad\n","W_{xh} = \\begin{bmatrix}0.8 & 0 \\\\ 0 & 0.8\\end{bmatrix}\n","$$\n","\n","Inputs:\n","\n","$$\n","x_1 = \\begin{bmatrix}1\\\\2\\end{bmatrix},\\quad\n","x_2 = \\begin{bmatrix}2\\\\1\\end{bmatrix},\\quad\n","h_0 = \\begin{bmatrix}0\\\\0\\end{bmatrix}\n","$$\n","\n","Forward pass:\n","\n","$$\n","\\begin{aligned}\n","z_1 &= W_{hh}h_0 + W_{xh}x_1 = \\begin{bmatrix}0.8\\\\1.6\\end{bmatrix} \\Rightarrow h_1 = \\tanh(z_1) \\approx \\begin{bmatrix}0.664\\\\0.921\\end{bmatrix} \\\\\n","z_2 &= W_{hh}h_1 + W_{xh}x_2 = \\begin{bmatrix}1.332\\\\1.321\\end{bmatrix} \\Rightarrow h_2 \\approx \\begin{bmatrix}0.869\\\\0.867\\end{bmatrix}\n","\\end{aligned}\n","$$\n","\n","Jacobians:\n","\n","$$\n","\\frac{\\partial h_1}{\\partial h_0} = \\operatorname{diag}(1-\\tanh^2(z_1)) W_{hh} \\approx \\begin{bmatrix}0.280 & 0 \\\\ 0 & 0.076\\end{bmatrix}\n","$$\n","\n","$$\n","\\frac{\\partial h_2}{\\partial h_1} \\approx \\begin{bmatrix}0.123 & 0 \\\\ 0 & 0.124\\end{bmatrix}\n","$$\n","\n","Gradient flow:\n","\n","Assume $\\frac{\\partial L}{\\partial h_2} = [1,1]^T$\n","\n","$$\n","\\frac{\\partial L}{\\partial h_0} =\n","\\frac{\\partial h_1}{\\partial h_0} \\cdot \n","\\frac{\\partial h_2}{\\partial h_1} \\cdot\n","\\frac{\\partial L}{\\partial h_2} \\approx \n","\\begin{bmatrix}0.034 \\\\ 0.009\\end{bmatrix}\n","$$\n","\n","- The original gradient at $h_2$: 1 ‚Üí after two steps: [0.034, 0.009]\n","- Exponentially shrunk because each Jacobian < 1\n","- Early hidden states receive almost zero gradient\n","\n","> Gradient shrinks drastically ‚Üí vanishing gradient\n","\n","\n","**Key Takeaways**\n","\n","- Hidden state derivative = diagonal Jacobian √ó weight\n","- Backprop across many steps = product of Jacobians\n","- Each diagonal <1 ‚Üí exponential shrink\n","- Longer sequences ‚Üí earlier gradients ‚Üí practically 0* \n","\n","---"]},{"cell_type":"markdown","id":"aa671f90","metadata":{"papermill":{"duration":0.00386,"end_time":"2026-01-05T10:54:12.441886","exception":false,"start_time":"2026-01-05T10:54:12.438026","status":"completed"},"tags":[]},"source":["# Long Short-Term Memory (LSTM)\n","\n","LSTM is a vanilla RNN on steroids:  \n","\n","It fixes the vanishing gradient problem by introducing controlled memory through gates.\n","\n","Core idea:\n","> LSTM decides what to remember, what to forget, and what to output at each time step.\n"]},{"cell_type":"markdown","id":"98e5b4f6","metadata":{"papermill":{"duration":0.003921,"end_time":"2026-01-05T10:54:12.44972","exception":false,"start_time":"2026-01-05T10:54:12.445799","status":"completed"},"tags":[]},"source":["## Components of LSTM\n","\n","1. **Cell state ($C_t$)**: long-term memory  \n","   - Carries information across time steps\n","   - Changes slowly (additive updates)\n","   \n","2. **Hidden state ($h_t$)**: short-term memory / output  \n","   - Used for predictions at each time step\n","\n","3. **Gates (sigmoid activations)**: control information flow:\n","   - **Forget gate ($f_t$)** ‚Üí decide what to erase from $C_{t-1}$\n","   - **Input gate ($i_t$)** ‚Üí decide what new info to add\n","   - **Output gate ($o_t$)** ‚Üí decide what part of $C_t$ to output as $h_t$\n","\n","Mathematically:\n","\n","**1Ô∏è. Forget Gate ($f_t$)**\n","\n","$$\n","f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)\n","$$\n","\n","Where,\n","\n","- $h_{t-1}$ ‚Üí previous hidden state (short-term memory)  \n","- $x_t$ ‚Üí current input  \n","- $[h_{t-1}, x_t]$ ‚Üí concatenation of previous hidden + current input  \n","- $W_f$ ‚Üí weights for forget gate  \n","- $b_f$ ‚Üí bias term  \n","- $\\sigma$ ‚Üí sigmoid activation ‚Üí outputs values between 0 and 1\n","\n","\n","> $f_t$ decides how much of the previous cell memory $C_{t-1}$ to keep or forget.  \n","\n","- $f_t = 0$ ‚Üí forget everything  \n","- $f_t = 1$ ‚Üí keep everything  \n","\n","\n","**2. Input Gate ($i_t$) and Candidate Memory ($\\tilde{C}_t$)**\n","\n","<u>Input Gate ($i_t$)</u>\n","\n","$$\n","i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)\n","$$\n","\n","Explanation:  \n","- Controls how much new information to write to the cell state.  \n","- Sigmoid ensures the gate outputs 0 (ignore new info) ‚Üí 1 (fully write new info).\n","\n","<u>Candidate Memory ($\\tilde{C}_t$)</u>\n","\n","$$\n","\\tilde{C}_t = \\tanh(W_c [h_{t-1}, x_t] + b_c)\n","$$\n","\n","Explanation: \n","- Generates new candidate values that could be added to memory.  \n","- $\\tanh$ squashes values to [-1, 1], keeping memory stable.  \n","- $\\tilde{C}_t$ is proposed new content; input gate $i_t$ decides how much actually enters $C_t$.\n","\n","\n","**3Ô∏è. Cell State Update ($C_t$)**\n","\n","$$\n","C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n","$$\n","\n","Explanation:  \n","\n","- **Previous memory:** $f_t \\cdot C_{t-1}$ ‚Üí retained portion of old memory  \n","- **New information:** $i_t \\cdot \\tilde{C}_t$ ‚Üí portion of new candidate added  \n","- **Additive update** (instead of overwriting) helps gradients flow easily, solving vanishing gradient problem\n","\n","Intuition:\n","> The cell state is like a water tank:  \n","> - Forget gate = drain valve  \n","> - Input gate = faucet adding new water  \n","\n","\n","**4Ô∏è. Output Gate ($o_t$) and Hidden State ($h_t$)**\n","\n","<u>Output Gate ($o_t$)</u>\n","\n","$$\n","o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)\n","$$\n","\n","Explanation:  \n","- Determines how much of the cell state $C_t$ should be exposed as the hidden state (short-term output)  \n","- Sigmoid ‚Üí 0 means ‚Äúhide everything‚Äù, 1 means ‚Äúreveal everything‚Äù\n","\n","<u>Hidden State ($h_t$)</u>\n","\n","$$\n","h_t = o_t \\cdot \\tanh(C_t)\n","$$\n","\n","Explanation: \n","- $\\tanh(C_t)$ ‚Üí squashes memory values to [-1,1]  \n","- Multiply by $o_t$ ‚Üí output only selected memory  \n","- $h_t$ is both the output for this step and the hidden input for next step\n","\n","Intuition:\n","\n","The hidden state is like the water flowing out of the tank tap.  \n","> - $C_t$ = water stored  \n","> - $o_t$ = how wide the tap is open    "]},{"cell_type":"markdown","id":"8a6676c0","metadata":{"papermill":{"duration":0.004031,"end_time":"2026-01-05T10:54:12.457811","exception":false,"start_time":"2026-01-05T10:54:12.45378","status":"completed"},"tags":[]},"source":["## How LSTM Works\n","\n","1. **Forget gate** ‚Üí erase old memory (partial)  \n","2. **Input gate + candidate memory** ‚Üí add new info  \n","3. **Cell state update** ‚Üí memory carries forward gradually  \n","4. **Output gate ‚Üí hidden state** ‚Üí controlled short-term output  \n","\n","> LSTM separates long-term memory ($C_t$) and short-term output ($h_t$), allowing it to preserve important information across long sequences.\n","\n","> Additive updates to $C_t$ preserve gradients, avoiding vanishing gradient problem.\n","\n","**Visual Analogy**\n","\n","Imagine a water tank:\n","\n","- **Cell state** = water level (long-term memory)  \n","- **Forget gate** = drain valve (how much old memory to discard)  \n","- **Input gate** = faucet (how much new memory to add)  \n","- **Output gate** = tap to output water (hidden state $h_t$)\n","\n","Water flows controlled by valves (gates), not by random overflow."]},{"cell_type":"markdown","id":"c6dc2483","metadata":{"papermill":{"duration":0.003864,"end_time":"2026-01-05T10:54:12.465569","exception":false,"start_time":"2026-01-05T10:54:12.461705","status":"completed"},"tags":[]},"source":["## Manual LSTM Example\n","\n","We will simulate a single LSTM layer with a simple sequence `[1, 2]` and scalar weights to see how memory ($C_t$) and hidden state ($h_t$) evolve.\n","\n","**Setup**\n","\n","- Input sequence: `x = [1, 2]`  \n","- Previous hidden state: `h0 = 0`  \n","- Previous cell state: `C0 = 0`  \n","- Simplified weights and biases:\n","\n","\n","| Gate | Weight | Bias |\n","|------|--------|------|\n","| Forget $W_f$ | 0.5 | 0 |\n","| Input $W_i$ | 0.6 | 0 |\n","| Candidate $W_c$ | 0.9 | 0 |\n","| Output $W_o$ | 0.7 | 0 |\n","\n","\n","- Activation: sigmoid $\\sigma(x) = 1/(1+e^{-x})$, $tanh$ as usual  \n","\n","<u>**Time Step 1: Input = 1**</u>\n","\n","**Step 1: Forget Gate**\n","\n","$$\n","f_1 = \\sigma(W_f * x_1 + W_f * h_0) = \\sigma(0.5*1 + 0.5*0) = \\sigma(0.5) \\approx 0.622\n","$$\n","\n","**Step 2: Input Gate**\n","\n","$$\n","i_1 = \\sigma(W_i * x_1 + W_i * h_0) = \\sigma(0.6*1 + 0.6*0) = \\sigma(0.6) \\approx 0.645\n","$$\n","\n","**Step 3: Candidate Memory**\n","\n","$$\n","\\tilde{C}_1 = \\tanh(W_c * x_1 + W_c * h_0) = \\tanh(0.9*1 + 0) = \\tanh(0.9) \\approx 0.716\n","$$\n","\n","**Step 4: Cell State Update**\n","\n","$$\n","C_1 = f_1 * C_0 + i_1 * \\tilde{C}_1 = 0.622*0 + 0.645*0.716 \\approx 0.462\n","$$\n","\n","**Step 5: Output Gate**\n","\n","$$\n","o_1 = \\sigma(W_o * x_1 + W_o * h_0) = \\sigma(0.7*1 + 0) = \\sigma(0.7) \\approx 0.668\n","$$\n","\n","**Step 6: Hidden State**\n","\n","$$\n","h_1 = o_1 * \\tanh(C_1) = 0.668 * \\tanh(0.462) \\approx 0.668 * 0.432 \\approx 0.288\n","$$\n","\n","After first time step: $C_1 \\approx 0.462$, $h_1 \\approx 0.288$\n","\n","\n","<u>**Time Step 2: Input = 2**</u>\n","\n","**Step 1: Forget Gate**\n","\n","$$\n","f_2 = \\sigma(W_f * x_2 + W_f * h_1) = \\sigma(0.5*2 + 0.5*0.288) = \\sigma(1.144) \\approx 0.758\n","$$\n","\n","**Step 2: Input Gate**\n","\n","$$\n","i_2 = \\sigma(W_i * x_2 + W_i * h_1) = \\sigma(0.6*2 + 0.6*0.288) = \\sigma(1.373) \\approx 0.797\n","$$\n","\n","**Step 3: Candidate Memory**\n","\n","$$\n","\\tilde{C}_2 = \\tanh(W_c * x_2 + W_c * h_1) = \\tanh(0.9*2 + 0.9*0.288) = \\tanh(1.96) \\approx 0.961\n","$$\n","\n","**Step 4: Cell State Update**\n","\n","$$\n","C_2 = f_2 * C_1 + i_2 * \\tilde{C}_2 = 0.758*0.462 + 0.797*0.961 \\approx 0.350 + 0.766 \\approx 1.116\n","$$\n","\n","**Step 5: Output Gate**\n","\n","$$\n","o_2 = \\sigma(W_o * x_2 + W_o * h_1) = \\sigma(0.7*2 + 0.7*0.288) = \\sigma(1.801) \\approx 0.858\n","$$\n","\n","**Step 6: Hidden State**\n","\n","$$\n","h_2 = o_2 * \\tanh(C_2) = 0.858 * \\tanh(1.116) \\approx 0.858 * 0.806 \\approx 0.691\n","$$\n","\n","After second time step: $C_2 \\approx 1.116$, $h_2 \\approx 0.691$\n","\n","\n","**Summary Table**\n","\n","\n","| Time step | Input | Forget $f_t$ | Input $i_t$ | Candidate $\\tilde{C}_t$ | Cell $C_t$ | Output $o_t$ | Hidden $h_t$ |\n","|-----------|-------|--------------|-------------|-------------------------|------------|--------------|---------------|\n","| 1         | 1     | 0.622        | 0.645       | 0.716                   | 0.462      | 0.668        | 0.288         |\n","| 2         | 2     | 0.758        | 0.797       | 0.961                   | 1.116      | 0.858        | 0.691         |\n","\n","\n","**What This Shows**\n","\n","1. Cell state $C_t$ accumulates long-term memory gradually  \n","2. Hidden state $h_t$ is controlled output at each step  \n","3. Gates regulate memory flow (forget old info, add new info, control output)  \n","4. LSTM prevents vanishing gradients due to additive updates in $C_t$  \n"]},{"cell_type":"markdown","id":"fa8e84df","metadata":{"papermill":{"duration":0.003964,"end_time":"2026-01-05T10:54:12.473492","exception":false,"start_time":"2026-01-05T10:54:12.469528","status":"completed"},"tags":[]},"source":["## LSTM vs RNN\n","\n","| Aspect | RNN | LSTM |\n","|----|----|----|\n","| Memory | Short | Long + Short |\n","| Vanishing Gradient | Yes |  Controlled |\n","| Gates | No |  Yes |\n","| Complexity | Simple | More parameters |\n","| Performance | Limited | Strong on sequences |\n"]},{"cell_type":"markdown","id":"3a59efe3","metadata":{"papermill":{"duration":0.003872,"end_time":"2026-01-05T10:54:12.481297","exception":false,"start_time":"2026-01-05T10:54:12.477425","status":"completed"},"tags":[]},"source":["#  Gated Recurrent Unit (GRU)\n","\n","GRU is a simplified version of LSTM designed to solve the vanishing gradient problem while being computationally lighter.\n","\n","- GRU is a type of RNN that controls memory using gates, similar to LSTM.  \n","- Differences from LSTM:\n","  1. **No separate cell state**: only a hidden state $h_t$.\n","  2. **Fewer gates**: combines forget + input gate into update gate.\n","  3. Faster to train and requires fewer parameters.  \n","\n","Core idea:  \n","\n","> Decide what to keep from the past and what to update from new input, using fewer gates.\n"]},{"cell_type":"markdown","id":"9d445a48","metadata":{"papermill":{"duration":0.004593,"end_time":"2026-01-05T10:54:12.489744","exception":false,"start_time":"2026-01-05T10:54:12.485151","status":"completed"},"tags":[]},"source":["## Components of GRU\n","\n","A GRU cell has two main gates:\n","\n","1. **Update gate ($z_t$)**: controls how much of the previous hidden state to keep\n","\n","   $$\n","   z_t = \\sigma(W_z [h_{t-1}, x_t] + b_z)\n","   $$\n","   \n","2. **Reset gate ($r_t$)**: controls how much of previous hidden state to combine with current input\n","\n","   $$\n","   r_t = \\sigma(W_r [h_{t-1}, x_t] + b_r)\n","   $$\n","\n","3. **Candidate hidden state ($\\tilde{h}_t$)**: new information to be added\n","\n","   $$\n","   \\tilde{h}_t = \\tanh(W_h [r_t * h_{t-1}, x_t] + b_h)\n","   $$\n","\n","4. **Final hidden state ($h_t$)**: combination of previous state and candidate\n","\n","   $$\n","   h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t\n","   $$\n","\n","\n","Intuition:\n","\n","- **Update gate $z_t$:** ‚ÄúHow much memory from the past should I keep?‚Äù  \n","- **Reset gate $r_t$:** ‚ÄúHow much of previous memory should I forget when calculating new info?‚Äù  \n","- **Candidate $\\tilde{h}_t$:** new info computed with selective memory  \n","- **Hidden state $h_t$:** mix of old and new memory controlled by $z_t$"]},{"cell_type":"markdown","id":"1571b135","metadata":{"papermill":{"duration":0.004175,"end_time":"2026-01-05T10:54:12.498347","exception":false,"start_time":"2026-01-05T10:54:12.494172","status":"completed"},"tags":[]},"source":["## How GRU Works\n","\n","For each time step:\n","\n","1. **Compute update gate** $z_t$ ‚Üí decide proportion of old hidden state to keep  \n","2. **Compute reset gate** $r_t$ ‚Üí decide how much past info affects candidate  \n","3. **Compute candidate hidden state** $\\tilde{h}_t$ using $r_t$  \n","4. **Update hidden state** $h_t$ ‚Üí weighted sum of previous hidden and candidate  \n","\n","> GRU merges LSTM‚Äôs forget and input gates into one update gate, so it‚Äôs simpler.\n"]},{"cell_type":"markdown","id":"a67286cb","metadata":{"papermill":{"duration":0.00402,"end_time":"2026-01-05T10:54:12.506837","exception":false,"start_time":"2026-01-05T10:54:12.502817","status":"completed"},"tags":[]},"source":["## Manual Example of GRU\n","\n","**Setup:**\n","\n","- Sequence: `x = [1, 2]`  \n","- Previous hidden: `h0 = 0`  \n","- Weights (scalars):\n","\n","| Gate | Weight | Bias |\n","|------|--------|------|\n","| Update $W_z$ | 0.6 | 0 |\n","| Reset $W_r$ | 0.5 | 0 |\n","| Candidate $W_h$ | 0.9 | 0 |\n","\n","- Activation: sigmoid & tanh  \n","\n","\n","<u>**Time Step 1: Input = 1**</u>\n","\n","1. **Update gate**\n","$$\n","z_1 = \\sigma(W_z * x_1 + W_z * h_0) = \\sigma(0.6*1 + 0.6*0) = \\sigma(0.6) \\approx 0.645\n","$$\n","\n","2. **Reset gate**\n","$$\n","r_1 = \\sigma(W_r * x_1 + W_r * h_0) = \\sigma(0.5*1 + 0) = \\sigma(0.5) \\approx 0.622\n","$$\n","\n","3. **Candidate hidden state**\n","$$\n","\\tilde{h}_1 = \\tanh(W_h * (r_1 * h_0 + x_1)) = \\tanh(0.9 * (0.622*0 + 1)) = \\tanh(0.9) \\approx 0.716\n","$$\n","\n","4. **Hidden state**\n","$$\n","h_1 = (1 - z_1) * h_0 + z_1 * \\tilde{h}_1 = (1 - 0.645)*0 + 0.645*0.716 \\approx 0.462\n","$$\n","\n","\n","<u>**Time Step 2: Input = 2**</u>\n","\n","1. **Update gate**\n","$$\n","z_2 = \\sigma(0.6*2 + 0.6*0.462) = \\sigma(1.677) \\approx 0.841\n","$$\n","\n","2. **Reset gate**\n","$$\n","r_2 = \\sigma(0.5*2 + 0.5*0.462) = \\sigma(1.231) \\approx 0.774\n","$$\n","\n","3. **Candidate hidden state**\n","$$\n","\\tilde{h}_2 = \\tanh(0.9*(r_2*h_1 + x_2)) = \\tanh(0.9*(0.774*0.462 + 2)) = \\tanh(1.816) \\approx 0.948\n","$$\n","\n","4. **Hidden state**\n","$$\n","h_2 = (1 - z_2) * h_1 + z_2 * \\tilde{h}_2 = (1 - 0.841)*0.462 + 0.841*0.948 \\approx 0.901\n","$$\n","\n","**Summary Table**\n","\n","\n","| Time step | Input | $z_t$ | $r_t$ | Candidate $\\tilde{h}_t$ | Hidden $h_t$ |\n","|-----------|-------|-----|-----|-------------------------|------------|\n","| 1         | 1     | 0.645 | 0.622 | 0.716 | 0.462 |\n","| 2         | 2     | 0.841 | 0.774 | 0.948 | 0.901 |\n","\n","\n","The hidden state grows, incorporating both previous memory and new input. GRU is simpler than LSTM because it has no separate cell state.\n"]},{"cell_type":"markdown","id":"06a1a806","metadata":{"papermill":{"duration":0.004155,"end_time":"2026-01-05T10:54:12.514959","exception":false,"start_time":"2026-01-05T10:54:12.510804","status":"completed"},"tags":[]},"source":["## LSTM vs GRU\n","\n","\n","| Aspect | LSTM | GRU |\n","|--------|------|-----|\n","| Gates | 3 (forget, input, output) | 2 (update, reset) |\n","| Cell State | Separate $C_t$ | No separate cell; only hidden state |\n","| Complexity | More parameters | Fewer parameters ‚Üí faster |\n","| Memory Control | Fine-grained (long + short term) | Combined memory (less flexible) |\n","| Training Speed | Slower | Faster |\n","| Performance | Slightly better on very long sequences | Comparable in practice |\n","| Use Case | When long-term dependencies are crucial | When data is smaller or speed is important |\n","\n","\n","- LSTM = heavy-duty memory machine  \n","- GRU = light, fast, almost as effective, simpler to implement  "]},{"cell_type":"markdown","id":"d3efae5a","metadata":{"papermill":{"duration":0.00398,"end_time":"2026-01-05T10:54:12.523472","exception":false,"start_time":"2026-01-05T10:54:12.519492","status":"completed"},"tags":[]},"source":["# What to Remember About RNN, LSTM, and GRU\n","\n","\n","1Ô∏è‚É£ **The ONE Thing You Must Never Forget**\n","\n","- **Vanilla RNN fails** ‚Üí vanishing gradients ‚Üí short memory\n","- **LSTM exists** ‚Üí protect long-term information\n","- **GRU exists** ‚Üí simpler, faster alternative to LSTM\n","\n","If you remember nothing else, remember this causal chain.\n","\n","2Ô∏è‚É£ **One-Line Mental Models**\n","\n","**Vanilla RNN**\n","> ‚ÄúHidden state is repeatedly multiplied ‚Üí gradients die.‚Äù\n","\n","**LSTM**\n","> ‚ÄúSeparate memory highway + gates decide what to keep, add, and expose.‚Äù\n","\n","**GRU**\n","> ‚ÄúSingle memory state that blends old and new information efficiently.‚Äù\n","\n","\n","3Ô∏è‚É£ **Structural Facts Worth Storing**\n","\n","**LSTM > remember ONLY these**\n","- Two states:\n","  - **Cell state ($C_t$)** ‚Üí long-term memory\n","  - **Hidden state ($h_t$)** ‚Üí output / short-term memory\n","- Gates are control valves, not math:\n","  - Forget ‚Üí erase memory\n","  - Input ‚Üí write memory\n","  - Output ‚Üí expose memory\n","- Additive memory update ‚Üí gradients survive\n","\n","**GRU > remember ONLY these**\n","- One state: **hidden state**\n","- Two gates:\n","  - **Update gate** ‚Üí how much past to keep\n","  - **Reset gate** ‚Üí how much past to ignore\n","- No separate cell state ‚Üí faster, simpler\n"]},{"cell_type":"markdown","id":"ff572f25","metadata":{"papermill":{"duration":0.003863,"end_time":"2026-01-05T10:54:12.531523","exception":false,"start_time":"2026-01-05T10:54:12.52766","status":"completed"},"tags":[]},"source":["# Practical Guidelines\n","\n","- Use **RNN** ‚Üí very short sequences, teaching concepts\n","- Use **LSTM** ‚Üí long dependencies, language, time series\n","- Use **GRU** ‚Üí limited data, faster training\n","\n","Industry default:\n","> Start with GRU, move to LSTM if needed\n"]},{"cell_type":"markdown","id":"148f608f","metadata":{"papermill":{"duration":0.003775,"end_time":"2026-01-05T10:54:12.539334","exception":false,"start_time":"2026-01-05T10:54:12.535559","status":"completed"},"tags":[]},"source":["# Key Takeaways from Day 25\n","\n","- Vanilla RNNs fail due to vanishing gradients\n","- LSTM introduces gated memory control\n","- Forget gate is the most critical innovation\n","- Cell state enables long-term dependency learning\n","- GRU is a lighter alternative to LSTM\n","\n","---"]},{"cell_type":"markdown","id":"a6378d59","metadata":{"papermill":{"duration":0.00379,"end_time":"2026-01-05T10:54:12.547021","exception":false,"start_time":"2026-01-05T10:54:12.543231","status":"completed"},"tags":[]},"source":["<p style=\"text-align:center; font-size:18px;\">\n","¬© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":5.167306,"end_time":"2026-01-05T10:54:12.972229","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-01-05T10:54:07.804923","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}