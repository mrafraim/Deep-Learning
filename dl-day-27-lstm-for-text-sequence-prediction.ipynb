{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-27-lstm-for-text-sequence-prediction?scriptVersionId=290688544\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"717494ae","metadata":{"papermill":{"duration":0.004347,"end_time":"2026-01-08T06:28:45.264331","exception":false,"start_time":"2026-01-08T06:28:45.259984","status":"completed"},"tags":[]},"source":["# Day 27: LSTM for Text Sequence Prediction\n","\n","\n","Today you’ll learn:\n","1. How text is converted into numbers for neural networks\n","2. What a character-level text model is\n","3. How LSTM processes text one character at a time\n","4. How to build an LSTM model in PyTorch\n","5. How sequence → memory → next-character prediction works\n","6. Why LSTM is better than RNN for text\n","\n","By the end of this notebook, you will understand how language models begin.\n","\n","If you found this notebook helpful, your **<b style=\"color:orange;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"7b97c12a","metadata":{"papermill":{"duration":0.003054,"end_time":"2026-01-08T06:28:45.270813","exception":false,"start_time":"2026-01-08T06:28:45.267759","status":"completed"},"tags":[]},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":1,"id":"7a854661","metadata":{"execution":{"iopub.execute_input":"2026-01-08T06:28:45.279956Z","iopub.status.busy":"2026-01-08T06:28:45.279645Z","iopub.status.idle":"2026-01-08T06:28:49.894648Z","shell.execute_reply":"2026-01-08T06:28:49.893646Z"},"papermill":{"duration":4.622156,"end_time":"2026-01-08T06:28:49.896947","exception":false,"start_time":"2026-01-08T06:28:45.274791","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np"]},{"cell_type":"markdown","id":"d70b76ac","metadata":{"papermill":{"duration":0.003161,"end_time":"2026-01-08T06:28:49.903832","exception":false,"start_time":"2026-01-08T06:28:49.900671","status":"completed"},"tags":[]},"source":["# Problem Setup\n","\n","We are training a next-character prediction model.\n","\n","Rule:\n","> Given characters up to time t, predict the character at time t+1.\n","\n","Example:\n","\n","- Input text: \"hello\"\n","- Training input: \"hell\"\n","- Training target: \"ello\"\n","\n","Meaning:\n","- Given characters so far → predict the next character\n","- This is the foundation of text generation\n"]},{"cell_type":"markdown","id":"98502a83","metadata":{"papermill":{"duration":0.003058,"end_time":"2026-01-08T06:28:49.910106","exception":false,"start_time":"2026-01-08T06:28:49.907048","status":"completed"},"tags":[]},"source":["# Prepare Text Data"]},{"cell_type":"code","execution_count":2,"id":"ba1a1fd5","metadata":{"execution":{"iopub.execute_input":"2026-01-08T06:28:49.91826Z","iopub.status.busy":"2026-01-08T06:28:49.917861Z","iopub.status.idle":"2026-01-08T06:28:49.92341Z","shell.execute_reply":"2026-01-08T06:28:49.922395Z"},"papermill":{"duration":0.012115,"end_time":"2026-01-08T06:28:49.925364","exception":false,"start_time":"2026-01-08T06:28:49.913249","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["number of unique characters in our dataset.:  4\n","Index mapping:  {'l': 0, 'o': 1, 'h': 2, 'e': 3}\n"]}],"source":["text = \"hello\"\n","\n","chars = list(set(text))\n","vocab_size = len(chars)\n","\n","# Character ↔ index mappings\n","char_to_idx = {ch: i for i, ch in enumerate(chars)}\n","idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n","\n","print(\"number of unique characters in our dataset.: \",vocab_size)\n","print(\"Index mapping: \", char_to_idx)\n"]},{"cell_type":"markdown","id":"4e796cb2","metadata":{"papermill":{"duration":0.00333,"end_time":"2026-01-08T06:28:49.932467","exception":false,"start_time":"2026-01-08T06:28:49.929137","status":"completed"},"tags":[]},"source":["Explanation:\n","\n","- Each character becomes a class\n","- This is a classification problem, not regression"]},{"cell_type":"markdown","id":"069ef6de","metadata":{"papermill":{"duration":0.003144,"end_time":"2026-01-08T06:28:49.939545","exception":false,"start_time":"2026-01-08T06:28:49.936401","status":"completed"},"tags":[]},"source":["# Encode Input & Target Sequences"]},{"cell_type":"code","execution_count":3,"id":"0603d866","metadata":{"execution":{"iopub.execute_input":"2026-01-08T06:28:49.947376Z","iopub.status.busy":"2026-01-08T06:28:49.947121Z","iopub.status.idle":"2026-01-08T06:28:50.001746Z","shell.execute_reply":"2026-01-08T06:28:50.00038Z"},"papermill":{"duration":0.060923,"end_time":"2026-01-08T06:28:50.003692","exception":false,"start_time":"2026-01-08T06:28:49.942769","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Input sequence line:  tensor([[2, 3, 0, 0]])\n","Target sequence line:  tensor([[3, 0, 0, 1]])\n"]}],"source":["# Input: h e l l\n","# Target: e l l o\n","input_seq = torch.tensor([[char_to_idx[c] for c in text[:-1]]]) # text[:-1] → all except last\n","target_seq = torch.tensor([[char_to_idx[c] for c in text[1:]]]) # text[1:] → all except first\n","\n","print(\"Input sequence line: \", input_seq)\n","print(\"Target sequence line: \", target_seq)\n"]},{"cell_type":"code","execution_count":4,"id":"fcb1405e","metadata":{"execution":{"iopub.execute_input":"2026-01-08T06:28:50.012574Z","iopub.status.busy":"2026-01-08T06:28:50.012256Z","iopub.status.idle":"2026-01-08T06:28:50.016647Z","shell.execute_reply":"2026-01-08T06:28:50.015781Z"},"papermill":{"duration":0.010912,"end_time":"2026-01-08T06:28:50.018324","exception":false,"start_time":"2026-01-08T06:28:50.007412","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Input Shape:  torch.Size([1, 4])\n","Target Shape:  torch.Size([1, 4])\n"]}],"source":["print(\"Input Shape: \", input_seq.size())\n","print(\"Target Shape: \", target_seq.size())"]},{"cell_type":"markdown","id":"73357527","metadata":{"papermill":{"duration":0.003419,"end_time":"2026-01-08T06:28:50.025681","exception":false,"start_time":"2026-01-08T06:28:50.022262","status":"completed"},"tags":[]},"source":["Shape meaning:\n","\n","- Batch size = 1\n","- Sequence length = 4"]},{"cell_type":"markdown","id":"8a3b045d","metadata":{"papermill":{"duration":0.003338,"end_time":"2026-01-08T06:28:50.032517","exception":false,"start_time":"2026-01-08T06:28:50.029179","status":"completed"},"tags":[]},"source":["# One-Hot Encoding"]},{"cell_type":"code","execution_count":5,"id":"45c2215e","metadata":{"execution":{"iopub.execute_input":"2026-01-08T06:28:50.040902Z","iopub.status.busy":"2026-01-08T06:28:50.040648Z","iopub.status.idle":"2026-01-08T06:28:50.100173Z","shell.execute_reply":"2026-01-08T06:28:50.099269Z"},"papermill":{"duration":0.066063,"end_time":"2026-01-08T06:28:50.101968","exception":false,"start_time":"2026-01-08T06:28:50.035905","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(tensor([[[0., 0., 1., 0.],\n","          [0., 0., 0., 1.],\n","          [1., 0., 0., 0.],\n","          [1., 0., 0., 0.]]]),\n"," torch.Size([1, 4, 4]))"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# (batch_size, sequence_length, vocab_size)\n","input_onehot = torch.zeros(1, input_seq.size(1), vocab_size) # (1, 4, 4)\n","\n","for t in range(input_seq.size(1)):\n","    input_onehot[0, t, input_seq[0, t]] = 1\n","\n","input_onehot, input_onehot.size()\n"]},{"cell_type":"markdown","id":"6b4111af","metadata":{"papermill":{"duration":0.003273,"end_time":"2026-01-08T06:28:50.108973","exception":false,"start_time":"2026-01-08T06:28:50.1057","status":"completed"},"tags":[]},"source":["- 1 → batch size (number of sequences fed at once)\n","- 4 → sequence length (number of time steps in the sequence)\n","- 4 → vocab_size (dimension of each input vector at a time step)\n","\n","Why one-hot?\n","\n","- Characters are categorical\n","- No numerical ordering exists"]},{"cell_type":"markdown","id":"19097d86","metadata":{"papermill":{"duration":0.003306,"end_time":"2026-01-08T06:28:50.115683","exception":false,"start_time":"2026-01-08T06:28:50.112377","status":"completed"},"tags":[]},"source":["# Define LSTM Model"]},{"cell_type":"code","execution_count":6,"id":"119a7756","metadata":{"execution":{"iopub.execute_input":"2026-01-08T06:28:50.124032Z","iopub.status.busy":"2026-01-08T06:28:50.123771Z","iopub.status.idle":"2026-01-08T06:28:50.12963Z","shell.execute_reply":"2026-01-08T06:28:50.128375Z"},"papermill":{"duration":0.012752,"end_time":"2026-01-08T06:28:50.131667","exception":false,"start_time":"2026-01-08T06:28:50.118915","status":"completed"},"tags":[]},"outputs":[],"source":["class CharLSTM(nn.Module):\n","\n","    # vocab_size → number of unique characters (input & output size)\n","    # hidden_size → number of memory units in LSTM (capacity of hidden state)\n","    def __init__(self, vocab_size, hidden_size): \n","        super().__init__()\n","        self.lstm = nn.LSTM(\n","            input_size=vocab_size,   # Each character is represented as a one-hot vector (length = vocab_size).\n","            hidden_size=hidden_size, # Number of hidden units\n","            batch_first=True         # Ensures input shape is (batch, seq_len, input_size)\n","        )\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, x):\n","        # out → hidden states at all time steps (shape = [batch, seq_len, hidden_size])\n","        # h_n → hidden state at last time step (short-term memory)\n","        # c_n → cell state at last time step (long-term memory) \n","        out, (h_n, c_n) = self.lstm(x)\n","        out = self.fc(out)\n","        return out\n"]},{"cell_type":"markdown","id":"a387f184","metadata":{"papermill":{"duration":0.0034,"end_time":"2026-01-08T06:28:50.1391","exception":false,"start_time":"2026-01-08T06:28:50.1357","status":"completed"},"tags":[]},"source":["Key idea:\n","\n","- LSTM returns output at every time step\n","- We predict a character at each step"]},{"cell_type":"markdown","id":"1975e1e8","metadata":{"papermill":{"duration":0.003337,"end_time":"2026-01-08T06:28:50.146031","exception":false,"start_time":"2026-01-08T06:28:50.142694","status":"completed"},"tags":[]},"source":["# Initialize Model, Loss, Optimizer"]},{"cell_type":"code","execution_count":7,"id":"cfa1434c","metadata":{"execution":{"iopub.execute_input":"2026-01-08T06:28:50.154212Z","iopub.status.busy":"2026-01-08T06:28:50.153968Z","iopub.status.idle":"2026-01-08T06:28:55.524242Z","shell.execute_reply":"2026-01-08T06:28:55.523518Z"},"papermill":{"duration":5.37663,"end_time":"2026-01-08T06:28:55.525957","exception":false,"start_time":"2026-01-08T06:28:50.149327","status":"completed"},"tags":[]},"outputs":[],"source":["model = CharLSTM(vocab_size=vocab_size, hidden_size=16)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"]},{"cell_type":"markdown","id":"35bcc8f9","metadata":{"papermill":{"duration":0.003234,"end_time":"2026-01-08T06:28:55.53294","exception":false,"start_time":"2026-01-08T06:28:55.529706","status":"completed"},"tags":[]},"source":["# Forward Pass"]},{"cell_type":"code","execution_count":8,"id":"53a24f05","metadata":{"execution":{"iopub.execute_input":"2026-01-08T06:28:55.541359Z","iopub.status.busy":"2026-01-08T06:28:55.540989Z","iopub.status.idle":"2026-01-08T06:28:55.667532Z","shell.execute_reply":"2026-01-08T06:28:55.666528Z"},"papermill":{"duration":0.132768,"end_time":"2026-01-08T06:28:55.669237","exception":false,"start_time":"2026-01-08T06:28:55.536469","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([1, 4, 4])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["outputs = model(input_onehot)\n","\n","outputs.shape\n"]},{"cell_type":"markdown","id":"df64b8f4","metadata":{"papermill":{"duration":0.003548,"end_time":"2026-01-08T06:28:55.676514","exception":false,"start_time":"2026-01-08T06:28:55.672966","status":"completed"},"tags":[]},"source":["Output shape:\n","\n","`(batch_size, sequence_length, vocab_size)`\n","\n","\n","Each time step predicts a probability distribution over characters."]},{"cell_type":"markdown","id":"a7564b79","metadata":{"papermill":{"duration":0.003399,"end_time":"2026-01-08T06:28:55.683317","exception":false,"start_time":"2026-01-08T06:28:55.679918","status":"completed"},"tags":[]},"source":["# Compute Loss"]},{"cell_type":"code","execution_count":9,"id":"4f92242f","metadata":{"execution":{"iopub.execute_input":"2026-01-08T06:28:55.691636Z","iopub.status.busy":"2026-01-08T06:28:55.691359Z","iopub.status.idle":"2026-01-08T06:28:55.697262Z","shell.execute_reply":"2026-01-08T06:28:55.696346Z"},"papermill":{"duration":0.012336,"end_time":"2026-01-08T06:28:55.699078","exception":false,"start_time":"2026-01-08T06:28:55.686742","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([[[0.0046, 0.0942, 0.1986, 0.1661],\n","         [0.0212, 0.1359, 0.1557, 0.1251],\n","         [0.0220, 0.1570, 0.1561, 0.1202],\n","         [0.0241, 0.1693, 0.1559, 0.1160]]], grad_fn=<ViewBackward0>)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["outputs"]},{"cell_type":"code","execution_count":10,"id":"0dda11e8","metadata":{"execution":{"iopub.execute_input":"2026-01-08T06:28:55.709018Z","iopub.status.busy":"2026-01-08T06:28:55.708051Z","iopub.status.idle":"2026-01-08T06:28:55.723535Z","shell.execute_reply":"2026-01-08T06:28:55.722394Z"},"papermill":{"duration":0.022127,"end_time":"2026-01-08T06:28:55.725169","exception":false,"start_time":"2026-01-08T06:28:55.703042","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["1.4073045253753662"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["loss = criterion(\n","    outputs.view(-1, vocab_size),\n","    target_seq.view(-1)\n",")\n","\n","loss.item()\n"]},{"cell_type":"markdown","id":"9dd69109","metadata":{"papermill":{"duration":0.003535,"end_time":"2026-01-08T06:28:55.732696","exception":false,"start_time":"2026-01-08T06:28:55.729161","status":"completed"},"tags":[]},"source":["Explanation:\n","\n","- Flatten sequence dimension\n","- Compare predicted vs true characters"]},{"cell_type":"markdown","id":"a9b53280","metadata":{"papermill":{"duration":0.003479,"end_time":"2026-01-08T06:28:55.739857","exception":false,"start_time":"2026-01-08T06:28:55.736378","status":"completed"},"tags":[]},"source":["# Training Loop"]},{"cell_type":"code","execution_count":11,"id":"f6c79ca1","metadata":{"execution":{"iopub.execute_input":"2026-01-08T06:28:55.748499Z","iopub.status.busy":"2026-01-08T06:28:55.748207Z","iopub.status.idle":"2026-01-08T06:28:56.108025Z","shell.execute_reply":"2026-01-08T06:28:56.107056Z"},"papermill":{"duration":0.36586,"end_time":"2026-01-08T06:28:56.109249","exception":false,"start_time":"2026-01-08T06:28:55.743389","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Loss: 1.4073\n","Epoch 50, Loss: 0.0776\n","Epoch 100, Loss: 0.0084\n","Epoch 150, Loss: 0.0039\n","Epoch 200, Loss: 0.0024\n","Epoch 250, Loss: 0.0016\n"]}],"source":["for epoch in range(300):\n","    optimizer.zero_grad()\n","    outputs = model(input_onehot)\n","    \n","    loss = criterion(\n","        outputs.view(-1, vocab_size),\n","        target_seq.view(-1)\n","    )\n","    \n","    loss.backward()\n","    optimizer.step()\n","\n","    if epoch % 50 == 0:\n","        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"]},{"cell_type":"markdown","id":"4beb9259","metadata":{"papermill":{"duration":0.003788,"end_time":"2026-01-08T06:28:56.11685","exception":false,"start_time":"2026-01-08T06:28:56.113062","status":"completed"},"tags":[]},"source":["This demonstrates:\n","\n","- Backpropagation Through Time\n","- LSTM learning character transitions"]},{"cell_type":"markdown","id":"002ac673","metadata":{"papermill":{"duration":0.004166,"end_time":"2026-01-08T06:28:56.126893","exception":false,"start_time":"2026-01-08T06:28:56.122727","status":"completed"},"tags":[]},"source":["# Text Prediction"]},{"cell_type":"code","execution_count":12,"id":"1c2eb894","metadata":{"execution":{"iopub.execute_input":"2026-01-08T06:28:56.138014Z","iopub.status.busy":"2026-01-08T06:28:56.13774Z","iopub.status.idle":"2026-01-08T06:28:56.154983Z","shell.execute_reply":"2026-01-08T06:28:56.154121Z"},"papermill":{"duration":0.024672,"end_time":"2026-01-08T06:28:56.156692","exception":false,"start_time":"2026-01-08T06:28:56.13202","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["'ello'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["with torch.no_grad():\n","    outputs = model(input_onehot)\n","    predicted_indices = torch.argmax(outputs, dim=2)\n","\n","predicted_text = \"\".join(idx_to_char[i.item()] for i in predicted_indices[0])\n","predicted_text\n"]},{"cell_type":"markdown","id":"9cba8a64","metadata":{"papermill":{"duration":0.003929,"end_time":"2026-01-08T06:28:56.166428","exception":false,"start_time":"2026-01-08T06:28:56.162499","status":"completed"},"tags":[]},"source":["Expected behavior:\n","\n","- Output should converge toward \"ello\""]},{"cell_type":"markdown","id":"aa6bd5d9","metadata":{"papermill":{"duration":0.003943,"end_time":"2026-01-08T06:28:56.175253","exception":false,"start_time":"2026-01-08T06:28:56.17131","status":"completed"},"tags":[]},"source":["# Why LSTM Works Here\n","\n","- Text has long-term dependencies\n","- Vanilla RNN forgets early characters\n","- LSTM cell state preserves memory\n","- Gates control information flow\n"]},{"cell_type":"markdown","id":"63e4d273","metadata":{"papermill":{"duration":0.005405,"end_time":"2026-01-08T06:28:56.184532","exception":false,"start_time":"2026-01-08T06:28:56.179127","status":"completed"},"tags":[]},"source":["# Optional: Memorize ONLY these 5 patterns\n","\n","That’s all experts carry in their head:\n","\n","- Sequence input → (B, T, F)\n","- RNN/LSTM output → (B, T, H)\n","- Hidden state → (num_layers, B, H)\n","- Linear layer → changes last dim only\n","- CrossEntropy → (N, C) vs (N,)\n","- Everything else is derived."]},{"cell_type":"markdown","id":"b26edb90","metadata":{"papermill":{"duration":0.005976,"end_time":"2026-01-08T06:28:56.196543","exception":false,"start_time":"2026-01-08T06:28:56.190567","status":"completed"},"tags":[]},"source":["# Key Takeaways from Day 2\n","\n","- Text modeling is a sequence prediction problem\n","- Characters are treated as classes\n","- LSTM processes text step by step\n","- Outputs predict the next character\n","- This is the foundation of text generation\n","\n","---"]},{"cell_type":"markdown","id":"554e6252","metadata":{"papermill":{"duration":0.003622,"end_time":"2026-01-08T06:28:56.204165","exception":false,"start_time":"2026-01-08T06:28:56.200543","status":"completed"},"tags":[]},"source":["<p style=\"text-align:center; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":16.435358,"end_time":"2026-01-08T06:28:58.66556","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-01-08T06:28:42.230202","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}