{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-11-backpropagation-in-numpy?scriptVersionId=287027922\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"10bc066c","metadata":{"papermill":{"duration":0.00513,"end_time":"2025-12-18T11:01:51.915623","exception":false,"start_time":"2025-12-18T11:01:51.910493","status":"completed"},"tags":[]},"source":["# Day 11: Backpropagation in NumPy\n","\n","Welcome to Day 11!\n","\n","Today you will:\n","- Build a tiny neural network from scratch\n","- Perform forward propagation\n","- Compute loss\n","- Apply backpropagation manually\n","- Update weights step by step\n","\n","This notebook mirrors what deep learning frameworks do internally.\n","\n","---"]},{"cell_type":"markdown","id":"7f22862a","metadata":{"papermill":{"duration":0.003905,"end_time":"2025-12-18T11:01:51.923707","exception":false,"start_time":"2025-12-18T11:01:51.919802","status":"completed"},"tags":[]},"source":["# Tiny Neural Network Architecture\n","\n","We use a 2-layer network:\n","\n","$$Input (2) → Hidden (2, ReLU) → Output (1, Sigmoid)$$\n","\n","Mathematically:\n","\n","Layer 1:\n","$$\n","Z^{(1)} = W^{(1)} X + b^{(1)}\n","$$\n","$$\n","A^{(1)} = ReLU(Z^{(1)})\n","$$\n","\n","Layer 2:\n","$$\n","Z^{(2)} = W^{(2)} A^{(1)} + b^{(2)}\n","$$\n","$$\n","A^{(2)} = \\sigma(Z^{(2)})\n","$$\n"]},{"cell_type":"code","execution_count":1,"id":"212a02ad","metadata":{"execution":{"iopub.execute_input":"2025-12-18T11:01:51.935406Z","iopub.status.busy":"2025-12-18T11:01:51.934688Z","iopub.status.idle":"2025-12-18T11:01:51.944319Z","shell.execute_reply":"2025-12-18T11:01:51.942791Z"},"papermill":{"duration":0.018659,"end_time":"2025-12-18T11:01:51.946977","exception":false,"start_time":"2025-12-18T11:01:51.928318","status":"completed"},"tags":[]},"outputs":[],"source":["# Import numpy library\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"id":"d0f251b2","metadata":{"execution":{"iopub.execute_input":"2025-12-18T11:01:51.956682Z","iopub.status.busy":"2025-12-18T11:01:51.956243Z","iopub.status.idle":"2025-12-18T11:01:51.965126Z","shell.execute_reply":"2025-12-18T11:01:51.963764Z"},"papermill":{"duration":0.01667,"end_time":"2025-12-18T11:01:51.967737","exception":false,"start_time":"2025-12-18T11:01:51.951067","status":"completed"},"tags":[]},"outputs":[],"source":["# Input (2 features, 1 sample)\n","X = np.array([[1.0],\n","              [2.0]])\n","\n","# True label\n","y = np.array([[1.0]])\n","\n","# Initialize weights\n","W1 = np.array([[0.4, -0.2],\n","               [0.1,  0.6]])\n","b1 = np.zeros((2, 1))\n","\n","W2 = np.array([[0.7, -0.3]])\n","b2 = np.zeros((1, 1))"]},{"cell_type":"code","execution_count":3,"id":"195c6bb0","metadata":{"execution":{"iopub.execute_input":"2025-12-18T11:01:51.977499Z","iopub.status.busy":"2025-12-18T11:01:51.977164Z","iopub.status.idle":"2025-12-18T11:01:51.983943Z","shell.execute_reply":"2025-12-18T11:01:51.982694Z"},"papermill":{"duration":0.0145,"end_time":"2025-12-18T11:01:51.986387","exception":false,"start_time":"2025-12-18T11:01:51.971887","status":"completed"},"tags":[]},"outputs":[],"source":["# Activation functions\n","\n","def relu(z):\n","    return np.maximum(0, z)\n","\n","def relu_derivative(z):\n","    return (z > 0).astype(float)\n","\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n"]},{"cell_type":"markdown","id":"e2ead57c","metadata":{"papermill":{"duration":0.003927,"end_time":"2025-12-18T11:01:51.994432","exception":false,"start_time":"2025-12-18T11:01:51.990505","status":"completed"},"tags":[]},"source":["# Forward Propagation\n","\n","We compute:\n","- Hidden layer output\n","- Final prediction\n","- Loss\n","\n","Loss used:\n","$$\n","L = (y - \\hat{y})^2\n","$$\n"]},{"cell_type":"code","execution_count":4,"id":"59f91708","metadata":{"execution":{"iopub.execute_input":"2025-12-18T11:01:52.004753Z","iopub.status.busy":"2025-12-18T11:01:52.004316Z","iopub.status.idle":"2025-12-18T11:01:52.015596Z","shell.execute_reply":"2025-12-18T11:01:52.014509Z"},"papermill":{"duration":0.018953,"end_time":"2025-12-18T11:01:52.017868","exception":false,"start_time":"2025-12-18T11:01:51.998915","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(array([[0. ],\n","        [1.3]]),\n"," array([[0. ],\n","        [1.3]]),\n"," array([[-0.39]]),\n"," array([[0.4037173]]),\n"," array([[0.35555306]]))"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Layer 1\n","Z1 = np.dot(W1, X) + b1\n","A1 = relu(Z1)\n","\n","# Layer 2\n","Z2 = np.dot(W2, A1) + b2\n","A2 = sigmoid(Z2)\n","\n","# Loss (MSE)\n","loss = (A2 - y)**2\n","\n","Z1, A1, Z2, A2, loss"]},{"cell_type":"markdown","id":"d0d05da7","metadata":{"papermill":{"duration":0.004069,"end_time":"2025-12-18T11:01:52.026411","exception":false,"start_time":"2025-12-18T11:01:52.022342","status":"completed"},"tags":[]},"source":["# Backpropagation Flow\n","\n","Backward order:\n","1. Output layer gradients\n","2. Hidden layer gradients\n","3. Parameter gradients\n","4. Weight updates\n","\n","Chain rule governs everything.\n"]},{"cell_type":"markdown","id":"f411d25d","metadata":{"papermill":{"duration":0.003907,"end_time":"2025-12-18T11:01:52.034287","exception":false,"start_time":"2025-12-18T11:01:52.03038","status":"completed"},"tags":[]},"source":["# Output Layer Gradients\n","\n","Loss:\n","$$\n","L = (A^{(2)} - y)^2\n","$$\n","\n","Gradients:\n","$$\n","\\frac{dL}{dA^{(2)}} = 2(A^{(2)} - y)\n","$$\n","$$\n","\\frac{dA^{(2)}}{dZ^{(2)}} = A^{(2)}(1 - A^{(2)})\n","$$\n"]},{"cell_type":"code","execution_count":5,"id":"b4dfae6f","metadata":{"execution":{"iopub.execute_input":"2025-12-18T11:01:52.044103Z","iopub.status.busy":"2025-12-18T11:01:52.043771Z","iopub.status.idle":"2025-12-18T11:01:52.049186Z","shell.execute_reply":"2025-12-18T11:01:52.048259Z"},"papermill":{"duration":0.013045,"end_time":"2025-12-18T11:01:52.051383","exception":false,"start_time":"2025-12-18T11:01:52.038338","status":"completed"},"tags":[]},"outputs":[],"source":["# Output layer gradients\n","dL_dA2 = 2 * (A2 - y)\n","dA2_dZ2 = A2 * (1 - A2)\n","\n","dZ2 = dL_dA2 * dA2_dZ2\n","\n","dW2 = np.dot(dZ2, A1.T)\n","db2 = dZ2\n"]},{"cell_type":"markdown","id":"7ba1819a","metadata":{"papermill":{"duration":0.003959,"end_time":"2025-12-18T11:01:52.059518","exception":false,"start_time":"2025-12-18T11:01:52.055559","status":"completed"},"tags":[]},"source":["# Hidden Layer Gradients\n","\n","Backpropagate error:\n","$$\n","\\frac{dL}{dA^{(1)}} = W^{(2)T} \\frac{dL}{dZ^{(2)}}\n","$$\n","\n","Apply ReLU derivative:\n","$$\n","\\frac{dA^{(1)}}{dZ^{(1)}} =\n","\\begin{cases}\n","1 & Z^{(1)} > 0 \\\\\n","0 & \\text{otherwise}\n","\\end{cases}\n","$$\n"]},{"cell_type":"code","execution_count":6,"id":"54324694","metadata":{"execution":{"iopub.execute_input":"2025-12-18T11:01:52.069388Z","iopub.status.busy":"2025-12-18T11:01:52.069027Z","iopub.status.idle":"2025-12-18T11:01:52.078527Z","shell.execute_reply":"2025-12-18T11:01:52.077516Z"},"papermill":{"duration":0.017332,"end_time":"2025-12-18T11:01:52.080881","exception":false,"start_time":"2025-12-18T11:01:52.063549","status":"completed"},"tags":[]},"outputs":[],"source":["dA1 = np.dot(W2.T, dZ2)\n","dZ1 = dA1 * relu_derivative(Z1)\n","\n","dW1 = np.dot(dZ1, X.T)\n","db1 = dZ1\n"]},{"cell_type":"markdown","id":"290155b8","metadata":{"papermill":{"duration":0.003919,"end_time":"2025-12-18T11:01:52.088889","exception":false,"start_time":"2025-12-18T11:01:52.08497","status":"completed"},"tags":[]},"source":["# Gradient Check (Sanity)\n","\n","Shapes must match parameters:\n","- dW1 → W1\n","- db1 → b1\n","- dW2 → W2\n","- db2 → b2\n"]},{"cell_type":"code","execution_count":7,"id":"d7e9c1f5","metadata":{"execution":{"iopub.execute_input":"2025-12-18T11:01:52.099705Z","iopub.status.busy":"2025-12-18T11:01:52.099346Z","iopub.status.idle":"2025-12-18T11:01:52.107079Z","shell.execute_reply":"2025-12-18T11:01:52.10603Z"},"papermill":{"duration":0.016144,"end_time":"2025-12-18T11:01:52.109514","exception":false,"start_time":"2025-12-18T11:01:52.09337","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["((2, 2), (2, 1), (1, 2), (1, 1))"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["dW1.shape, db1.shape, dW2.shape, db2.shape\n"]},{"cell_type":"markdown","id":"d16b8602","metadata":{"papermill":{"duration":0.004175,"end_time":"2025-12-18T11:01:52.118168","exception":false,"start_time":"2025-12-18T11:01:52.113993","status":"completed"},"tags":[]},"source":["# Gradient Descent Update\n","\n","Update rule:\n","$$\n","\\theta = \\theta - \\alpha \\nabla_\\theta L\n","$$\n"]},{"cell_type":"code","execution_count":8,"id":"5e0e156b","metadata":{"execution":{"iopub.execute_input":"2025-12-18T11:01:52.128905Z","iopub.status.busy":"2025-12-18T11:01:52.128534Z","iopub.status.idle":"2025-12-18T11:01:52.136885Z","shell.execute_reply":"2025-12-18T11:01:52.135692Z"},"papermill":{"duration":0.017195,"end_time":"2025-12-18T11:01:52.139506","exception":false,"start_time":"2025-12-18T11:01:52.122311","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(array([[ 0.4       , -0.2       ],\n","        [ 0.09138742,  0.58277485]]),\n"," array([[ 0.7       , -0.26267884]]))"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["lr = 0.1\n","\n","W1 = W1 - lr * dW1\n","b1 = b1 - lr * db1\n","\n","W2 = W2 - lr * dW2\n","b2 = b2 - lr * db2\n","\n","W1, W2\n"]},{"cell_type":"markdown","id":"f4bf0729","metadata":{"papermill":{"duration":0.00408,"end_time":"2025-12-18T11:01:52.148732","exception":false,"start_time":"2025-12-18T11:01:52.144652","status":"completed"},"tags":[]},"source":["# Result of One Training Step\n","\n","- Loss decreased (if learning rate is reasonable)\n","- Weights updated using gradients\n","- This is exactly how training works internally\n","\n","Repeating this loop = training a neural network.\n"]},{"cell_type":"code","execution_count":9,"id":"b59457e2","metadata":{"execution":{"iopub.execute_input":"2025-12-18T11:01:52.158622Z","iopub.status.busy":"2025-12-18T11:01:52.158241Z","iopub.status.idle":"2025-12-18T11:01:52.166869Z","shell.execute_reply":"2025-12-18T11:01:52.165892Z"},"papermill":{"duration":0.016044,"end_time":"2025-12-18T11:01:52.168885","exception":false,"start_time":"2025-12-18T11:01:52.152841","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(array([[0.35555306]]), array([[0.32975951]]))"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Forward pass again to see loss change\n","Z1 = np.dot(W1, X) + b1\n","A1 = relu(Z1)\n","\n","Z2 = np.dot(W2, A1) + b2\n","A2 = sigmoid(Z2)\n","\n","new_loss = (A2 - y)**2\n","loss, new_loss"]},{"cell_type":"markdown","id":"1807a8b5","metadata":{"papermill":{"duration":0.004567,"end_time":"2025-12-18T11:01:52.178049","exception":false,"start_time":"2025-12-18T11:01:52.173482","status":"completed"},"tags":[]},"source":["# Deep Dive\n","\n","\n","### 0️. Freeze the Graph (This Matters)\n","\n","Before any math, lock this dependency graph in your head:\n","\n","$$\n","W^{(1)}, b^{(1)}\n","\\;\\rightarrow\\;\n","Z^{(1)}\n","\\;\\rightarrow\\;\n","A^{(1)}\n","\\;\\rightarrow\\;\n","W^{(2)}, b^{(2)}\n","\\;\\rightarrow\\;\n","Z^{(2)}\n","\\;\\rightarrow\\;\n","A^{(2)}\n","\\;\\rightarrow\\;\n","L\n","$$\n","\n","Backpropagation is nothing but walking this graph backward.\n","\n","\n","### 1️. Output Layer Gradients\n","\n","Loss function:\n","\n","$$\n","L = (A^{(2)} - y)^2\n","$$\n","\n","Ask the only valid first question:\n","\n","> If the output changes slightly, how does the loss change?\n","\n","That is:\n","\n","$$\n","\\frac{\\partial L}{\\partial A^{(2)}} = 2(A^{(2)} - y)\n","$$\n","\n","This is called the **output layer gradient** because:\n","\n","- Loss depends directly on $A^{(2)}$\n","- No chain rule is needed yet\n","\n","**Output Is Not a Parameter**\n","\n","The output comes from:\n","\n","$$\n","A^{(2)} = \\sigma(Z^{(2)})\n","$$\n","\n","So the gradient must pass through the sigmoid:\n","\n","$$\n","\\frac{\\partial A^{(2)}}{\\partial Z^{(2)}} = A^{(2)}(1 - A^{(2)})\n","$$\n","\n","Now apply the chain rule:\n","\n","$$\n","\\boxed{\n","\\frac{\\partial L}{\\partial Z^{(2)}} =\n","\\frac{\\partial L}{\\partial A^{(2)}}\n","\\cdot\n","\\frac{\\partial A^{(2)}}{\\partial Z^{(2)}}\n","}\n","$$\n","\n","This quantity is often denoted as:\n","\n","$$\n","\\delta^{(2)} \\equiv \\frac{\\partial L}{\\partial Z^{(2)}}\n","$$\n","\n","\n","### 2️. Parameter Gradients at the Output Layer\n","\n","Ask the next forced question:\n","\n","> *Which parameters directly created $Z^{(2)}$?*\n","\n","From the model:\n","\n","$$\n","Z^{(2)} = W^{(2)} A^{(1)} + b^{(2)}\n","$$\n","\n","So the local derivatives are:\n","\n","$$\n","\\frac{\\partial Z^{(2)}}{\\partial W^{(2)}} = A^{(1)},\n","\\quad\n","\\frac{\\partial Z^{(2)}}{\\partial b^{(2)}} = 1\n","$$\n","\n","Apply the chain rule:\n","\n","$$\n","\\boxed{\n","\\frac{\\partial L}{\\partial W^{(2)}} =\n","\\frac{\\partial L}{\\partial Z^{(2)}} \\cdot A^{(1)}\n","}\n","$$\n","\n","$$\n","\\boxed{\n","\\frac{\\partial L}{\\partial b^{(2)}} =\n","\\frac{\\partial L}{\\partial Z^{(2)}}\n","}\n","$$\n","\n","**Important observation**:\n","\n","> No new gradient was created.  \n","> The same gradient simply touched a parameter.\n","\n","### 3️. Hidden Layer Gradients\n","\n","The hidden layer never sees the loss directly.\n","\n","So we must ask:\n","\n","> *If the hidden activation changes, how does the loss change?*\n","\n","Chain rule forces:\n","\n","$$\n","\\frac{\\partial L}{\\partial A^{(1)}} =\n","\\frac{\\partial L}{\\partial Z^{(2)}}\n","\\cdot\n","\\frac{\\partial Z^{(2)}}{\\partial A^{(1)}}\n","$$\n","\n","From:\n","\n","$$\n","Z^{(2)} = W^{(2)} A^{(1)} + b^{(2)}\n","$$\n","\n","We get:\n","\n","$$\n","\\boxed{\n","\\frac{\\partial L}{\\partial A^{(1)}} =\n","(W^{(2)})^T \\frac{\\partial L}{\\partial Z^{(2)}}\n","}\n","$$\n","\n","This is what people casually call the **hidden layer gradient**.\n","\n","But note:\n","\n","- It is not new\n","- It is just the output gradient pushed backward\n","\n","\n","### 4️. ReLU Gate: The Gradient Filter\n","\n","Hidden activation:\n","\n","$$\n","A^{(1)} = \\text{ReLU}(Z^{(1)})\n","$$\n","\n","Derivative of ReLU:\n","\n","$$\n","\\frac{\\partial A^{(1)}}{\\partial Z^{(1)}} =\n","\\begin{cases}\n","1 & Z^{(1)} > 0 \\\\\n","0 & Z^{(1)} \\le 0\n","\\end{cases}\n","$$\n","\n","So:\n","\n","$$\n","\\boxed{\n","\\frac{\\partial L}{\\partial Z^{(1)}} =\n","\\frac{\\partial L}{\\partial A^{(1)}}\n","\\cdot\n","\\frac{\\partial A^{(1)}}{\\partial Z^{(1)}}\n","}\n","$$\n","\n","This is often denoted as:\n","\n","$$\n","\\delta^{(1)} \\equiv \\frac{\\partial L}{\\partial Z^{(1)}}\n","$$\n","\n"," **This is where gradients can die** (ReLU = 0).\n","\n","\n","### 5️. Parameter Gradients at the Hidden Layer\n","\n","From:\n","\n","$$\n","Z^{(1)} = W^{(1)} X + b^{(1)}\n","$$\n","\n","We get:\n","\n","$$\n","\\frac{\\partial Z^{(1)}}{\\partial W^{(1)}} = X,\n","\\quad\n","\\frac{\\partial Z^{(1)}}{\\partial b^{(1)}} = 1\n","$$\n","\n","Thus:\n","\n","$$\n","\\boxed{\n","\\frac{\\partial L}{\\partial W^{(1)}} =\n","\\frac{\\partial L}{\\partial Z^{(1)}} \\cdot X\n","}\n","$$\n","\n","$$\n","\\boxed{\n","\\frac{\\partial L}{\\partial b^{(1)}} =\n","\\frac{\\partial L}{\\partial Z^{(1)}}\n","}\n","$$\n","\n","\n","### 6️. One Complete Chain-Rule Expression\n","\n","For hidden-layer weights $W^{(1)}$:\n","\n","$$\n","\\frac{\\partial L}{\\partial W^{(1)}}\n","=\n","\\frac{\\partial L}{\\partial A^{(2)}}\n","\\cdot\n","\\frac{\\partial A^{(2)}}{\\partial Z^{(2)}}\n","\\cdot\n","\\frac{\\partial Z^{(2)}}{\\partial A^{(1)}}\n","\\cdot\n","\\frac{\\partial A^{(1)}}{\\partial Z^{(1)}}\n","\\cdot\n","\\frac{\\partial Z^{(1)}}{\\partial W^{(1)}}\n","$$\n","\n","\n","**Substituting Each Local Derivative**\n","\n","Loss derivative (Mean Squared Error)\n","\n","$$\n","\\frac{\\partial L}{\\partial A^{(2)}} = 2(A^{(2)} - y)\n","$$\n","\n","Sigmoid activation derivative\n","\n","$$\n","\\frac{\\partial A^{(2)}}{\\partial Z^{(2)}} = A^{(2)}(1 - A^{(2)})\n","$$\n","\n","Output layer linear transformation\n","\n","$$\n","\\frac{\\partial Z^{(2)}}{\\partial A^{(1)}} = (W^{(2)})^T\n","$$\n","\n","ReLU activation gate\n","\n","$$\n","\\frac{\\partial A^{(1)}}{\\partial Z^{(1)}} =\n","\\mathbb{1}(Z^{(1)} > 0)\n","$$\n","\n","Hidden-layer linear transformation\n","\n","$$\n","\\frac{\\partial Z^{(1)}}{\\partial W^{(1)}} = X\n","$$\n","\n","\n","Final Combined Gradient (Hidden-Layer Weights)\n","\n","$$\n","\\boxed{\n","\\frac{\\partial L}{\\partial W^{(1)}} =\n","2(A^{(2)} - y)\n","\\cdot\n","A^{(2)}(1 - A^{(2)})\n","\\cdot\n","(W^{(2)})^T\n","\\cdot\n","\\mathbb{1}(Z^{(1)} > 0)\n","\\cdot\n","X\n","}\n","$$\n","\n","\n","\n","Each term is a **local derivative** (Local derivative = derivative of the neuron's output with respect to its input.) in the dependency graph.\n","\n","\n","### 7️. The Core Insight (This Is the Point)\n","\n","> “Output gradients” and “hidden gradients” are not separate equations.\n","\n","They are:\n","\n","- Partial products inside one long chain-rule\n","- Checkpoints humans name for convenience\n","\n","Mathematically:\n","\n","> There is only one gradient signal, repeatedly transformed.\n","\n","\n","Final Sentence to Remember\n","\n","> Backpropagation is one gradient flowing backward, multiplied by local derivatives until it reaches a parameter.\n","\n"]},{"cell_type":"markdown","id":"0ddc5985","metadata":{"papermill":{"duration":0.004323,"end_time":"2025-12-18T11:01:52.186771","exception":false,"start_time":"2025-12-18T11:01:52.182448","status":"completed"},"tags":[]},"source":["# General Backpropagation Formula (Optional)\n","\n","Here we establish a complete, layer-wise, engineer-level formula for backpropagation, applicable to any fully connected neural network of depth $L$.\n","\n","### 0️. Network Definition\n","\n","Consider a neural network with $L$ layers. For each layer $l = 1, 2, ..., L$, we define:\n","\n","$$\n","Z^{(l)} = W^{(l)} A^{(l-1)} + b^{(l)}\n","$$\n","\n","$$\n","A^{(l)} = f^{(l)}(Z^{(l)})\n","$$\n","\n","Here:\n","\n","- $A^{(0)} = X$ is the input layer\n","- $f^{(l)}$ is the activation function for layer $l$ (ReLU, Sigmoid, Tanh, etc.)\n","- $W^{(l)}$ and $b^{(l)}$ are the trainable weights and biases for layer $l$\n","- $Z^{(l)}$ is the pre-activation (weighted input) of layer $l$\n","\n","The loss function is:\n","\n","$$\n","L = \\mathcal{L}(A^{(L)}, y)\n","$$\n","\n","where $y$ is the ground truth and $A^{(L)}$ is the output of the network.\n","\n","\n","### 1️. Single Gradient Source\n","\n","The only point where the gradient originates is at the **loss**:\n","\n","$$\n","\\frac{\\partial L}{\\partial A^{(L)}} = \\frac{\\partial \\mathcal{L}}{\\partial A^{(L)}}\n","$$\n","\n","Everything else in backpropagation is just **propagating this gradient** backward through the network using local derivatives.\n","\n","\n","### 2️. Define the Error Signal\n","\n","For clarity, define a layer-wise error signal for each layer $l$ as:\n","\n","$$\n","\\delta^{(l)} \\equiv \\frac{\\partial L}{\\partial Z^{(l)}}\n","$$\n","\n","This $\\delta^{(l)}$ is not a new gradient, but simply a checkpoint representing the gradient of the loss with respect to the pre-activation of layer $l$. These are the “output gradient” and “hidden gradient” that are commonly discussed. \n","\n","\n","### 3️. Output Layer Backpropagation\n","\n","For the output layer $L$, the error signal is computed as:\n","\n","$$\n","\\delta^{(L)} = \\frac{\\partial L}{\\partial A^{(L)}} \\;\\odot\\; f'^{(L)}(Z^{(L)})\n","$$\n","\n","Here:\n","\n","- $\\odot$ denotes element-wise multiplication\n","- $f'^{(L)}$ is the derivative of the activation function at the output layer\n","\n","This step is the base case for backpropagation. The gradient is born at the loss and scaled by the derivative of the output activation.\n","\n","\n","### 4️. Hidden Layer Backpropagation (Recursive Rule)\n","\n","For any hidden layer $l = L-1, L-2, ..., 1$, the error signal propagates backward recursively:\n","\n","$$\n","\\delta^{(l)} = (W^{(l+1)})^T \\delta^{(l+1)} \\;\\odot\\; f'^{(l)}(Z^{(l)})\n","$$\n","\n","Explanation:\n","\n","- $(W^{(l+1)})^T \\delta^{(l+1)}$ propagates the gradient from the next layer\n","- $f'^{(l)}(Z^{(l)})$ scales it according to the local slope of the activation function\n","- This captures the **core insight** of backpropagation: one gradient signal flows backward, repeatedly transformed by local derivatives\n","\n","These $\\delta^{(l)}$ values are the **checkpoints** that engineers inspect to understand gradient health, detect vanishing/exploding gradients, or debug networks.\n","\n","\n","### 5️. Compute Parameter Gradients\n","\n","Once $\\delta^{(l)}$ is known, computing the gradients for the parameters is straightforward.\n","\n","**Weights:**\n","\n","$$\n","\\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} \\left(A^{(l-1)}\\right)^T\n","$$\n","\n","Explanation:\n","\n","- Each weight receives a portion of the gradient proportional to its contribution to $Z^{(l)}$\n","- The input $A^{(l-1)}$ multiplies the error signal to determine its influence\n","\n","**Biases:**\n","\n","$$\n","\\frac{\\partial L}{\\partial b^{(l)}} = \\delta^{(l)}\n","$$\n","\n","- Each bias receives the same error signal as the pre-activation it feeds into\n","- This is because the derivative of $Z^{(l)}$ w.r.t. $b^{(l)}$ is 1\n","\n","\n","### 6️. Full Layer-Wise Backpropagation Algorithm\n","\n","```markdown\n","Forward pass:\n","for l = 1 to L:\n","    Z(l) = W(l) @ A(l-1) + b(l)\n","    A(l) = f(l)(Z(l))\n","\n","Backward pass:\n","δ(L) = dL/dA(L) ⊙ f'(L)(Z(L))\n","\n","for l = L-1 down to 1:\n","    δ(l) = W(l+1).T @ δ(l+1) ⊙ f'(l)(Z(l))\n","\n","Parameter gradients:\n","dW(l) = δ(l) @ A(l-1).T\n","db(l) = δ(l)\n"]},{"cell_type":"markdown","id":"d7f02e03","metadata":{"papermill":{"duration":0.004118,"end_time":"2025-12-18T11:01:52.195198","exception":false,"start_time":"2025-12-18T11:01:52.19108","status":"completed"},"tags":[]},"source":["# Key Takeaways from Day 11\n","\n","- Backpropagation = chain rule + matrix math\n","- Gradients flow backward layer by layer\n","- Each parameter update reduces loss\n","- NumPy implementation matches framework logic\n","- Understanding this removes “black box” fear\n","\n","---"]},{"cell_type":"markdown","id":"93b653e3","metadata":{"papermill":{"duration":0.004109,"end_time":"2025-12-18T11:01:52.203855","exception":false,"start_time":"2025-12-18T11:01:52.199746","status":"completed"},"tags":[]},"source":["<p style=\"text-align:center; font-size:18px;\">\n","© 2025 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":4.968898,"end_time":"2025-12-18T11:01:52.629292","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-18T11:01:47.660394","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}