{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-13-pytorch-introduction?scriptVersionId=287732914\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"d4dbc902","metadata":{"papermill":{"duration":0.005298,"end_time":"2025-12-22T04:28:50.073851","exception":false,"start_time":"2025-12-22T04:28:50.068553","status":"completed"},"tags":[]},"source":["# Day 13: PyTorch Introduction\n","\n","Welcome to day 13!\n","\n","Today you will learn:\n","- What PyTorch is and why it’s widely used\n","- Tensors: PyTorch’s core data structure\n","- Basic tensor operations\n","- GPU usage for acceleration\n","\n","---"]},{"cell_type":"markdown","id":"a8f08222","metadata":{"papermill":{"duration":0.004021,"end_time":"2025-12-22T04:28:50.082035","exception":false,"start_time":"2025-12-22T04:28:50.078014","status":"completed"},"tags":[]},"source":["#  What is PyTorch?\n","\n","PyTorch is an open-source deep learning framework developed by Facebook AI Research (FAIR).  \n","It is designed for building, training, and deploying neural networks efficiently, with support for both CPU and GPU computation.  \n","\n","Key components of PyTorch:\n","\n","- **Tensors**: multidimensional arrays, similar to NumPy arrays, but can run on GPUs.  \n","- **Autograd**: automatic differentiation engine that computes gradients for learning.  \n","- **nn module**: provides layers, loss functions, and utilities to define neural networks.  \n","- **optim module**: optimizers like SGD, Adam to update network parameters.  \n","- **Data utilities**: `Dataset` and `DataLoader` for batching, shuffling, and feeding data.\n","\n","## Why do we need PyTorch?\n","\n","Training deep learning models manually is extremely complex because:\n","\n","1. **Models have millions of parameters**: manually computing gradients is impractical.  \n","2. **Layers and operations are chained**: forward pass and backpropagation involve many computations.  \n","3. **GPU acceleration is necessary**: CPUs are too slow for large-scale training.  \n","\n","PyTorch solves these problems by:\n","\n","- Computing gradients automatically via autograd.  \n","- Allowing GPU-accelerated operations with minimal code changes.  \n","- Providing pre-built layers and optimizers to avoid writing everything from scratch.  \n","- Supporting flexible model design using dynamic computation graphs.\n","\n","## Why PyTorch over other frameworks?\n","\n","- **Dynamic computation graph**: Unlike TensorFlow 1.x, PyTorch builds the graph on-the-fly. This is more intuitive for debugging and experimentation.  \n","- **NumPy-like syntax**: If you know NumPy, PyTorch feels familiar, making it easy to transition to deep learning.  \n","- **Research + production**: Widely adopted by researchers for experiments and by engineers for production deployments.  \n","- **Community & ecosystem**: Strong support, libraries, tutorials, and pre-trained models available.\n","\n","## Summary\n","\n","- PyTorch is a tool to simplify deep learning.  \n","- It handles tensors, gradients, and optimization automatically.  \n","- Makes training, debugging, and experimenting with neural networks far easier.  \n","- Essential for building modern deep learning models quickly and safely.\n","\n","## Quick Mental Model\n","\n","Think of PyTorch as a smart math engine:\n","\n","1. You define your inputs, weights, and operations.  \n","2. You do a forward pass → PyTorch calculates outputs.  \n","3. You do a backward pass → PyTorch computes all gradients automatically.  \n","4. You update weights → model learns iteratively.  \n","\n","Without PyTorch, you’d be calculating derivatives manually, which is error-prone and slow."]},{"cell_type":"code","execution_count":1,"id":"0ef0648a","metadata":{"execution":{"iopub.execute_input":"2025-12-22T04:28:50.091235Z","iopub.status.busy":"2025-12-22T04:28:50.090948Z","iopub.status.idle":"2025-12-22T04:28:53.462427Z","shell.execute_reply":"2025-12-22T04:28:53.461837Z"},"papermill":{"duration":3.378144,"end_time":"2025-12-22T04:28:53.4641","exception":false,"start_time":"2025-12-22T04:28:50.085956","status":"completed"},"tags":[]},"outputs":[],"source":["# Install if not available (uncomment if needed)\n","# !pip install torch\n","\n","import torch\n"]},{"cell_type":"markdown","id":"b7712821","metadata":{"papermill":{"duration":0.004103,"end_time":"2025-12-22T04:28:53.472504","exception":false,"start_time":"2025-12-22T04:28:53.468401","status":"completed"},"tags":[]},"source":["# What is a Tensor?\n","\n","A tensor is a multidimensional array that generalizes scalars, vectors, and matrices.\n","\n","- **0D tensor** → scalar (single number)  \n","- **1D tensor** → vector (array of numbers)  \n","- **2D tensor** → matrix (table of numbers)  \n","- **3D tensor** → cube of numbers (e.g., image with height × width × channels)  \n","- **ND tensor** → higher-dimensional arrays\n","\n","In short:\n","> Tensor = multidimensional array that can be used in linear algebra and deep learning."]},{"cell_type":"markdown","id":"e2156190","metadata":{"papermill":{"duration":0.00397,"end_time":"2025-12-22T04:28:53.48042","exception":false,"start_time":"2025-12-22T04:28:53.47645","status":"completed"},"tags":[]},"source":["## Why do we need Tensors?\n","\n","Deep learning deals with inputs, weights, and outputs in multidimensional form:\n","\n","- Images → 3D tensor (Height × Width × Channels)  \n","- Text sequences → 2D tensor (Sequence Length × Embedding Size)  \n","- Batch of images → 4D tensor (Batch × Height × Width × Channels)  \n","\n","Tensors are the core data structure for all DL frameworks (PyTorch, TensorFlow, etc.)."]},{"cell_type":"markdown","id":"dda22eed","metadata":{"papermill":{"duration":0.003805,"end_time":"2025-12-22T04:28:53.488135","exception":false,"start_time":"2025-12-22T04:28:53.48433","status":"completed"},"tags":[]},"source":["## Tensor Order (Rank) and Indices\n","\n","The order of a tensor = number of indices required to identify a single element.\n","\n","| Object | Symbol | Index Notation | Order |\n","|------|------|---------------|------|\n","| Scalar | $a$ | none | 0 |\n","| Vector | $v_i$ | 1 index | 1 |\n","| Matrix | $M_{ij}$ | 2 indices | 2 |\n","| Tensor | $T_{ijk}$ | 3 indices | 3 |\n","| General | $T_{i_1 i_2 \\dots i_n}$ | $n$ indices | $n$ |\n","\n","Each index corresponds to one axis / dimension."]},{"cell_type":"markdown","id":"1263443b","metadata":{"papermill":{"duration":0.003798,"end_time":"2025-12-22T04:28:53.496113","exception":false,"start_time":"2025-12-22T04:28:53.492315","status":"completed"},"tags":[]},"source":["## Different Orders of Tensors\n","### 1. Scalars (0th-Order Tensor)\n","\n","A scalar is a single number:\n","$$\n","a \\in \\mathbb{R}\n","$$\n","\n","Examples:\n","- Loss value $L$\n","- Learning rate $\\eta$\n","- Bias offset\n","\n","No direction, no structure.\n","\n","### 2. Vectors (1st-Order Tensor)\n","\n","A vector is an ordered collection of scalars:\n","$$\n","\\mathbf{v} =\n","\\begin{bmatrix}\n","v_1 \\\\\n","v_2 \\\\\n","\\vdots \\\\\n","v_n\n","\\end{bmatrix}\n","\\quad\\text{or}\\quad v_i\n","$$\n","\n","- One index `i`\n","- Shape: $(n)$\n","\n","Meaning:\n","- Each component represents a feature\n","- Direction + magnitude\n","\n","Examples:\n","- Feature vector\n","- Bias vector\n","- Word embedding\n","\n","### 3. Matrices (2nd-Order Tensor)\n","\n","A matrix has two indices:\n","$$\n","M_{ij} =\n","\\begin{bmatrix}\n","m_{11} & m_{12} & \\dots \\\\\n","m_{21} & m_{22} & \\dots \\\\\n","\\vdots & \\vdots & \\ddots\n","\\end{bmatrix}\n","$$\n","\n","- `i` → row index\n","- `j` → column index\n","- Shape: $(m, n)$\n","\n","#### Matrix–vector multiplication\n","$$\n","y_i = \\sum_{j=1}^{n} M_{ij} x_j\n","$$\n","\n","This equation is the core computation of a neural layer.\n","\n","\n","#### Example\n","\n","$$\n","M =\n","\\begin{bmatrix}\n","1 & 0 & -2 \\\\\n","3 & 4 & 1\n","\\end{bmatrix}\n","$$\n","\n","$$\n","\\quad\n","x =\n","\\begin{bmatrix}\n","2 \\\\\n","-1 \\\\\n","3\n","\\end{bmatrix}\n","$$\n","\n","Compute each output:\n","\n","$$\n","\\begin{aligned}\n","y_1\n","&= M_{11}x_1 + M_{12}x_2 + M_{13}x_3 \\\\\n","&= (1)(2) + (0)(-1) + (-2)(3) \\\\\n","&= -4\n","\\end{aligned}\n","$$\n","\n","$$\n","\\begin{aligned}\n","y_2\n","&= M_{21}x_1 + M_{22}x_2 + M_{23}x_3 \\\\\n","&= (3)(2) + (4)(-1) + (1)(3) \\\\\n","&= 5\n","\\end{aligned}\n","$$\n","\n","Final result:\n","\n","$$\n","y =\n","\\begin{bmatrix}\n","-4 \\\\\n","5\n","\\end{bmatrix}\n","$$\n","\n","\n","### 4. 3D Tensor (3rd-Order Tensor )\n","\n","A 3rd-order tensor is a tensor with three indices:\n","\n","$$\n","T_{ijk}\n","$$\n","\n","Each index answers one question:\n","- $i$ → row (height)\n","- $j$ → column (width)\n","- $k$ → slice / channel / feature map\n","\n","So $T_{ijk}$ means:\n","> the value at **row $i$**, **column $j$**, in the **$k$-th matrix**.\n","\n","**Shape Meaning**\n","\n","If a tensor has shape:\n","\n","$$\n","H \\times W \\times C\n","$$\n","\n","then:\n","- $H$ = number of rows in each matrix\n","- $W$ = number of columns in each matrix\n","- $C$ = number of matrices (channels)\n","\n","> A 3rd-order tensor is a stack of $C$ matrices, each of size $H \\times W$.\n","\n","#### Example: Tensor of Shape $2 \\times 2 \\times 3$\n","\n","This tensor contains **3 matrices**, each of size $2 \\times 2$.\n","\n","Channel $k = 1$\n","$$\n","T_{::1} =\n","\\begin{bmatrix}\n","1 & 2 \\\\\n","3 & 4\n","\\end{bmatrix}\n","$$\n","\n","Channel $k = 2$\n","$$\n","T_{::2} =\n","\\begin{bmatrix}\n","5 & 6 \\\\\n","7 & 8\n","\\end{bmatrix}\n","$$\n","\n","Channel $k = 3$\n","$$\n","T_{::3} =\n","\\begin{bmatrix}\n","9 & 10 \\\\\n","11 & 12\n","\\end{bmatrix}\n","$$\n","\n","Together, these three matrices form the tensor $T$ with shape $2 \\times 2 \\times 3$.\n","\n","\n","**Single Element**\n","\n","$$\n","T_{2,1,3} = 11\n","$$\n","\n","Read as:\n","- row $i = 2$\n","- column $j = 1$\n","- channel $k = 3$\n","\n","Value at that position is **11**.\n","\n","#### Slicing a 3rd-Order Tensor\n","\n","Slicing is selecting a subset of elements along one or more axes of a tensor, which may reduce its dimensionality depending on how many indices are fixed.\n","\n","| Term      | Dimensional Meaning               | Physical Analogy                        |\n","|-----------|---------------------------------|----------------------------------------|\n","| Frontal   | Fixes the depth index (`k`)       | One slice of bread from a loaf         |\n","| Horizontal| Fixes the row index (`i`)         | A layer cut parallel to the floor      |\n","| Lateral   | Fixes the column index (`j`)      | A vertical cut from the side           |\n","\n","\n","#### Fixing the third index ($k$) - The frontal slice\n","\n","The k-th frontal slice of T\n","\n","$$\n","T_{::k}\n","$$\n","\n","This gives a 2D matrix (one slice of the tensor).\n","\n","Example:\n","$$\n","T_{::2} =\n","\\begin{bmatrix}\n","5 & 6 \\\\\n","7 & 8\n","\\end{bmatrix}\n","$$\n","\n","\n","#### Fixing the first index ($i$) - The horizontal slice\n","\n","The i-th horizontal slice of T\n","\n","$$\n","T_{i::}\n","$$\n","\n","This gives a row across all channels.\n","\n","Example:\n","$$\n","T_{1::} =\n","\\begin{bmatrix}\n","1 & 2 & 5 & 6 & 9 & 10\n","\\end{bmatrix}\n","$$\n","\n","(conceptually: row 1 from each matrix)\n","\n","#### Fixing the second index ($j$) - The lateral slice\n","\n","The j-th lateral slice of T\n","\n","$$\n","T_{:j:}\n","$$\n","\n","This gives a column across all channels.\n","\n","Example: \n","\n","$$\n","T_{:2:} =\n","\\begin{bmatrix}\n","2 & 6 & 10 \\\\\n","4 & 8 & 12\n","\\end{bmatrix}\n","$$\n","(conceptually: column  2 from each matrix)\n","\n","#### Fixing Two Indices → Fiber\n","\n","When two indices are fixed, the resulting 1D vector is called a **fiber**.  \n","Fibers are the lines running through the tensor along the axis that wasn’t fixed.\n","\n","\n","- **Mode-1 fiber (fix j and k, vary i):**  \n","$$\n","T_{:,j,k}  \n","$$\n","Column along the first dimension  \n","\n","- **Mode-2 fiber (fix i and k, vary j):**  \n","$$\n","T_{i,:,k}  \n","$$\n","Row along the second dimension  \n","\n","\n","- **Mode-3 fiber (fix i and j, vary k):**  \n","$$\n","T_{i,j,:}  \n","$$\n","Vector along the third dimension (channels)  \n","\n","\n","#### Examples of Two Indices Fixed (Fibers)\n","\n","Tensor of Shape $2 \\times 2 \\times 3$\n","\n","This tensor contains **3 matrices**, each of size $2 \\times 2$.\n","\n","Channel $k = 1$:\n","$$\n","T_{::1} =\n","\\begin{bmatrix}\n","1 & 2 \\\\\n","3 & 4\n","\\end{bmatrix}\n","$$\n","\n","Channel $k = 2$:\n","$$\n","T_{::2} =\n","\\begin{bmatrix}\n","5 & 6 \\\\\n","7 & 8\n","\\end{bmatrix}\n","$$\n","\n","Channel $k = 3$:\n","$$\n","T_{::3} =\n","\\begin{bmatrix}\n","9 & 10 \\\\\n","11 & 12\n","\\end{bmatrix}\n","$$\n","\n","1. **Mode-3 fiber (fix i=1, j=2):**  \n","$$\n","T_{1,2,:} = [2, 6, 10]\n","$$  \n","Vector of values across channels at **row 1, column 2**.\n","\n","2. **Mode-2 fiber (fix i=2, k=3):**  \n","$$\n","T_{2,:,3} = [11, 12]\n","$$  \n","Row vector of **row 2 in channel 3**.\n","\n","3. **Mode-1 fiber (fix j=1, k=1):**  \n","$$\n","T_{:,1,1} = [1, 3]\n","$$  \n","Column vector of **column 1 in channel 1**.\n","\n","\n","#### Single Element (all three indices fixed)\n","\n","$$\n","T_{2,1,3} = 11\n","$$\n","\n","- row $i=2$  \n","- column $j=1$  \n","- channel $k=3$  \n","\n","#### Key Intuition\n","\n","- Fix **one index** → **matrix slice**  \n","- Fix **two indices** → **fiber (vector)**  \n","- Fix **all three indices** → **scalar (single number)**  \n","\n","### 5. Higher-Order Tensors (ND)\n","\n","General tensor:\n","$$\n","T_{i_1 i_2 i_3 \\dots i_n}\n","$$\n","\n","Each index corresponds to:\n","- A dimension\n","- A semantic axis\n","\n","Examples in DL:\n","- Batch dimension\n","- Time steps\n","- Feature channels"]},{"cell_type":"markdown","id":"9878ed5b","metadata":{"papermill":{"duration":0.003838,"end_time":"2025-12-22T04:28:53.504532","exception":false,"start_time":"2025-12-22T04:28:53.500694","status":"completed"},"tags":[]},"source":["## Tensor Shape and Dimension Meaning\n","\n","### 1. Tensor Shape\n","\n","- The shape of a tensor describes how many elements it has along each axis.  \n","- Written as a tuple:  \n","  $$\n","  (d_1, d_2, d_3, \\dots, d_n)\n","  $$  \n","  where $d_k$ is the size of the $k$-th axis.\n","\n","- Examples:\n","  - Scalar → shape `()` → 0D  \n","  - Vector of length 5 → shape `(5,)` → 1D  \n","  - Matrix 3×4 → shape `(3, 4)` → 2D  \n","  - 3rd-order tensor 2×2×3 → shape `(2, 2, 3)` → 3D  \n","  - Batch of 10 images (RGB 32×32) → shape `(10, 32, 32, 3)` → 4D\n","\n","### 2. Tensor Dimension\n","\n","- Dimension (or rank / order) = number of axes in the tensor.  \n","- Number of axes = length of the shape tuple.\n","- Examples:\n","  - Scalar → 0D  \n","  - Vector → 1D  \n","  - Matrix → 2D  \n","  - 3rd-order tensor → 3D  \n","  - ND tensor → N dimensions\n","\n","### 3. Semantic Meaning of Axes\n","\n","- Each axis often has a conceptual meaning:\n","  - **Batch axis** → number of samples  \n","  - **Height / width axis** → spatial dimensions of an image  \n","  - **Channel / feature axis** → color channels, feature maps, embedding dimensions  \n","  - **Time axis** → positions in a sequence\n","\n","- Example: Batch of 10 RGB images of size 32×32:\n","  $$\n","  \\text{Shape: } (10, 32, 32, 3)\n","  $$\n","  - Axis 0 → batch size  \n","  - Axis 1 → height  \n","  - Axis 2 → width  \n","  - Axis 3 → channels\n","\n","### 4. Quick Reference Table\n","\n","| Tensor | Shape Example | Dimensions | Semantic Axes |\n","|--------|---------------|------------|---------------|\n","| Scalar | ()            | 0D         | N/A           |\n","| Vector | (5,)          | 1D         | features      |\n","| Matrix | (3,4)         | 2D         | rows × columns |\n","| 3D Tensor | (2,2,3)    | 3D         | height × width × channels |\n","| Batch of images | (10,32,32,3) | 4D | batch × height × width × channels |\n","\n","\n","### 5. Why Shape & Dimension Matter\n","\n","- Operations like matrix multiplication, convolution, and broadcasting rely on knowing shape and axes meaning.  \n","- Misunderstanding axes often causes runtime errors or wrong results in deep learning.\n"]},{"cell_type":"markdown","id":"9850d921","metadata":{"papermill":{"duration":0.003893,"end_time":"2025-12-22T04:28:53.512293","exception":false,"start_time":"2025-12-22T04:28:53.5084","status":"completed"},"tags":[]},"source":["## Tensor Operations: Index Notation\n","\n","Tensor operations can be neatly described using index notation, which helps understand how elements interact along axes.\n","\n","### (a) Element-wise Operations\n","\n","- These operate on tensors of the same shape.  \n","- No index is removed. The result has the same shape as the inputs.\n","\n","Example: Addition of two matrices\n","\n","$$\n","A =\n","\\begin{bmatrix}\n","1 & 2 \\\\\n","3 & 4\n","\\end{bmatrix}, \\quad\n","B =\n","\\begin{bmatrix}\n","5 & 6 \\\\\n","7 & 8\n","\\end{bmatrix}\n","$$\n","\n","Element-wise addition:\n","\n","$$\n","C_{ij} = A_{ij} + B_{ij}\n","$$\n","\n","Compute each element:\n","\n","$$\n","C =\n","\\begin{bmatrix}\n","1+5 & 2+6 \\\\\n","3+7 & 4+8\n","\\end{bmatrix} =\n","\\begin{bmatrix}\n","6 & 8 \\\\\n","10 & 12\n","\\end{bmatrix}\n","$$\n","\n","- Shape of $C$ = shape of $A$ = shape of $B$  \n","- Indices $i,j$ are retained.\n","\n","### (b) Tensor Contraction (Generalized Dot Product)\n","\n","- Some indices are summed over → they disappear in the output.  \n","- Remaining indices define the shape of the resulting tensor.  \n","- Matrix multiplication is a special case.\n","\n","Matrix multiplication example\n","\n","$$\n","C_{ik} = \\sum_{j} A_{ij} B_{jk}\n","$$\n","\n","- $A$ shape = (2×3)  \n","- $B$ shape = (3×2)  \n","- Sum over index $j=1..3$  \n","- Result $C$ shape = (2×2)  \n","\n","Compute Example:\n","\n","$$\n","A =\n","\\begin{bmatrix}\n","1 & 2 & 3 \\\\\n","4 & 5 & 6\n","\\end{bmatrix}, \\quad\n","B =\n","\\begin{bmatrix}\n","7 & 8 \\\\\n","9 & 10 \\\\\n","11 & 12\n","\\end{bmatrix}\n","$$\n","\n","Compute $C_{ik}$:\n","\n","- $C_{11} = 1*7 + 2*9 + 3*11 = 58$  \n","- $C_{12} = 1*8 + 2*10 + 3*12 = 64$  \n","- $C_{21} = 4*7 + 5*9 + 6*11 = 139$  \n","- $C_{22} = 4*8 + 5*10 + 6*12 = 154$\n","\n","$$\n","C =\n","\\begin{bmatrix}\n","58 & 64 \\\\\n","139 & 154\n","\\end{bmatrix}\n","$$\n","\n","**Key Points:**\n","\n","- Index $j$ is contracted (summed over) → disappears in $C$  \n","- Remaining indices $i,k$ define the output shape \n","\n","\n","### (c) Generalization to Higher-Order Tensors\n","\n","- Tensor contraction generalizes dot products to any order:  \n","  $$\n","  C_{ab\\ldots} = \\sum_{i} A_{ai\\ldots} B_{i b\\ldots}\n","  $$\n","\n","- This operation underlies all neural network layers:\n","  - Fully connected layers → matrix multiplication  \n","  - Convolution layers → sum over input channels  \n","  - Attention → sum over sequence positions  "]},{"cell_type":"markdown","id":"4b76ca62","metadata":{"papermill":{"duration":0.00395,"end_time":"2025-12-22T04:28:53.520022","exception":false,"start_time":"2025-12-22T04:28:53.516072","status":"completed"},"tags":[]},"source":["## Einstein Summation Convention\n","\n","The Einstein summation convention (or einsum) is a concise notation for expressing tensor operations without explicitly writing summation symbols.\n","\n","### Basic Rule\n","\n","- Repeated index in a product implies summation over that index\n","- Example (matrix multiplication):\n","\n","$$\n","C_{ik} = \\sum_j A_{ij} B_{jk} \\quad \\longrightarrow \\quad C_{ik} = A_{ij} B_{jk}\n","$$\n","\n","- Here, index `j` appears twice → automatically summed over  \n","- Indices `i` and `k` appear once → remain in the output\n","\n","### Benefits\n","\n","- Removes clutter of explicit summation symbols  \n","- Makes operations more readable** for higher-order tensors  \n","- Easily generalizes to ND tensor contractions\n","\n","Examples\n","\n","**(a) Matrix Multiplication**\n","\n","$$\n","C_{ik} = A_{ij} B_{jk}\n","$$\n","\n","- Shapes: \\(A\\) = (2×3), \\(B\\) = (3×2)  \n","- Index `j` summed → output shape = (2×2)\n","\n","**(b) Vector Dot Product**\n","\n","$$\n","s = x_i y_i\n","$$\n","\n","- Repeated index `i` → sum over all elements  \n","- Equivalent to $\\sum_i x_i y_i$\n","\n","**(c) 3rd-Order Tensor Contraction**\n","\n","$$\n","C_{il} = A_{ijk} B_{jkl}\n","$$`\n","\n","- Repeated indices `j` and `k` → summed  \n","- Remaining indices `i` and `l` → output shape\n"]},{"cell_type":"markdown","id":"c58e5270","metadata":{"papermill":{"duration":0.003829,"end_time":"2025-12-22T04:28:53.527802","exception":false,"start_time":"2025-12-22T04:28:53.523973","status":"completed"},"tags":[]},"source":["## Tensor Products (Outer Products)\n","\n","The tensor product, often called the outer product, is an operation that combines lower-order tensors to produce a higher-order tensor.\n","\n","For two vectors \\(a\\) and \\(b\\):\n","\n","$$\n","T_{ij} = a_i b_j\n","$$\n","\n","- Input: two vectors $a \\in \\mathbb{R}^{m}$, $b \\in \\mathbb{R}^{n}$\n","- Output: a matrix $T \\in \\mathbb{R}^{m \\times n}$  \n","- No summation occurs; each element of $T$ is simply the product of elements from a and b.\n","\n","> Tensor product generalizes to any order of tensors, building higher-order tensors from lower-order ones.\n","\n","Example: Vector Outer Product\n","\n","Let:\n","\n","$$\n","a = \n","\\begin{bmatrix}\n","1 \\\\ 2 \\\\ 3\n","\\end{bmatrix}, \\quad\n","b =\n","\\begin{bmatrix}\n","4 \\\\ 5\n","\\end{bmatrix}\n","$$\n","\n","Compute outer product $T = a \\otimes b$:\n","\n","$$\n","T_{ij} = a_i b_j\n","$$\n","\n","$$\n","T =\n","\\begin{bmatrix}\n","1*4 & 1*5 \\\\\n","2*4 & 2*5 \\\\\n","3*4 & 3*5\n","\\end{bmatrix} =\n","\\begin{bmatrix}\n","4 & 5 \\\\\n","8 & 10 \\\\\n","12 & 15\n","\\end{bmatrix}\n","$$\n","\n","- Shape of \\(T\\) = (3 × 2)  \n","- Each row corresponds to one element of \\(a\\) multiplied by all elements of \\(b\\)  \n","\n","\n","### Generalization\n","\n","- Outer product can combine **higher-order tensors**:  \n","  $$\n","  T_{ijkl} = A_{ij} B_{kl}\n","  $$\n","- Input: 2×2 tensor \\(A\\), 2×3 tensor \\(B\\)  \n","- Output: 4D tensor \\(T\\) with shape 2×2×2×3  \n","\n","> Outer products are used in deep learning for constructing bilinear layers, attention scores, and higher-order feature interactions.\n"]},{"cell_type":"markdown","id":"4150c31c","metadata":{"papermill":{"duration":0.003815,"end_time":"2025-12-22T04:28:53.535653","exception":false,"start_time":"2025-12-22T04:28:53.531838","status":"completed"},"tags":[]},"source":["## Why Gradients Are Tensors\n","\n","In deep learning, gradients of the loss with respect to parameters are themselves tensors. This comes from the fact that each parameter can have multiple components, and the gradient must match that shape.\n","\n","### Scalar Loss\n","\n","Let the loss be a scalar:\n","\n","$$\n","L \\in \\mathbb{R}\n","$$\n","\n","- This is just a single number representing error.\n","\n","### Gradient w.r.t a Vector\n","\n","Suppose we have a vector parameter $v \\in \\mathbb{R}^n$.  \n","The gradient of the loss w.r.t $v$ is:\n","\n","$$\n","\\frac{\\partial L}{\\partial v_i}, \\quad i = 1, \\dots, n\n","$$\n","\n","- This is a vector of the same size as $v$ \n","- Each element tells us how changing that component of \\(v\\) affects the loss.\n","\n","**Example:**\n","\n","$$\n","v = \n","\\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix}, \\quad\n","\\frac{\\partial L}{\\partial v} =\n","\\begin{bmatrix} \\frac{\\partial L}{\\partial v_1} \\\\ \\frac{\\partial L}{\\partial v_2} \\\\ \\frac{\\partial L}{\\partial v_3} \\end{bmatrix}\n","$$\n","\n","### Gradient w.r.t a Matrix\n","\n","Suppose we have a weight matrix $W \\in \\mathbb{R}^{m \\times n}$.  \n","The gradient is:\n","\n","$$\n","\\frac{\\partial L}{\\partial W_{ij}}, \\quad i = 1,\\dots,m, \\; j = 1,\\dots,n\n","$$\n","\n","- This is a matrix of the same shape as $W $\n","- Each element tells how changing that weight affects the loss.\n","\n","**Example:**\n","\n","$$\n","W =\n","\\begin{bmatrix}\n","w_{11} & w_{12} \\\\\n","w_{21} & w_{22}\n","\\end{bmatrix}, \\quad\n","\\frac{\\partial L}{\\partial W} =\n","\\begin{bmatrix}\n","\\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial w_{12}} \\\\\n","\\frac{\\partial L}{\\partial w_{21}} & \\frac{\\partial L}{\\partial w_{22}}\n","\\end{bmatrix}\n","$$\n","\n","### Rule of Thumb\n","\n","> **Gradient has the same shape as the parameter it differentiates**\n","\n","- Scalar → gradient is scalar  \n","- Vector → gradient is vector  \n","- Matrix → gradient is matrix  \n","- Higher-order tensor → gradient is higher-order tensor  \n","\n","### Implications in Deep Learning\n","\n","- **Weights are tensors** → gradients are tensors  \n","- **Backpropagation** → sequence of tensor operations (matrix multiplication, contraction, addition)  \n","- This is why automatic differentiation frameworks treat gradients as tensors, matching the shapes of parameters.\n"]},{"cell_type":"markdown","id":"750afa42","metadata":{"papermill":{"duration":0.003894,"end_time":"2025-12-22T04:28:53.543403","exception":false,"start_time":"2025-12-22T04:28:53.539509","status":"completed"},"tags":[]},"source":["## Neural Networks = Tensor Transformations\n","\n","A neural network layer:\n","$$\n","Z = XW + b\n","$$\n","\n","In index form:\n","$$\n","Z_{ik} = \\sum_j X_{ij} W_{jk} + b_k\n","$$\n","\n","Followed by:\n","$$\n","A_{ik} = f(Z_{ik})\n","$$\n","\n","Training adjusts tensor values to minimize:\n","$$\n","L(y, f_\\theta(X))\n","$$"]},{"cell_type":"markdown","id":"b6b095a4","metadata":{"execution":{"iopub.execute_input":"2025-12-20T18:15:19.71441Z","iopub.status.busy":"2025-12-20T18:15:19.713787Z","iopub.status.idle":"2025-12-20T18:15:19.745248Z","shell.execute_reply":"2025-12-20T18:15:19.744092Z","shell.execute_reply.started":"2025-12-20T18:15:19.714375Z"},"papermill":{"duration":0.003897,"end_time":"2025-12-22T04:28:53.551161","exception":false,"start_time":"2025-12-22T04:28:53.547264","status":"completed"},"tags":[]},"source":["## Quick Mental Model \n","\n","- **Scalars** → measure error\n","- **Vectors** → represent features\n","- **Matrices** → mix features\n","- **Tensors** → organize learning at scale\n","\n","Deep learning is:\n","> **Tensor algebra + nonlinear functions + optimization**\n","\n","\n","## Final Insight\n","\n","> A neural network is a sequence of tensor contractions followed by nonlinear mappings, optimized via gradient descent.\n","\n","If this sentence eventually feels *obvious*, you truly understand deep learning.\n"]},{"cell_type":"markdown","id":"451b70d8","metadata":{"papermill":{"duration":0.004485,"end_time":"2025-12-22T04:28:53.559566","exception":false,"start_time":"2025-12-22T04:28:53.555081","status":"completed"},"tags":[]},"source":["# Implementation"]},{"cell_type":"code","execution_count":2,"id":"74c25ece","metadata":{"execution":{"iopub.execute_input":"2025-12-22T04:28:53.568967Z","iopub.status.busy":"2025-12-22T04:28:53.568339Z","iopub.status.idle":"2025-12-22T04:28:53.651729Z","shell.execute_reply":"2025-12-22T04:28:53.650946Z"},"papermill":{"duration":0.08959,"end_time":"2025-12-22T04:28:53.653057","exception":false,"start_time":"2025-12-22T04:28:53.563467","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Scalar: tensor(5)\n","Vector: tensor([1., 2., 3.])\n","Matrix:\n"," tensor([[1., 2.],\n","        [3., 4.]])\n","3D Tensor:\n"," tensor([[[1., 1.],\n","         [1., 1.]],\n","\n","        [[1., 1.],\n","         [1., 1.]]])\n"]}],"source":["# -------------------------\n","# Scalar Tensor\n","# -------------------------\n","\n","# Create a 0-dimensional tensor (a scalar)\n","# torch.tensor(5) wraps the Python number 5 into a PyTorch tensor object\n","a = torch.tensor(5)\n","\n","# Print the tensor\n","# Even though it looks like a number, it is a tensor with shape ()\n","print(\"Scalar:\", a)\n","\n","\n","# -------------------------\n","# Vector Tensor\n","# -------------------------\n","\n","# Create a 1-dimensional tensor (vector) with 3 elements\n","# The list defines values along a single axis\n","v = torch.tensor([1.0, 2.0, 3.0])\n","\n","# Print the vector tensor\n","# Shape is (3,)\n","print(\"Vector:\", v)\n","\n","\n","# -------------------------\n","# Matrix Tensor\n","# -------------------------\n","\n","# Create a 2-dimensional tensor (matrix)\n","# Outer list → rows\n","# Inner lists → columns\n","m = torch.tensor([\n","    [1.0, 2.0],\n","    [3.0, 4.0]\n","])\n","\n","# Print the matrix tensor\n","# Shape is (2, 2)\n","print(\"Matrix:\\n\", m)\n","\n","\n","# -------------------------\n","# 3D Tensor\n","# -------------------------\n","\n","# Create a 3-dimensional tensor filled with ones\n","# Shape: (2, 2, 2)\n","# This means:\n","# - 2 blocks\n","# - each block has 2 rows\n","# - each row has 2 columns\n","t3 = torch.ones((2, 2, 2))\n","\n","# Print the 3D tensor\n","print(\"3D Tensor:\\n\", t3)\n","\n"]},{"cell_type":"markdown","id":"f41d0809","metadata":{"papermill":{"duration":0.004041,"end_time":"2025-12-22T04:28:53.6613","exception":false,"start_time":"2025-12-22T04:28:53.657259","status":"completed"},"tags":[]},"source":["`torch` is doing three critical things:\n","\n","- Allocates memory for a tensor\n","- Assigns shape and datatype\n","- Registers the tensor in PyTorch’s computation system"]},{"cell_type":"code","execution_count":3,"id":"3e052548","metadata":{"execution":{"iopub.execute_input":"2025-12-22T04:28:53.670544Z","iopub.status.busy":"2025-12-22T04:28:53.670318Z","iopub.status.idle":"2025-12-22T04:28:53.674169Z","shell.execute_reply":"2025-12-22T04:28:53.673551Z"},"papermill":{"duration":0.009975,"end_time":"2025-12-22T04:28:53.67553","exception":false,"start_time":"2025-12-22T04:28:53.665555","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of scalar: torch.Size([])\n","Datatype of scalar: torch.int64\n","Device of scalar: cpu\n"]}],"source":["# -------------------------\n","# Tensor attributes\n","# -------------------------\n","\n","# -------------------------\n","# Scalar attributes\n","# -------------------------\n","\n","# Shape: number of elements along each axis\n","# For a scalar, shape is empty → ()\n","print(\"Shape of scalar:\", a.shape)\n","\n","# Data type of elements stored in the tensor\n","# Important for precision, memory, and performance\n","print(\"Datatype of scalar:\", a.dtype)\n","\n","# Device where the tensor lives (CPU or GPU)\n","print(\"Device of scalar:\", a.device)"]},{"cell_type":"code","execution_count":4,"id":"6ba13a7b","metadata":{"execution":{"iopub.execute_input":"2025-12-22T04:28:53.684842Z","iopub.status.busy":"2025-12-22T04:28:53.684609Z","iopub.status.idle":"2025-12-22T04:28:53.688389Z","shell.execute_reply":"2025-12-22T04:28:53.6878Z"},"papermill":{"duration":0.010177,"end_time":"2025-12-22T04:28:53.689765","exception":false,"start_time":"2025-12-22T04:28:53.679588","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of vector: torch.Size([3])\n","Datatype of vector: torch.float32\n","Device of vector: cpu\n"]}],"source":["# -------------------------\n","# Vector attributes\n","# -------------------------\n","\n","# Shape of vector → (3,)\n","# One axis with 3 elements\n","print(\"Shape of vector:\", v.shape)\n","\n","# Floating point type (default: float32)\n","print(\"Datatype of vector:\", v.dtype)\n","\n","# Device of vector\n","print(\"Device of vector:\", v.device)"]},{"cell_type":"code","execution_count":5,"id":"fce27cd7","metadata":{"execution":{"iopub.execute_input":"2025-12-22T04:28:53.699456Z","iopub.status.busy":"2025-12-22T04:28:53.699235Z","iopub.status.idle":"2025-12-22T04:28:53.702935Z","shell.execute_reply":"2025-12-22T04:28:53.70236Z"},"papermill":{"duration":0.009871,"end_time":"2025-12-22T04:28:53.704329","exception":false,"start_time":"2025-12-22T04:28:53.694458","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of matrix: torch.Size([2, 2])\n","Datatype of matrix: torch.float32\n","Device of matrix: cpu\n"]}],"source":["# -------------------------\n","# Matrix attributes\n","# -------------------------\n","\n","# Shape of matrix → (2, 2)\n","# Two axes: rows × columns\n","print(\"Shape of matrix:\", m.shape)\n","\n","# Datatype of matrix\n","print(\"Datatype of matrix:\", m.dtype)\n","\n","# Device of matrix\n","print(\"Device of matrix:\", m.device)"]},{"cell_type":"code","execution_count":6,"id":"f4069bbe","metadata":{"execution":{"iopub.execute_input":"2025-12-22T04:28:53.713784Z","iopub.status.busy":"2025-12-22T04:28:53.713563Z","iopub.status.idle":"2025-12-22T04:28:53.717546Z","shell.execute_reply":"2025-12-22T04:28:53.716805Z"},"papermill":{"duration":0.010212,"end_time":"2025-12-22T04:28:53.718874","exception":false,"start_time":"2025-12-22T04:28:53.708662","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of 3D tensor: torch.Size([2, 2, 2])\n","Datatype of 3D tensor: torch.float32\n","Device of 3D tensor: cpu\n"]}],"source":["# -------------------------\n","# 3D Tensor attributes\n","# -------------------------\n","\n","# Shape of 3D tensor → (2, 2, 2)\n","# Three axes: blocks × rows × columns\n","print(\"Shape of 3D tensor:\", t3.shape)\n","\n","# Datatype of 3D tensor\n","print(\"Datatype of 3D tensor:\", t3.dtype)\n","\n","# Device of 3D tensor\n","print(\"Device of 3D tensor:\", t3.device)"]},{"cell_type":"code","execution_count":7,"id":"c4302f11","metadata":{"execution":{"iopub.execute_input":"2025-12-22T04:28:53.728468Z","iopub.status.busy":"2025-12-22T04:28:53.727953Z","iopub.status.idle":"2025-12-22T04:28:53.733576Z","shell.execute_reply":"2025-12-22T04:28:53.733031Z"},"papermill":{"duration":0.011843,"end_time":"2025-12-22T04:28:53.734868","exception":false,"start_time":"2025-12-22T04:28:53.723025","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Add: tensor([4., 6.])\n"]}],"source":["# -------------------------\n","# Basic operations\n","# -------------------------\n","\n","# -------------------------\n","# Element-wise addition\n","# -------------------------\n","\n","# Create two 1D tensors (vectors) of the same shape (2,)\n","x = torch.tensor([1.0, 2.0])\n","y = torch.tensor([3.0, 4.0])\n","\n","# Element-wise addition\n","# Each element is added independently:\n","# [1+3, 2+4]\n","print(\"Add:\", x + y)"]},{"cell_type":"code","execution_count":8,"id":"a82de50e","metadata":{"execution":{"iopub.execute_input":"2025-12-22T04:28:53.744555Z","iopub.status.busy":"2025-12-22T04:28:53.744Z","iopub.status.idle":"2025-12-22T04:28:53.748393Z","shell.execute_reply":"2025-12-22T04:28:53.747769Z"},"papermill":{"duration":0.010579,"end_time":"2025-12-22T04:28:53.749689","exception":false,"start_time":"2025-12-22T04:28:53.73911","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Multiply: tensor([3., 8.])\n"]}],"source":["# -------------------------\n","# Element-wise multiplication\n","# -------------------------\n","\n","# Element-wise multiplication (Hadamard product)\n","# Each element is multiplied independently:\n","# [1*3, 2*4]\n","print(\"Multiply:\", x * y)\n"]},{"cell_type":"code","execution_count":9,"id":"dbf28345","metadata":{"execution":{"iopub.execute_input":"2025-12-22T04:28:53.759232Z","iopub.status.busy":"2025-12-22T04:28:53.758941Z","iopub.status.idle":"2025-12-22T04:28:53.773742Z","shell.execute_reply":"2025-12-22T04:28:53.773151Z"},"papermill":{"duration":0.020966,"end_time":"2025-12-22T04:28:53.774962","exception":false,"start_time":"2025-12-22T04:28:53.753996","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["MatMul:\n"," tensor([[2., 4.],\n","        [6., 8.]])\n"]}],"source":["# -------------------------\n","# Matrix multiplication\n","# -------------------------\n","\n","# Create two 2×2 matrices\n","# Explicitly set dtype to float32 (required for most DL ops)\n","a = torch.tensor([[1, 2],\n","                  [3, 4]], dtype=torch.float32)\n","\n","b = torch.tensor([[2, 0],\n","                  [0, 2]], dtype=torch.float32)\n","\n","# Matrix multiplication\n","print(\"MatMul:\\n\", torch.matmul(a, b))"]},{"cell_type":"code","execution_count":10,"id":"2203b287","metadata":{"execution":{"iopub.execute_input":"2025-12-22T04:28:53.784453Z","iopub.status.busy":"2025-12-22T04:28:53.784259Z","iopub.status.idle":"2025-12-22T04:28:53.790079Z","shell.execute_reply":"2025-12-22T04:28:53.789471Z"},"papermill":{"duration":0.012149,"end_time":"2025-12-22T04:28:53.791405","exception":false,"start_time":"2025-12-22T04:28:53.779256","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Original: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n","Reshaped (3x4):\n"," tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11]])\n"]}],"source":["# -------------------------\n","# Reshaping tensors\n","# -------------------------\n","\n","# Create a 1D tensor with values from 0 to 11\n","# torch.arange(12) → [0, 1, 2, ..., 11]\n","t = torch.arange(12)\n","\n","# Print the original tensor\n","# Shape: (12,)\n","print(\"Original:\", t)\n","\n","\n","# Reshape the tensor into a 3×4 matrix\n","t_reshaped = t.view(3,4)\n","\n","# Print reshaped tensor\n","print(\"Reshaped (3x4):\\n\", t_reshaped)"]},{"cell_type":"code","execution_count":11,"id":"e4d1c62f","metadata":{"execution":{"iopub.execute_input":"2025-12-22T04:28:53.801147Z","iopub.status.busy":"2025-12-22T04:28:53.80072Z","iopub.status.idle":"2025-12-22T04:28:53.889962Z","shell.execute_reply":"2025-12-22T04:28:53.889167Z"},"papermill":{"duration":0.095689,"end_time":"2025-12-22T04:28:53.891379","exception":false,"start_time":"2025-12-22T04:28:53.79569","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Gradient dy/dx: tensor([7.])\n"]}],"source":["# -------------------------\n","# Gradients & Autograd\n","# -------------------------\n","\n","# Create a tensor with value 2.0\n","# requires_grad=True tells PyTorch to track operations on this tensor\n","# so that derivatives (gradients) can be computed later\n","x = torch.tensor([2.0], requires_grad=True)\n","\n","# Define a mathematical function using x\n","# This is the forward pass\n","# y = x^2 + 3x + 1\n","y = x**2 + 3*x + 1\n","\n","# Trigger backpropagation\n","# PyTorch computes dy/dx using the stored computation graph\n","# This works because y is a scalar\n","y.backward()\n","\n","# Access the gradient of y with respect to x\n","# The result is stored in x.grad\n","print(\"Gradient dy/dx:\", x.grad)"]},{"cell_type":"code","execution_count":12,"id":"bbcbe3a3","metadata":{"execution":{"iopub.execute_input":"2025-12-22T04:28:53.901714Z","iopub.status.busy":"2025-12-22T04:28:53.90118Z","iopub.status.idle":"2025-12-22T04:28:53.90472Z","shell.execute_reply":"2025-12-22T04:28:53.904153Z"},"papermill":{"duration":0.010067,"end_time":"2025-12-22T04:28:53.906063","exception":false,"start_time":"2025-12-22T04:28:53.895996","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cuda\n"]}],"source":["# Check GPU availability\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", device)"]},{"cell_type":"markdown","id":"eb6ca78e","metadata":{"papermill":{"duration":0.004413,"end_time":"2025-12-22T04:28:53.915266","exception":false,"start_time":"2025-12-22T04:28:53.910853","status":"completed"},"tags":[]},"source":["CUDA (Compute Unified Device Architecture) is NVIDIA's parallel computing platform and API that lets developers use NVIDIA Graphics Processing Units (GPUs) for general-purpose tasks, not just graphics, enabling massive speedups for data-intensive applications like AI, scientific simulation, and high-performance computing (HPC) by harnessing thousands of GPU cores. "]},{"cell_type":"code","execution_count":13,"id":"cdeeae2c","metadata":{"execution":{"iopub.execute_input":"2025-12-22T04:28:53.925164Z","iopub.status.busy":"2025-12-22T04:28:53.924745Z","iopub.status.idle":"2025-12-22T04:28:54.348657Z","shell.execute_reply":"2025-12-22T04:28:54.347905Z"},"papermill":{"duration":0.43039,"end_time":"2025-12-22T04:28:54.350067","exception":false,"start_time":"2025-12-22T04:28:53.919677","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor after moving to device: tensor([2.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n"]}],"source":["# Move tensor to GPU if available\n","\n","x = x.to(device)\n","print(\"Tensor after moving to device:\", x)"]},{"cell_type":"markdown","id":"4dd11881","metadata":{"papermill":{"duration":0.004913,"end_time":"2025-12-22T04:28:54.359927","exception":false,"start_time":"2025-12-22T04:28:54.355014","status":"completed"},"tags":[]},"source":["- `torch.cuda.is_available()` → Checks if a CUDA-compatible GPU is present.  \n","- `.to(device)` → Moves tensor to the selected device (`cpu` or `cuda`).  \n","- Even if you only have CPU, this code runs safely, defaulting to CPU.  \n","- This is the standard PyTorch pattern for device-agnostic code.  \n"]},{"cell_type":"code","execution_count":14,"id":"e030691d","metadata":{"execution":{"iopub.execute_input":"2025-12-22T04:28:54.370123Z","iopub.status.busy":"2025-12-22T04:28:54.369691Z","iopub.status.idle":"2025-12-22T04:29:04.919596Z","shell.execute_reply":"2025-12-22T04:29:04.918825Z"},"papermill":{"duration":10.556642,"end_time":"2025-12-22T04:29:04.921026","exception":false,"start_time":"2025-12-22T04:28:54.364384","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["7.51 ms ± 126 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n","564 µs ± 3.45 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"]}],"source":["# -------------------------\n","# GPU vs CPU Computation\n","# -------------------------\n","\n","# We create two square matrices of size 1000 × 1000\n","# This size is large enough to see CPU vs GPU difference\n","size = (1000, 1000)\n","\n","# torch.randn generates random values from a normal distribution\n","# These tensors live in CPU memory by default\n","a_cpu = torch.randn(size)\n","b_cpu = torch.randn(size)\n","\n","\n","# Move tensors to GPU \n","if device == \"cuda\":\n","    \n","    a_gpu = a_cpu.to(device)\n","    b_gpu = b_cpu.to(device)\n","\n","    # %timeit is a Jupyter magic command\n","    # It runs the operation multiple times and reports average execution time\n","\n","    # Matrix multiplication on CPU\n","    %timeit torch.matmul(a_cpu, b_cpu)\n","\n","    # Matrix multiplication on GPU\n","    %timeit torch.matmul(a_gpu, b_gpu)"]},{"cell_type":"markdown","id":"ca6322a9","metadata":{"papermill":{"duration":0.00453,"end_time":"2025-12-22T04:29:04.930385","exception":false,"start_time":"2025-12-22T04:29:04.925855","status":"completed"},"tags":[]},"source":["The GPU execution is significantly faster than the CPU for a 1000×1000 matrix multiplication (~10× speedup). This is because matrix multiplication is a highly parallel, compute-intensive operation that maps well to GPU architecture. Since the tensors were moved to the GPU before timing, this benchmark reflects pure compute performance without CPU–GPU transfer overhead. For large tensor operations and repeated computations (as in neural network training), GPUs provide substantial speedups over CPUs."]},{"cell_type":"markdown","id":"e23d8c87","metadata":{"papermill":{"duration":0.004378,"end_time":"2025-12-22T04:29:04.939322","exception":false,"start_time":"2025-12-22T04:29:04.934944","status":"completed"},"tags":[]},"source":["# Key Takeaways from Day 13\n","\n","- PyTorch tensors = NumPy arrays + GPU + autograd\n","- Supports automatic differentiation with `requires_grad`\n","- GPU usage drastically accelerates large computations\n","- PyTorch is foundation for building neural networks\n","\n","---"]},{"cell_type":"markdown","id":"3723a0e0","metadata":{"papermill":{"duration":0.004476,"end_time":"2025-12-22T04:29:04.948373","exception":false,"start_time":"2025-12-22T04:29:04.943897","status":"completed"},"tags":[]},"source":["<p style=\"text-align:center; font-size:18px;\">\n","© 2025 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31234,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":18.61184,"end_time":"2025-12-22T04:29:06.272567","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-22T04:28:47.660727","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}