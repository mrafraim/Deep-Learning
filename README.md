# **Deep Learning 44-Day Daily Roadmap**


## **Phase 1: Foundations (Day 1–8)**

| Day | Topic                     | Goal                                                                           |
| --- | ------------------------- | -------------------------------------------------------------------------------------------------------- |
| 1   | What is Deep Learning?    | Introduction, history, real-world examples, first neuron intuition, forward propagation of single neuron |
| 2   | Neurons, Weights, Bias & Activations    | Deep dive into neuron structure, weights, bias, activation functions, visualizations                     |
| 3   | Forward Propagation       | Manual calculations, numpy implementation, ReLU, Sigmoid, Tanh                                           |
| 4   | Tiny Neural Network       | Build 2-layer network from scratch in numpy                                                              |
| 5   | Loss Functions            | MSE, Cross-Entropy, small examples                                                                       |
| 6   | Optimizers                | Gradient Descent intuition, learning rate, update rules, simple code                                     |
| 7   | Mini Exercise             | Manual neuron calculations, small network experiments                                                    |
| 8   | Summary (Day 1 - Day 7) | wrap-up, code + visuals (if needed)                                               |


## **Phase 2: First Neural Networks & Backpropagation (Day 9–19)**

| Day | Topic                           | Goal                                    |
| --- | ------------------------------- | -------------------------------------------------------- |
| 9   | Gradient & Derivative Intuition | Calculus review, small examples                          |
| 10  | Backpropagation Basics          | Manual chain rule calculations                           |
| 11  | Backpropagation in Numpy        | Tiny network training step-by-step                       |
| 12  | Full Forward + Backprop Example | Numpy mini training loop                                 |
| 13  | PyTorch Introduction            | Tensors, basic operations, GPU usage                     |
| 14  | PyTorch Neural Network          | Build first simple model                                 |
| 15  | Training loop in PyTorch        | Forward + loss + backward + optimizer step               |
| 16  | Overfitting vs Underfitting     | Visualize loss curves, concept explanation               |
| 17  | Validation & Test split         | Data handling in PyTorch, metrics                        |
| 18  | Hyperparameters                 | Learning rate, epochs, batch size, grid search intuition |
| 19  | Phase Summary                   | Notebook wrap-up, code + visuals                         |


## **Phase 3: Convolutional & Recurrent Networks (Day 20–33)**

| Day | Topic                       | Goal                            |
| --- | --------------------------- | ------------------------------------------------- |
| 20  | CNN intuition               | Filters, stride, padding, convolution example     |
| 21  | CNN Layers                  | Pooling, flatten, fully connected layers          |
| 22  | CNN in PyTorch              | Build simple CNN for MNIST                        |
| 23  | CNN training                | Forward + backward + optimizer, visualize filters |
| 24  | RNN intuition               | Sequence data, hidden state, unrolling            |
| 25  | LSTM & GRU                  | Why LSTM > RNN, gates explanation                 |
| 26  | Simple RNN in PyTorch       | Manual sequence prediction                        |
| 27  | LSTM example                | Text sequence prediction                          |
| 28  | NLP preprocessing           | Tokenization, embedding, padding                  |
| 29  | RNN mini project            | Predict sentiment on small dataset                |
| 30  | CNN + RNN comparison        | When to use which, pros/cons                      |
| 31  | Regularization in CNN/RNN   | Dropout, batch norm, visualization                |
| 32  | Hyperparameters for CNN/RNN | Learning rate, optimizer tuning                   |
| 33  | Phase Summary               | Notebook wrap-up with multiple small examples     |


## **Phase 4: Advanced Topics & Regularization (Day 34–44)**

| Day | Topic                               | Goal                     |
| --- | ----------------------------------- | ----------------------------------------- |
| 34  | Dropout & BatchNorm                 | Concept + small CNN example               |
| 35  | Weight Initialization               | Why it matters, demo code                 |
| 36  | Advanced optimizers                 | Adam, AdamW, RMSProp                      |
| 37  | Learning Rate Scheduling            | StepLR, CosineAnnealing, practical tuning |
| 38  | Data Augmentation                   | Image augmentations, PyTorch transforms   |
| 39  | Transfer Learning                   | Using pretrained ResNet, fine-tuning      |
| 40  | Multi-class classification tricks   | Softmax, CrossEntropy, metrics            |
| 41  | Multi-label classification          | BCEWithLogits, thresholding               |
| 42  | Early stopping & checkpointing      | Save best model, prevent overfitting      |
| 43  | Model evaluation & confusion matrix | Visualization, metrics explanation        |
| 44  | Phase Summary                       | Notebook wrap-up, all techniques combined |

---
