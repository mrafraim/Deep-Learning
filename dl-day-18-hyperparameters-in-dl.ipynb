{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-18-hyperparameters-in-dl?scriptVersionId=288422925\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"b1c5eda1","metadata":{"papermill":{"duration":0.004416,"end_time":"2025-12-25T17:59:08.738804","exception":false,"start_time":"2025-12-25T17:59:08.734388","status":"completed"},"tags":[]},"source":["# Day 18: Hyperparameters in DL\n","\n","Welcome to Day 18!\n","\n","Today you'll learn:\n","- Understand key hyperparameters: **learning rate, epochs, batch size**\n","- Learn how hyperparameters affect training and generalization\n","- Introduce grid search intuition for hyperparameter tuning\n","\n","\n","If you found this notebook helpful, your **<b style=\"color:red;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"a031c7d2","metadata":{"papermill":{"duration":0.003055,"end_time":"2025-12-25T17:59:08.745194","exception":false,"start_time":"2025-12-25T17:59:08.742139","status":"completed"},"tags":[]},"source":["# What are Hyperparameters?\n","\n","- Hyperparameters are parameters set before training, not learned from data.  \n","- They control how the model learns, how fast it converges, and how well it generalizes.\n","\n","Key hyperparameters we will explore:\n","1. **Learning Rate (LR)**: Step size for gradient updates  \n","2. **Epochs**: Number of complete passes through the training data  \n","3. **Batch Size**: Number of samples processed before each gradient update\n"]},{"cell_type":"markdown","id":"91289d63","metadata":{"papermill":{"duration":0.002901,"end_time":"2025-12-25T17:59:08.751203","exception":false,"start_time":"2025-12-25T17:59:08.748302","status":"completed"},"tags":[]},"source":["# Learning Rate (LR)\n","\n","The learning rate controls how big a step the optimizer takes when updating model parameters during training.  \n","It is the single most sensitive hyperparameter in deep learning, get this wrong and nothing else matters.\n","\n","\n","## What Learning Rate Actually Does\n","\n","At each training step, parameters are updated as:\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)\n","$$\n","\n","Where:\n","- $\\theta$ = model parameters  \n","- $\\eta$ = learning rate  \n","- $\\nabla L(\\theta)$ = gradient of the loss  \n","\n","LR scales the gradient. It does not change direction, only *how far* you move.\n","\n","\n","## Too Large Learning Rate\n","\n","**Symptoms**\n","- Loss oscillates wildly\n","- Loss increases instead of decreasing\n","- NaN or Inf values appear\n","- Validation loss explodes early\n","\n","**Why It Happens**\n","- Updates overshoot the minimum\n","- Parameters jump back and forth across the loss valley\n","- Optimizer never settles\n","\n","**Real-World Impact**\n","- Training becomes unstable\n","- Model fails silently (especially with Adam)\n","- Wasted compute with no learning\n","\n","\n","## Too Small Learning Rate\n","\n","**Symptoms**\n","- Loss decreases very slowly\n","- Training appears “stuck”\n","- Validation loss plateaus early\n","\n","**Why It Happens**\n","- Updates are too tiny to escape flat regions\n","- Requires excessive epochs\n","- Can get trapped in sharp local minima or saddle points\n","\n","**Real-World Impact**\n","- Overfitting risk (long training)\n","- Inefficient GPU usage\n","- False belief that model architecture is bad\n","\n","## Sweet Spot: The Goldilocks Zone\n","\n","A good learning rate:\n","- Decreases training loss smoothly\n","- Validation loss follows with a small gap\n","- No large oscillations\n","- Reaches convergence in reasonable epochs\n","\n","## Rule of Thumb (Practitioner Defaults)\n","\n","| Model Type | Typical LR |\n","|---------|------------|\n","| Linear / Logistic Regression | `0.1 – 0.01` |\n","| Neural Networks (SGD) | `0.01 – 0.001` |\n","| Neural Networks (Adam) | `0.001 – 0.0001` |\n","| Fine-tuning Pretrained Models | `1e-5 – 1e-4` |\n","\n","> Start large enough to learn, small enough to survive."]},{"cell_type":"markdown","id":"20b0522f","metadata":{"papermill":{"duration":0.002932,"end_time":"2025-12-25T17:59:08.75721","exception":false,"start_time":"2025-12-25T17:59:08.754278","status":"completed"},"tags":[]},"source":["# Epoch\n","\n","An epoch is one complete pass of the entire training dataset through the model. That’s it.\n","\n","But that definition alone is shallow. The meaning of an epoch comes from what actually happens during it.\n","\n","## What Actually Happens Inside One Epoch\n","\n","Assume:\n","\n","* Dataset size = 1,000 samples\n","* Batch size = 100\n","\n","Inside 1 epoch:\n","\n","* The data is split into 10 batches\n","* For each batch:\n","\n","  1. Forward pass\n","  2. Loss calculation\n","  3. Backpropagation\n","  4. Weight update\n","\n","So:\n","\n","1 epoch = 10 gradient updates\n","\n","General formula:\n","\n","$$\n","\\text{Updates per epoch} = \\frac{\\text{Number of samples}}{\\text{Batch size}}\n","$$\n","\n","This is the real operational meaning.\n","\n","## Why Epoch ≠ Learning\n","\n","A common beginner mistake:\n","\n","> “More epochs means better learning”\n","\n","<b style=\"color:red;\">Wrong!</b>\n","\n","Epochs only define how many times the model sees the data, not how well it understands it.\n","\n","Learning quality depends on:\n","\n","* Learning rate\n","* Batch size\n","* Model capacity\n","* Data quality\n","\n","Epochs just control exposure count.\n","\n","## Why We Need Multiple Epochs\n","\n","On the first epoch:\n","\n","* Weights are random\n","* Gradients are large and chaotic\n","* The model captures only coarse patterns\n","\n","Each additional epoch:\n","\n","* Refines parameters\n","* Reduces loss\n","* Fits finer structure\n","\n","But after a point:\n","\n","* The model starts fitting noise\n","* Validation performance degrades\n","\n","That’s where overfitting begins.\n","\n","\n","## Epoch vs Iteration\n","\n","| Term          | Meaning                       |\n","| ------------- | ----------------------------- |\n","| Iteration | One batch → one weight update |\n","| Epoch     | All batches processed once    |\n","\n","Relationship:\n","\n","$$\n","\\text{Iterations} = \\text{Epochs} \\times \\frac{N}{\\text{Batch Size}}\n","$$\n","\n","Professionals think in iterations, not epochs.\n","\n","\n","## Why Epoch Count is a Weak Hyperparameter\n","\n","In real systems:\n","\n","* Dataset sizes vary\n","* Batch sizes change\n","* Distributed training alters update frequency\n","\n","So:\n","\n","* “Train for 50 epochs” is meaningless without context\n","* “Train for 100k updates” is precise\n","\n","Epochs are a human convenience, not a fundamental unit.\n","\n","## Industry Reality\n","\n","What actually happens in production-grade training:\n","\n","* Epoch count is rarely fixed\n","* Training stops based on:\n","\n","  * Validation loss plateau\n","  * Early stopping\n","  * Budget constraints\n","\n","* Epochs are used only for:\n","\n","  * Logging\n","  * Checkpointing\n","  * Monitoring progress\n","\n","\n","## Mental Model\n","\n","> Epoch = one full opportunity for the model to correct itself using the entire dataset.\n","\n","But:\n","\n","* Too few opportunities → underfitting\n","* Too many → memorization\n","\n","An epoch is one full pass over the training data, resulting in multiple weight updates, its value lies not in the number itself, but in how it interacts with batch size, learning rate, and stopping criteria.\n"]},{"cell_type":"markdown","id":"84087c61","metadata":{"papermill":{"duration":0.003039,"end_time":"2025-12-25T17:59:08.763152","exception":false,"start_time":"2025-12-25T17:59:08.760113","status":"completed"},"tags":[]},"source":["# Batch Size\n","\n","Batch size is the number of training samples used to compute one gradient update.\n","\n","One batch → one forward pass + one backward pass + one weight update.\n","\n","That’s the atomic unit of learning.\n","\n","## What Batch Size Really Controls\n","\n","Batch size does not control speed alone. It controls how noisy your learning signal is.\n","\n","Mathematically, each gradient is an estimate of the true gradient over the full dataset:\n","\n","$$\n","\\nabla \\mathcal{L}*{batch} \\approx \\nabla \\mathcal{L}*{data}\n","$$\n","\n","Batch size determines the quality of this approximation.\n","\n","## Small vs Large Batch: The Real Difference\n","\n","### Small Batch (e.g. 8–64)\n","\n","**Behavior**\n","\n","* Gradient changes direction a lot  \n","  → because each update sees only a small, incomplete view of the data  \n","* Loss curve looks jumpy  \n","  → sometimes up, sometimes down, even if learning is happening  \n","* Many updates per epoch  \n","  → weights are adjusted very frequently  \n","\n","**Why it often generalizes better**\n","\n","* The randomness forces the model to learn patterns that work across many batches, not just one  \n","* The model can’t perfectly memorize training data  \n","  → this naturally reduces overfitting  \n","* The model settles into wide, stable solutions  \n","  → small input changes don’t break performance  \n","\n","**Cost**\n","\n","* Training takes longer in real time  \n","  → more updates = more computation  \n","* GPU is not fully utilized  \n","  → hardware waits for small batches  \n","* If learning rate is high, updates can overshoot  \n","  → training becomes unstable or diverges  \n","\n","### Large Batch (e.g. 512–8192)\n","\n","**Behavior**\n","\n","* Gradient direction is consistent  \n","  → each update sees a more complete picture of the data  \n","* Loss curve is smooth  \n","  → steady decrease with fewer sudden jumps  \n","* Fewer updates per epoch  \n","  → weights change less frequently  \n","\n","**Why it can generalize worse**\n","\n","* Lack of randomness lets the model lock onto very specific solutions\n","* These solutions fit training data well but break on new data  \n","* The model becomes sensitive  \n","  → small input changes can hurt predictions  \n","\n","**Benefit**\n","\n","* Much faster per epoch  \n","  → fewer updates, more parallel computation  \n","* Excellent GPU utilization  \n","  → hardware works at full capacity  \n","* Necessary for huge datasets  \n","  → small batches would take impractically long  \n","\n","## Batch Size vs Epoch vs Iteration\n","\n","Assume:\n","\n","* Dataset = 10,000 samples\n","* Batch size = 100\n","\n","Then:\n","\n","* Batches per epoch = 100\n","* Updates per epoch = 100\n","* 1 update = 1 batch\n","\n","| Term       | Meaning                    |\n","| ---------- | -------------------------- |\n","| Batch      | Chunk of data              |\n","| Batch size | Samples per batch          |\n","| Iteration  | One batch processed        |\n","| Epoch      | All batches processed once |\n","\n","## The Hidden Equation Most People Ignore\n","\n","Total learning depends on number of updates, not epochs:\n","\n","$$\n","\\text{Total Updates} = \\text{Iterations} = \\frac{N}{\\text{Batch Size}} \\times \\text{Epochs}\n","$$\n","\n","Change batch size → you change how many times weights are updated.\n","\n","That’s why batch size tuning without epoch or LR adjustment breaks training.\n","\n","## Learning Rate Coupling\n","\n","Batch size and learning rate are coupled.\n","\n","**Linear scaling heuristic:**\n","\n","* Double batch size → double learning rate\n","\n","Why?\n","\n","* Larger batch = more confident gradient\n","* Needs a larger step to stay efficient\n","\n","Ignore this → either slow convergence or divergence.\n","\n","## Why “Bigger Batch = Better” Is False\n","\n","### Large Batch\n","\n","* Optimizes faster\n","\n","  → each update uses lots of data, so the direction is confident  \n","  → loss drops smoothly and quickly  \n","\n","* Learns narrower solutions\n","\n","  → the model settles into very specific parameter settings  \n","  → works extremely well on training data  \n","  → small changes in input or data distribution can hurt performance  \n","\n","\n","### Small Batch\n","\n","* Optimizes slower\n","\n","  → each update is based on limited data  \n","  → progress looks messy and takes more time  \n","\n","* Learns more robust solutions\n","\n","  → randomness forces the model to perform well across many different mini-samples  \n","  → solutions are tolerant to noise and unseen data  \n","  → better real-world performance  \n","\n","### Generalization Lives in Noise\n","\n","* Noise prevents the model from becoming overconfident  \n","* Noise pushes the model away from fragile solutions  \n","* Noise acts like built-in regularization  \n","\n","> A model that learns smoothly is not always a model that learns **well**.\n","\n","## Industry Defaults\n","\n","| Task                     | Typical Batch        |\n","| ------------------------ | -------------------- |\n","| Tabular ML               | 32–128               |\n","| CNN (vision)             | 64–256               |\n","| Transformers fine-tuning | 8–64                 |\n","| Pretraining LLMs         | 2k–32k (distributed) |\n","\n","Context decides, not dogma.\n","\n","## Mental Model\n","\n","> Batch size = how many examples you trust before changing your mind.\n","\n","Small batch → “I update my belief often, even if noisy”\n","\n","Large batch → “I wait for more evidence before updating”\n","\n","Batch size is the number of samples used per weight update, controlling gradient noise, update frequency, generalization behavior, and training efficiency, not just speed."]},{"cell_type":"markdown","id":"41b01502","metadata":{"papermill":{"duration":0.002924,"end_time":"2025-12-25T17:59:08.769322","exception":false,"start_time":"2025-12-25T17:59:08.766398","status":"completed"},"tags":[]},"source":["# Sample Experiment \n","\n","You can play by changing the learning rate, batch size and epoch to observe the model performance.\n","\n","Recommended:\n","\n","- Keep the experiment small (like 200 samples) for speed\n","- Change one hyperparameter at a time to isolate effect\n","- Track training loss vs. validation loss for each setting\n","- Optional: Make plots for each change to visually see the impact"]},{"cell_type":"code","execution_count":1,"id":"d87916e9","metadata":{"execution":{"iopub.execute_input":"2025-12-25T17:59:08.777219Z","iopub.status.busy":"2025-12-25T17:59:08.776859Z","iopub.status.idle":"2025-12-25T17:59:14.0341Z","shell.execute_reply":"2025-12-25T17:59:14.033029Z"},"papermill":{"duration":5.264273,"end_time":"2025-12-25T17:59:14.036547","exception":false,"start_time":"2025-12-25T17:59:08.772274","status":"completed"},"tags":[]},"outputs":[],"source":["# Import libraries\n","\n","import torch                                           # Import core PyTorch library\n","import torch.nn as nn                                  # Import neural network modules (layers, loss functions)\n","from torch.utils.data import DataLoader, TensorDataset # Utilities for batching and dataset handling\n"]},{"cell_type":"code","execution_count":2,"id":"288b1ab1","metadata":{"execution":{"iopub.execute_input":"2025-12-25T17:59:14.045127Z","iopub.status.busy":"2025-12-25T17:59:14.044657Z","iopub.status.idle":"2025-12-25T17:59:14.121629Z","shell.execute_reply":"2025-12-25T17:59:14.120649Z"},"papermill":{"duration":0.083913,"end_time":"2025-12-25T17:59:14.123927","exception":false,"start_time":"2025-12-25T17:59:14.040014","status":"completed"},"tags":[]},"outputs":[],"source":["# Simple Dataset\n","\n","torch.manual_seed(42)                           # same sequence of random numbers every time\n","X = torch.randn(200, 2)                         # 200 samples and 2 features\n","y = X[:,0]*2 + X[:,1]*-3 + torch.randn(200)*0.5 # linear function + noise\n","y = y.unsqueeze(1)                              # Reshape target to (200, 1) to match model shape           "]},{"cell_type":"code","execution_count":3,"id":"e64491ca","metadata":{"execution":{"iopub.execute_input":"2025-12-25T17:59:14.132259Z","iopub.status.busy":"2025-12-25T17:59:14.131886Z","iopub.status.idle":"2025-12-25T17:59:14.138728Z","shell.execute_reply":"2025-12-25T17:59:14.137708Z"},"papermill":{"duration":0.013724,"end_time":"2025-12-25T17:59:14.140908","exception":false,"start_time":"2025-12-25T17:59:14.127184","status":"completed"},"tags":[]},"outputs":[],"source":["# Dataset & Dataloader\n","\n","df =  TensorDataset(X,y)                                 # Combines inputs and targets into PyTorch dataset\n","loader = DataLoader(df, batch_size = 16, shuffle = True) # DataLoader handles batching and shuffling automatically"]},{"cell_type":"code","execution_count":4,"id":"18f8f1dd","metadata":{"execution":{"iopub.execute_input":"2025-12-25T17:59:14.149128Z","iopub.status.busy":"2025-12-25T17:59:14.148749Z","iopub.status.idle":"2025-12-25T17:59:21.050503Z","shell.execute_reply":"2025-12-25T17:59:21.049356Z"},"papermill":{"duration":6.908866,"end_time":"2025-12-25T17:59:21.05299","exception":false,"start_time":"2025-12-25T17:59:14.144124","status":"completed"},"tags":[]},"outputs":[],"source":["# Model\n","\n","model = nn.Linear(2,1) # Simple LR model with 2 input features and 1 output\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"]},{"cell_type":"code","execution_count":5,"id":"d7420767","metadata":{"execution":{"iopub.execute_input":"2025-12-25T17:59:21.061361Z","iopub.status.busy":"2025-12-25T17:59:21.060896Z","iopub.status.idle":"2025-12-25T17:59:21.508265Z","shell.execute_reply":"2025-12-25T17:59:21.507071Z"},"papermill":{"duration":0.454295,"end_time":"2025-12-25T17:59:21.510578","exception":false,"start_time":"2025-12-25T17:59:21.056283","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0: Loss = 6.6718\n","Epoch 10: Loss = 0.3366\n","Epoch 20: Loss = 0.2362\n","Epoch 30: Loss = 0.2341\n","Epoch 40: Loss = 0.2338\n"]}],"source":["# Training Loop\n","\n","epochs = 50\n","\n","for epoch in range(epochs):\n","\n","    model.train()                                # Set model to training model\n","    running_loss = 0\n","\n","    for xb,yb in loader:\n","\n","        optimizer.zero_grad()                     # Clear old gradients\n","        preds = model(xb)                         # Forward pass: predictions\n","        loss = criterion(preds, yb)               # Loss calculation\n","        loss.backward()                           # Backward pass: compute gradients\n","        optimizer.step()                          # Weights updated\n","\n","        running_loss += loss.item() * xb.size(0)  # Batch avg. loss ---> total loss\n","\n","    running_loss /= len(loader.dataset)           # Avg. loss per sample\n","\n","    if epoch %10 == 0:                            # Print progress\n","        print(f\"Epoch {epoch}: Loss = {running_loss:.4f}\")        "]},{"cell_type":"markdown","id":"42c970cb","metadata":{"papermill":{"duration":0.003173,"end_time":"2025-12-25T17:59:21.517097","exception":false,"start_time":"2025-12-25T17:59:21.513924","status":"completed"},"tags":[]},"source":["The loss decreases rapidly in the first few epochs, indicating that the model is quickly capturing the main patterns in the data. After epoch 20, the loss plateaus, suggesting convergence. Minimal improvement beyond this point indicates that further training may not yield significant gains and early stopping could be considered."]},{"cell_type":"markdown","id":"376042e2","metadata":{"papermill":{"duration":0.003084,"end_time":"2025-12-25T17:59:21.523489","exception":false,"start_time":"2025-12-25T17:59:21.520405","status":"completed"},"tags":[]},"source":["# Grid Search for Hyperparameters\n","\n","- **Goal:** Find the combination of hyperparameters that minimizes validation loss\n","- **Approach:**\n","    1. Define a set of candidate values for each hyperparameter  \n","        - e.g., LR = [0.001, 0.01, 0.1], Batch Size = [16,32,64], Epochs = [20,50]  \n","    2. Train a model for every combination  \n","    3. Evaluate on validation set  \n","    4. Pick the combination with lowest validation error\n","\n","- **Tip:**  \n","    - Grid search is exhaustive → slow for many hyperparameters  \n","    - Alternatives: Random search, Bayesian optimization, or learning rate schedulers\n"]},{"cell_type":"markdown","id":"f70a6557","metadata":{"papermill":{"duration":0.002997,"end_time":"2025-12-25T17:59:21.529592","exception":false,"start_time":"2025-12-25T17:59:21.526595","status":"completed"},"tags":[]},"source":["#  Key Takeaways FROM Day 18\n","\n","- Hyperparameters control how the model learns, not what it learns  \n","- Learning rate affects convergence speed and stability  \n","- Epochs control total training time → watch for overfitting  \n","- Batch size affects gradient stability and generalization  \n","- Grid search helps systematically find optimal hyperparameters  \n","- Validation set is essential for hyperparameter tuning\n","\n","---"]},{"cell_type":"markdown","id":"1eba2f1e","metadata":{"papermill":{"duration":0.003023,"end_time":"2025-12-25T17:59:21.535663","exception":false,"start_time":"2025-12-25T17:59:21.53264","status":"completed"},"tags":[]},"source":["<p style=\"text-align:center; font-size:18px;\">\n","© 2025 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":18.568869,"end_time":"2025-12-25T17:59:23.565117","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-25T17:59:04.996248","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}