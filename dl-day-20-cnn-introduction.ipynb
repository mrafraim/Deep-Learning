{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e88ce9",
   "metadata": {
    "papermill": {
     "duration": 0.005175,
     "end_time": "2025-12-29T05:05:27.482026",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.476851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Day 20: CNN Introduction\n",
    "\n",
    "Welcome to Day 20!\n",
    "\n",
    "Today you'll learn:\n",
    "\n",
    "- What is CNN?\n",
    "- Understand why CNNs exist\n",
    "- Learn convolution operation step-by-step\n",
    "- Understand filters, stride, padding\n",
    "- Manually compute a convolution\n",
    "- Implement convolution using NumPy\n",
    "\n",
    "If you found this notebook helpful, your **<b style=\"color:red;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc8afb",
   "metadata": {
    "papermill": {
     "duration": 0.003996,
     "end_time": "2025-12-29T05:05:27.490236",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.486240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is a CNN?\n",
    "\n",
    "A Convolutional Neural Network (CNN) is a specialized type of neural network designed to work with grid-structured data, especially images.\n",
    "\n",
    "Examples of grid-structured data:\n",
    "- Images → 2D grid of pixels  \n",
    "- Videos → 3D grid (height × width × time)  \n",
    "- Spectrograms → time × frequency grid  \n",
    "\n",
    "CNNs are the standard architecture for computer vision because they preserve and exploit the spatial structure of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abac87f",
   "metadata": {
    "papermill": {
     "duration": 0.003818,
     "end_time": "2025-12-29T05:05:27.498045",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.494227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Why Traditional Neural Networks Fail for Images\n",
    "\n",
    "Consider a grayscale image of size 28 × 28:\n",
    "\n",
    "- Total pixels:  \n",
    "  $$\n",
    "  28 \\times 28 = 784\n",
    "  $$\n",
    "\n",
    "If we use a fully connected layer with just 1,000 neurons:\n",
    "\n",
    "$$\n",
    "784 \\times 1000 = 784{,}000 \\text{ parameters}\n",
    "$$\n",
    "\n",
    "Problems:\n",
    "1. **Too many parameters** → slow training, high memory usage  \n",
    "2. **No spatial awareness**  \n",
    "   - Neighboring pixels are treated the same as distant pixels  \n",
    "3. **No translation understanding**  \n",
    "   - The same object in a different position looks completely new  \n",
    "\n",
    "Fully connected networks ignore how images are structured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3273bcb",
   "metadata": {
    "papermill": {
     "duration": 0.003842,
     "end_time": "2025-12-29T05:05:27.505896",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.502054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Core Ideas Behind CNNs\n",
    "\n",
    "CNNs are built on three key assumptions about images. Let’s break down each one carefully.\n",
    "\n",
    "### 1. Locality\n",
    "\n",
    "Locality means most important visual information in an image is contained in small, local regions, rather than spread across the entire image.\n",
    "\n",
    "* **Local region / patch:** A small area of the image, e.g., a 3×3 or 5×5 block of pixels.\n",
    "* **Edge:** A boundary where the intensity of pixels changes sharply (e.g., where light meets dark).\n",
    "* **Corner:** A point where two edges meet.\n",
    "* **Texture:** Repeating patterns in a small region, like stripes or dots.\n",
    "\n",
    "Pixels are meaningful relative to their neighbors, not the entire image. Detecting edges, corners, or textures locally helps the network understand small building blocks of the image.\n",
    "\n",
    "### 2. Parameter Sharing\n",
    "\n",
    "Parameter sharing means the same set of weights (filter) is applied across multiple positions in the image.\n",
    "\n",
    "* **Filter (kernel):** A small matrix of numbers that slides over the image to detect specific patterns.\n",
    "* **Weights:** Numbers in the filter that the network learns during training.\n",
    "* **Feature map:** The result of applying a filter across the image, showing where the pattern occurs.\n",
    "\n",
    "Instead of learning a separate detector for every location in the image, CNNs learn one filter and reuse it everywhere.\n",
    "\n",
    "* This drastically reduces the number of parameters (learnable weights), making training more efficient.\n",
    "* It also allows the network to recognize the same pattern regardless of its position, a property called **translation invariance**.\n",
    "\n",
    "### 3. Spatial Hierarchy\n",
    "\n",
    "Spatial hierarchy means simple patterns combine to form complex structures in a layered manner.\n",
    "\n",
    "* **Layer:** A level in the neural network that transforms input into more abstract features.\n",
    "* **Edges → shapes → object parts → full objects:** This describes how visual features are learned progressively:\n",
    "\n",
    "  1. Early layers detect simple features (edges, corners)\n",
    "  2. Middle layers combine them into shapes or textures\n",
    "  3. Deep layers recognize complex objects\n",
    "\n",
    "CNNs automatically learn a hierarchy of features, building up from local patterns to high-level concepts. This is one of the main reasons CNNs work so well for images.\n",
    "\n",
    "\n",
    " **Summary of New Terms**\n",
    "\n",
    "| Term                   | Definition                                                    |\n",
    "| ---------------------- | ------------------------------------------------------------- |\n",
    "| Local region / patch   | Small area of the image (e.g., 3×3 pixels)                    |\n",
    "| Edge                   | Boundary of sharp pixel intensity change                      |\n",
    "| Corner                 | Intersection point of two edges                               |\n",
    "| Texture                | Repeating pattern in a local area                             |\n",
    "| Filter / Kernel        | Small learnable matrix applied across the image               |\n",
    "| Weights                | Learnable numbers in the filter that detect patterns          |\n",
    "| Feature map            | Output of a filter showing where a pattern occurs             |\n",
    "| Layer                  | Level in a neural network that transforms input into features |\n",
    "| Translation invariance | Ability to detect the same pattern regardless of its location |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d465f",
   "metadata": {
    "papermill": {
     "duration": 0.003802,
     "end_time": "2025-12-29T05:05:27.513827",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.510025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What Makes a CNN?\n",
    "\n",
    "A Convolutional Neural Network (CNN) is a neural network designed to work with images by understanding what patterns exist and where they appear.\n",
    "\n",
    "To understand this, we must clearly define three new terms:\n",
    "- Convolution operation  \n",
    "- Filter (Kernel)  \n",
    "- Feature map  \n",
    "\n",
    "We will explain each from scratch.\n",
    "\n",
    "### 1. Convolution Operation\n",
    "\n",
    "A convolution is a mathematical operation where a small matrix (filter) slides over an input (like an image) and computes a weighted sum at each position.\n",
    "\n",
    "Instead of looking at the entire image at once (like fully connected layers), convolution focuses on small local regions.\n",
    "\n",
    "- Imagine placing a small transparent grid on top of an image.\n",
    "- You slide it step by step.\n",
    "- At each position, you check how well the grid matches the image underneath.\n",
    "- This is different from full matrix multiplication (used in dense layers) because we only focus on small local patches instead of the whole image at once.\n",
    "\n",
    "This is how CNNs scan images.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Input image patch:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 0 \\\\\n",
    "0 & 1 & 3 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Filter (kernel):\n",
    "$$\n",
    "K =\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & -4 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Slide the filter across the image\n",
    "- At each position, compute:\n",
    "$$\n",
    "\\text{sum of } (X \\odot K) = \\sum_{i,j} X_{i,j} \\cdot K_{i,j}\n",
    "$$\n",
    "\n",
    "The result (feature map) is  one number that tells how strongly the pattern exists at that location.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "\n",
    "Image (3×3):\n",
    "\n",
    "$$\n",
    "I = \\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Filter (2×2):\n",
    "\n",
    "$$\n",
    "F = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Step 1: Patches of the image\n",
    "\n",
    "Top-left patch:\n",
    "\n",
    "$$\n",
    "P_1 = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "4 & 5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Top-right patch:\n",
    "\n",
    "$$\n",
    "P_2 = \\begin{bmatrix}\n",
    "2 & 3 \\\\\n",
    "5 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Bottom-left patch:\n",
    "\n",
    "$$\n",
    "P_3 = \\begin{bmatrix}\n",
    "4 & 5 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Bottom-right patch:\n",
    "\n",
    "$$\n",
    "P_4 = \\begin{bmatrix}\n",
    "5 & 6 \\\\\n",
    "8 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Step 2: Flatten for dot product\n",
    "\n",
    "Filter flattened: $[1, 0, 0, -1]$  \n",
    "Top-left patch flattened: $[1, 2, 4, 5]$  \n",
    "\n",
    "Step 3: Dot product\n",
    "\n",
    "Top-left patch: $1*1 + 0*2 + 0*4 + (-1)*5 = -4$  \n",
    "\n",
    "Top-right patch: $1*2 + 0*3 + 0*5 + (-1)*6 = -4$  \n",
    "\n",
    "Bottom-left patch: $1*4 + 0*5 + 0*7 + (-1)*8 = -4$  \n",
    "\n",
    "Bottom-right patch: $1*5 + 0*6 + 0*8 + (-1)*9 = -4$  \n",
    "\n",
    "\n",
    "Resulting Feature Map (2×2)\n",
    "\n",
    "$$\n",
    "\\text{Feature Map} = \\begin{bmatrix}\n",
    "-4 & -4 \\\\\n",
    "-4 & -4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This demonstrates how the convolution operation computes a dot product between the filter and each patch of the image, producing a feature map that highlights where the filter pattern appears.\n",
    "\n",
    "In short:\n",
    "\n",
    "Convolution is a mathematical operation that:\n",
    "- Slides a small matrix (filter / kernel) over an input (like an image)\n",
    "- Computes element-wise multiplication\n",
    "- Sums the result to produce a feature map\n",
    "\n",
    "This allows the network to detect:\n",
    "- Edges\n",
    "- Corners\n",
    "- Textures\n",
    "- Shapes\n",
    "\n",
    "\n",
    "### 2. Filter (Kernel)\n",
    "\n",
    "A filter (also called a kernel) is a small learnable matrix of numbers used to detect a specific visual pattern.\n",
    "\n",
    "- Its job: detect specific patterns (edges, textures, corners) in images.\n",
    "- Typical sizes: 3×3, 5×5, 7×7  \n",
    "- Learns during training  \n",
    "- The same filter is applied across the entire image  \n",
    "\n",
    "**Example:**  Filter (Edge Detector)\n",
    "\n",
    "Vertical edge filter (Sobel):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\n",
    "-2 & 0 & 2 \\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- This filter responds strongly to vertical edges*.\n",
    "- When applied to an image:\n",
    "    - Strong response → vertical edge exists\n",
    "    - Weak response → no vertical edge\n",
    "\n",
    "**Why Filters Matter**\n",
    "- One filter → detects one type of pattern  \n",
    "- Multiple filters → detect multiple patterns (edges, corners, textures)\n",
    "- NN learns the best filter values during training, we don’t handcraft them.\n",
    "\n",
    "---\n",
    "#### <b style=\"color:skyblue;\">*What Does “Vertical Edge” Mean?</b>\n",
    "\n",
    "A vertical edge is a location in an image where the pixel intensity changes sharply from left to right, while remaining relatively similar from top to bottom.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "> A vertical edge is a vertical boundary between two different regions of brightness or color.\n",
    "\n",
    "\n",
    "Imagine a black object on a white background:\n",
    "\n",
    "White | Black <br>\n",
    "White | Black <br>\n",
    "White | Black\n",
    "\n",
    "\n",
    "The boundary between white and black is vertical, so this boundary is called a vertical edge.\n",
    "\n",
    "Your eyes immediately notice this boundary, CNNs are trained to notice the same thing.\n",
    "\n",
    "\n",
    "Consider a grayscale image patch:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "10 & 10 & 200 \\\\\n",
    "10 & 10 & 200 \\\\\n",
    "10 & 10 & 200\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Left side pixels → dark (low values)\n",
    "- Right side pixels → bright (high values)\n",
    "\n",
    "The sudden change from left to right indicates a vertical edge.\n",
    "\n",
    "\n",
    "**Why CNNs Detect Vertical Edges First**\n",
    "\n",
    "Edges are the simplest and most informative visual patterns:\n",
    "- They define object boundaries\n",
    "- They help form shapes\n",
    "- They are stable across lighting changes\n",
    "\n",
    "CNNs usually learn:\n",
    "- Vertical edges\n",
    "- Horizontal edges\n",
    "- Diagonal edges\n",
    "\n",
    "in their first convolutional layer.\n",
    "\n",
    "\n",
    "**Vertical Edge Filter (Example)**\n",
    "\n",
    "A common vertical edge detector (Sobel filter):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\n",
    "-2 & 0 & 2 \\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**What This Filter Does**\n",
    "- Left side → negative weights\n",
    "- Right side → positive weights\n",
    "- Strong response when left ≠ right\n",
    "\n",
    "High output → vertical edge detected  \n",
    "Low output → no vertical edge\n",
    "\n",
    "---\n",
    "### 3. Feature Map\n",
    "\n",
    "A feature map is the output of applying a filter over an image.\n",
    "\n",
    "- Each value in the feature map shows how strongly the filter matched at that position.\n",
    "- Feature maps preserve the 2D structure of the image.\n",
    "\n",
    "Example\n",
    "- Input image size: 28×28  \n",
    "- Filter size: 3×3  \n",
    "- *Output feature map size: 26×26 (each value indicates how strongly the pattern is present at that location) \n",
    "\n",
    "High values in the feature map = strong presence of the pattern.\n",
    "\n",
    "Think of it as a heatmap highlighting the pattern the filter detects.\n",
    "\n",
    "---\n",
    "\n",
    "#### <b style=\"color:skyblue;\">*Why does a 28×28 image become a 26×26 feature map?</b>\n",
    "\n",
    "A 3×3 filter is a small matrix that looks at 9 pixels at a time.\n",
    "\n",
    "The filter:\n",
    "- Starts at the top-left corner\n",
    "- Computes a dot product with the underlying 3×3 patch\n",
    "- Moves one pixel at a time (this movement is called stride = 1)\n",
    "- Stops when it can no longer fully fit inside the image\n",
    "\n",
    "Critical rule\n",
    "> The filter must fit entirely inside the image. No partial overlap allowed [unless padding(talk later about that) is added, we are NOT using padding here].\n",
    "\n",
    "Visual intuition (1D first)\n",
    "\n",
    "Imagine a 1D line of 28 pixels  \n",
    "A filter of size 3\n",
    "\n",
    "Valid positions:\n",
    "\n",
    "[1 2 3] ✔ <br>\n",
    "[2 3 4] ✔ <br>\n",
    "... <br>\n",
    "[26 27 28] ✔\n",
    "\n",
    "\n",
    "How many positions?<br>\n",
    "28 - 3 + 1 = 26\n",
    "\n",
    "\n",
    "That’s the key formula.\n",
    "\n",
    "Now extend to 2D (real images)\n",
    "\n",
    "Input\n",
    "- Height = 28\n",
    "- Width = 28\n",
    "\n",
    "Filter\n",
    "- Height = 3\n",
    "- Width = 3\n",
    "\n",
    "Output height\n",
    "\n",
    "28 - 3 + 1 = 26\n",
    "\n",
    "Output width\n",
    "\n",
    "28 - 3 + 1 = 26\n",
    "\n",
    "Final output feature map size: 26 × 26\n",
    "\n",
    "**General formula:**\n",
    "\n",
    "For no padding, stride = 1:\n",
    "\n",
    "- Output Size = (N - F + 1)\n",
    "\n",
    "Where:\n",
    "- `N` = input size\n",
    "- `F` = filter size\n",
    "\n",
    "For 2D:\n",
    "\n",
    "- Output Height = (H - FH + 1)\n",
    "- Output Width = (W - FW + 1)\n",
    "\n",
    "**Why not 28×28?**\n",
    "\n",
    "Because:\n",
    "- The filter cannot sit partially outside the image\n",
    "- The last 2 rows and columns don’t have enough pixels to support a full 3×3 window\n",
    "\n",
    "CNNs are physically constrained pattern scanners, not abstract math tricks.\n",
    "\n",
    "If you keep stacking convolutions without padding:\n",
    "- Feature maps shrink fast\n",
    "- Deep networks collapse spatial dimensions\n",
    "\n",
    "That’s why padding exists to control information loss.\n",
    "\n",
    "---\n",
    "\n",
    "### Spatial Structure\n",
    "\n",
    "Spatial structure means the 2D arrangement of pixels in an image is preserved.\n",
    "\n",
    "- Unlike dense layers that flatten the image and destroy pixel positions, CNNs maintain height × width of the image through feature maps.\n",
    "- his allows the network to understand where patterns occur, not just what patterns exist.\n",
    "\n",
    "Why This Is Important\n",
    "- A pattern at the top of an image remains at the top of the feature map.\n",
    "- CNNs know both what the pattern is and where it appears.\n",
    "\n",
    "### Putting Everything Together\n",
    "\n",
    "1. Start with an image.\n",
    "2. Slide a filter over small local regions.\n",
    "3. Perform convolution at each position.\n",
    "4. Produce a feature map.\n",
    "5. Repeat with multiple filters and layers.\n",
    "\n",
    "Early layers detect edges and textures.  \n",
    "Deeper layers combine them into shapes and objects.\n",
    "\n",
    "> CNNs learn small pattern detectors (filters) that slide across images, creating feature maps that show what patterns exist and where they appear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6565d08",
   "metadata": {
    "papermill": {
     "duration": 0.003819,
     "end_time": "2025-12-29T05:05:27.521507",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.517688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stride\n",
    "\n",
    "* Stride is the number of pixels the filter moves at each step when sliding over the input image.\n",
    "* Default stride is 1 (move one pixel at a time).\n",
    "* Increasing stride reduces the size of the output feature map because the filter skips positions.\n",
    "\n",
    "Example 1: Stride = 1\n",
    "\n",
    "Input image (5×5):\n",
    "\n",
    "$$\n",
    "I = \\begin{bmatrix}\n",
    "1 & 2 & 3 & 0 & 1 \\\\\n",
    "0 & 1 & 2 & 3 & 1 \\\\\n",
    "1 & 0 & 1 & 2 & 0 \\\\\n",
    "2 & 1 & 0 & 1 & 2 \\\\\n",
    "1 & 2 & 1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Filter (3×3), stride 1:\n",
    "\n",
    "* Top-left patch = rows 0–2, cols 0–2\n",
    "* Next patch = rows 0–2, cols 1–3\n",
    "* Next patch = rows 0–2, cols 2–4\n",
    "\n",
    "Output feature map size formula:\n",
    "\n",
    "$$\n",
    "\\text{Output Height} = \\frac{H - F}{\\text{stride}} + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Output Width} = \\frac{W - F}{\\text{stride}} + 1\n",
    "$$\n",
    "\n",
    "Here, H = 5, W = 5, F = 3, stride = 1 → Output = 3×3\n",
    "\n",
    "Example 2: Stride = 2\n",
    "\n",
    "* Filter jumps 2 pixels at a time instead of 1.\n",
    "* Top-left patch = rows 0–2, cols 0–2\n",
    "* Next patch = rows 0–2, cols 2–4\n",
    "* Next patch would start at col 4 → exceeds boundary → stop\n",
    "\n",
    "Output feature map size:\n",
    "\n",
    "$$\n",
    "\\text{Output Height} = \\frac{5 - 3}{2} + 1 = 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Output Width} = \\frac{5 - 3}{2} + 1 = 2\n",
    "$$\n",
    "\n",
    "* So, stride = 2 produces a smaller 2×2 feature map.\n",
    "\n",
    "### Key Points About Stride\n",
    "\n",
    "1. **Controls feature map size**\n",
    "\n",
    "   * Larger stride → smaller feature map → fewer computations\n",
    "   * Smaller stride → larger feature map → more detailed spatial info\n",
    "\n",
    "2. **Works with padding**\n",
    "\n",
    "   * Sometimes stride > 1 and padding is used to preserve spatial dimensions\n",
    "\n",
    "3. **Intuition**\n",
    "\n",
    "   * Think of the filter as a “window” scanning the image.\n",
    "   * Stride = 1 → window moves slowly, capturing fine details.\n",
    "   * Stride = 2 → window moves faster, capturing coarse information.\n",
    "\n",
    "### Quick Visual Intuition\n",
    "\n",
    "If filter = 2×2, stride = 1:\n",
    "\n",
    "Step 1: filter covers top-left 2x2<br>\n",
    "Step 2: move right by 1 pixel → next 2x2 patch<br>\n",
    "Step 3: move right by 1 pixel → next 2x2 patch\n",
    "...\n",
    "\n",
    "\n",
    "If stride = 2:\n",
    "\n",
    "Step 1: filter covers top-left 2x2<br>\n",
    "Step 2: move right by 2 pixels → next 2x2 patch (skips 1 column)\n",
    "...\n",
    "\n",
    "* Each move defines where the dot product is computed and thus which position in the feature map gets filled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c12e7",
   "metadata": {
    "papermill": {
     "duration": 0.003799,
     "end_time": "2025-12-29T05:05:27.529247",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.525448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Padding \n",
    "\n",
    "Padding is a technique used to add extra pixels around the border of an image before applying a convolution. It controls the spatial size of the output feature map and affects how edges are treated.\n",
    "\n",
    "\n",
    "### Why Padding is Used\n",
    "\n",
    "1. **Control output size**  \n",
    "   - Without padding, feature maps shrink after each convolution.  \n",
    "   - Padding can preserve the original spatial dimensions.\n",
    "\n",
    "2. **Preserve edge information**  \n",
    "   - Without padding, pixels at the edge are included in fewer patches, so their influence is reduced.  \n",
    "   - Padding ensures all pixels are treated equally.\n",
    "\n",
    "3. **Enable deeper networks**  \n",
    "   - When stacking many convolution layers, padding prevents feature maps from shrinking too quickly.\n",
    "\n",
    "\n",
    "### Common Types of Padding\n",
    "\n",
    "#### 1. Valid Padding (\"no padding\")\n",
    "- No extra pixels added.  \n",
    "- Output size shrinks:\n",
    "\n",
    "$$\n",
    "\\text{Output Height} = H - F + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Output Width} = W - F + 1\n",
    "$$\n",
    "\n",
    "Example: 5×5 image, 3×3 filter → Output: 3×3\n",
    "\n",
    "#### 2. Same Padding\n",
    "- Pads the image so that the output size equals the input size.  \n",
    "- Formula for padding size:\n",
    "\n",
    "$$\n",
    "\\text{Padding} = \\frac{F - 1}{2} \\quad (\\text{assuming stride = 1})\n",
    "$$\n",
    "\n",
    "- For odd-sized filters (e.g., 3×3), padding = 1  \n",
    "- 5×5 image with 3×3 filter and padding = 1 → Output = 5×5\n",
    "\n",
    "\n",
    "### How Padding Works\n",
    "\n",
    "Example: 5×5 image, 3×3 filter, **padding = 1**\n",
    "\n",
    "Original image:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 & 0 & 1 \\\\\n",
    "0 & 1 & 2 & 3 & 1 \\\\\n",
    "1 & 0 & 1 & 2 & 0 \\\\\n",
    "2 & 1 & 0 & 1 & 2 \\\\\n",
    "1 & 2 & 1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "After padding with zeros around the border:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 2 & 3 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 & 2 & 3 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 1 & 2 & 0 & 0 \\\\\n",
    "0 & 2 & 1 & 0 & 1 & 2 & 0 \\\\\n",
    "0 & 1 & 2 & 1 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Convolution is now applied to the padded image.  \n",
    "- Output feature map size can now equal the original input size.\n",
    "\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- Padding helps preserve spatial dimensions.  \n",
    "- Common in modern CNNs to maintain consistent feature map size across layers.  \n",
    "- Works together with stride to control output shape.  \n",
    "- Two main types: valid (no padding), same (padding to preserve size).\n",
    "\n",
    "### Output Size Formula with Padding\n",
    "\n",
    "If input height = H, filter size = F, padding = P, stride = S:\n",
    "\n",
    "$$\n",
    "\\text{Output Height} = \\frac{H - F + 2P}{S} + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Output Width} = \\frac{W - F + 2P}{S} + 1\n",
    "$$\n",
    "\n",
    "Example: H = 5, F = 3, P = 1, S = 1 → Output Height = $(5-3+2*1)/1 + 1$ = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4911b7",
   "metadata": {
    "papermill": {
     "duration": 0.003925,
     "end_time": "2025-12-29T05:05:27.537048",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.533123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Basic Components of Convolution\n",
    "\n",
    "- **Input Image**: $H \\times W$ (Height × Width)  \n",
    "- **Filter (Kernel)**: $k \\times k$ (Height × Width)  \n",
    "- **Stride ($S$)**: Number of pixels the filter moves at each step  \n",
    "- **Padding ($P$)**: Extra pixels (usually zeros) added around the input to control output size  \n",
    "\n",
    "\n",
    "### Output Size Formula\n",
    "\n",
    "For an input of size $N$ and a filter of size $K$:\n",
    "\n",
    "$$\n",
    "\\text{Output Size} = \\frac{N - K + 2P}{S} + 1\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $N$ = input size (height or width)  \n",
    "- $K$ = kernel size (height or width)  \n",
    "- $P$ = padding size  \n",
    "- $S$ = stride  \n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "1. **No padding, stride = 1**  \n",
    "Input: $5 \\times 5$, Filter: $3 \\times 3$, $P=0$, $S=1$  \n",
    "\n",
    "$$\n",
    "\\text{Output Size} = \\frac{5-3+2*0}{1} + 1 = 3\n",
    "$$  \n",
    "\n",
    "Feature map size: $3 \\times 3$\n",
    "\n",
    "2. **Same padding, stride = 1**  \n",
    "Input: $5 \\times 5$, Filter: $3 \\times 3$, $P=1$, $S=1$  \n",
    "\n",
    "$$\n",
    "\\text{Output Size} = \\frac{5-3+2*1}{1} + 1 = 5\n",
    "$$  \n",
    "\n",
    "Feature map size: $5 \\times 5$  \n",
    "\n",
    "Using padding preserves the spatial dimensions of the input.\n",
    "\n",
    "### Solve for Padding\n",
    "\n",
    "If you want same padding (output size $O = N$), plug in $O = N$:\n",
    "\n",
    "$$\n",
    "N = \\frac{N - F + 2P}{S} + 1\n",
    "$$\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$$\n",
    "N - 1 = \\frac{N - F + 2P}{S}\n",
    "$$\n",
    "\n",
    "$$\n",
    "2P = S(N-1) - N + F\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{P = \\frac{(S \\cdot (N-1) - N + F)}{2}}\n",
    "$$\n",
    "\n",
    "* This is the **general formula for padding**, valid for any stride $S$.\n",
    "* For stride $S=1$, it simplifies to:\n",
    "\n",
    "$$\n",
    "P = \\frac{F-1}{2}\n",
    "$$\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "1. **Odd-sized filters**: $F = 3,5,7,\\dots$ → $P$ is integer, simple.\n",
    "2. **Even-sized filters**: $F = 2,4,\\dots$ → $P$ may be fractional → split asymmetrically top/bottom or left/right.\n",
    "3. **Stride > 1**: Padding ensures the output feature map does not shrink too much.\n",
    "\n",
    "### Example\n",
    "\n",
    "* Input: $7$, Filter: $3$, Stride: $1$\n",
    "\n",
    "$$\n",
    "P = \\frac{(1 \\cdot (7-1) - 7 + 3)}{2} = \\frac{(6-7+3)}{2} = \\frac{2}{2} = 1\n",
    "$$\n",
    "\n",
    "* Input: $7$, Filter: $3$, Stride: $2$\n",
    "\n",
    "$$\n",
    "P = \\frac{(2 \\cdot (7-1) - 7 + 3)}{2} = \\frac{(12-7+3)}{2} = \\frac{8}{2} = 4\n",
    "$$\n",
    "\n",
    "\n",
    "* | Input N | Filter F | Stride S | Padding P | Output O |\n",
    "|---------|----------|----------|-----------|----------|\n",
    "| 7       | 3        | 1        | 1         | 7        |\n",
    "| 7       | 3        | 2        | 4         | 7        |\n",
    "| 5       | 5        | 1        | 2         | 5        |\n",
    "| 5       | 3        | 1        | 0         | 3        |\n",
    "\n",
    "This ensures the feature map has the desired size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31ca6f9",
   "metadata": {
    "papermill": {
     "duration": 0.003794,
     "end_time": "2025-12-29T05:05:27.544860",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.541066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How CNNs Represent an Image\n",
    "\n",
    "CNNs treat images not as long vectors, but as structured data with spatial meaning.\n",
    "This single design choice is why CNNs work and why fully connected networks fail at vision.\n",
    "\n",
    "Let’s break it down from first principles.\n",
    "\n",
    "\n",
    "### What an image really is (numerically)\n",
    "\n",
    "An image is not a picture to a computer. It is a grid of numbers.\n",
    "\n",
    "#### Grayscale image\n",
    "Each pixel → one intensity value (brightness)\n",
    "\n",
    "- 0 = black  \n",
    "- 255 = white (or 0–1 if normalized)\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "X \\in \\mathbb{R}^{H \\times W}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "28 × 28 → MNIST digit*\n",
    "\n",
    "That is 784 numbers arranged in a grid, not a list.\n",
    "\n",
    "<u>*MNIST Dataset</u>\n",
    "\n",
    "MNIST (Modified National Institute of Standards and Technology) is a benchmark dataset of handwritten digits used to introduce and evaluate computer vision models.\n",
    "\n",
    "It contains 70,000 grayscale images of digits from 0 to 9, where each image is a 28 × 28 pixel grid representing a single handwritten digit. Every image is paired with a label indicating the correct digit.\n",
    "\n",
    "MNIST is widely used because it is simple, standardized, and ideal for learning fundamental concepts such as image representation, convolution, and feature extraction in Convolutional Neural Networks (CNNs).\n",
    "\n",
    "#### RGB image\n",
    "Each pixel has 3 values:\n",
    "- Red\n",
    "- Green\n",
    "- Blue\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "X \\in \\mathbb{R}^{H \\times W \\times 3}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "224 × 224 × 3 → ImageNet image*\n",
    "\n",
    "Think of it as:\n",
    "- 3 stacked grayscale images\n",
    "- One per color channel\n",
    "\n",
    "<u>*ImageNet Images</u>\n",
    "\n",
    "ImageNet is a large-scale computer vision dataset designed for real-world image recognition tasks.\n",
    "\n",
    "An ImageNet image is typically a high-resolution RGB image represented as a tensor of shape:\n",
    "$$\n",
    "X \\in \\mathbb{R}^{H \\times W \\times 3}\n",
    "$$\n",
    "where the three channels correspond to Red, Green, and Blue color values.\n",
    "\n",
    "In practice, images are commonly resized to 224 × 224 × 3 before being fed into CNNs. ImageNet contains millions of such images across 1,000 object categories, including animals, vehicles, tools, and everyday objects.\n",
    "\n",
    "ImageNet is used to train and evaluate deep CNN architectures and serves as a standard benchmark for modern computer vision systems.\n",
    "\n",
    "\n",
    "### Why CNNs do NOT flatten images\n",
    "\n",
    "#### What flattening does\n",
    "\n",
    "Flattening is the process of converting a multi-dimensional tensor (like an image) into a 1-dimensional vector.\n",
    "\n",
    "Flattening converts:\n",
    "\n",
    "H × W × C  →  (H · W · C)\n",
    "\n",
    "Example:\n",
    "\n",
    "28 × 28 → 784\n",
    "\n",
    "This destroys spatial relationships.\n",
    "\n",
    "After flattening, the model no longer knows:\n",
    "- Which pixels were neighbors\n",
    "- Where edges or corners were\n",
    "- Whether patterns are local or global\n",
    "\n",
    "To the model:\n",
    "\n",
    "> pixel #1 and pixel #782 look equally unrelated\n",
    "\n",
    "That’s catastrophic for vision.\n",
    "\n",
    "#### CNN core assumption\n",
    "\n",
    "CNNs assume:\n",
    "\n",
    "> Important visual patterns are local and repeat across space\n",
    "\n",
    "Examples:\n",
    "- An edge is meaningful wherever it appears\n",
    "- A corner is still a corner at any location\n",
    "- A texture is defined by neighboring pixels\n",
    "\n",
    "This assumption only holds if:\n",
    "- Height, width, and channels are preserved\n",
    "\n",
    "Hence:\n",
    "\n",
    "No flattening at the beginning.\n",
    "\n",
    "### Tensor representation (how CNNs see images)\n",
    "\n",
    "CNNs represent images as 3D tensors:\n",
    "\n",
    "(height, width, channels)\n",
    "\n",
    "Example (*CIFAR-10):\n",
    "\n",
    "32 × 32 × 3\n",
    "\n",
    "This allows:\n",
    "- Convolutions across height & width\n",
    "- Filters to look at local pixel neighborhoods\n",
    "- Channels to encode different visual information\n",
    "\n",
    "<u>*CIFAR-10 Dataset</u>\n",
    "\n",
    "CIFAR-10 is a benchmark dataset for image classification consisting of 60,000 color images (32 × 32 × 3) across 10 classes such as airplane, car, cat, dog, and more.\n",
    "\n",
    "- **Training images:** 50,000  \n",
    "- **Test images:** 10,000  \n",
    "- **Image size:** 32 × 32 pixels  \n",
    "- **Channels:** 3 (RGB)  \n",
    "- **Classes:** 10\n",
    "\n",
    "CIFAR-10 is more complex than MNIST because images are colored, contain natural objects, and include variations in pose, lighting, and background. It is widely used to test CNN architectures and understand how models detect patterns like edges, textures, and shapes across all channels.\n",
    "\n",
    "### What convolution actually uses from this tensor\n",
    "\n",
    "A convolutional filter operates like this:\n",
    "\n",
    "- Looks at a small spatial window (e.g., 3×3)\n",
    "- Sees all channels at once\n",
    "\n",
    "Example filter shape for RGB input:\n",
    "\n",
    "3 × 3 × 3\n",
    "\n",
    "So for each spatial location, the filter:\n",
    "- Reads Red, Green, Blue together\n",
    "- Learns color-aware patterns (e.g., colored edges)\n",
    "\n",
    "This is impossible after flattening.\n",
    "\n",
    "### Concrete example (edge detection)\n",
    "\n",
    "Suppose this vertical edge exists:\n",
    "\n",
    "| dark | dark | bright |<br>\n",
    "| dark | dark | bright |<br>\n",
    "| dark | dark | bright |\n",
    "\n",
    "A CNN:\n",
    "- Detects it using local neighborhoods\n",
    "- Keeps its position\n",
    "- Passes this structure to deeper layers\n",
    "\n",
    "A fully connected network:\n",
    "- Sees 9 unrelated numbers\n",
    "- Has no idea they form a vertical line\n",
    "\n",
    "### When flattening DOES happen\n",
    "\n",
    "Flattening is delayed until:\n",
    "- The network has extracted high-level features\n",
    "- Spatial structure is no longer critical\n",
    "\n",
    "Typical flow:\n",
    "\n",
    "Image → Conv → Conv → Pool → Conv → Flatten → Dense → Output\n",
    "\n",
    "At that point:\n",
    "- Each neuron represents a meaningful concept\n",
    "- Position matters less than presence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d40a286",
   "metadata": {
    "papermill": {
     "duration": 0.003786,
     "end_time": "2025-12-29T05:05:27.552432",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.548646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CNN vs Fully Connected Networks\n",
    "\n",
    "| Aspect | Fully Connected NN | CNN |\n",
    "|------|-------------------|-----|\n",
    "| Input handling | Flattened | Spatially preserved |\n",
    "| Parameter count | Very large | Much smaller |\n",
    "| Translation awareness | No | Yes |\n",
    "| Scalability to images | Poor | Excellent |\n",
    "| Vision performance | Weak | State-of-the-art |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e712aa2",
   "metadata": {
    "papermill": {
     "duration": 0.003721,
     "end_time": "2025-12-29T05:05:27.560036",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.556315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Intuition from Human Vision\n",
    "\n",
    "CNNs loosely mimic the human visual system:\n",
    "- Early layers → detect edges\n",
    "- Middle layers → detect shapes\n",
    "- Deeper layers → recognize objects\n",
    "\n",
    "This hierarchical processing is a key reason for CNN success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841cf96",
   "metadata": {
    "papermill": {
     "duration": 0.003807,
     "end_time": "2025-12-29T05:05:27.567661",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.563854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Common Applications of CNNs\n",
    "\n",
    "CNNs are used in:\n",
    "- Image classification (ResNet, EfficientNet)\n",
    "- Object detection (YOLO, Faster R-CNN)\n",
    "- Face recognition\n",
    "- Medical image analysis\n",
    "- Autonomous vehicle perception\n",
    "\n",
    "If your data has spatial structure, CNNs are usually the right tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1a40e4",
   "metadata": {
    "papermill": {
     "duration": 0.003822,
     "end_time": "2025-12-29T05:05:27.575523",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.571701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A Critical Misconception\n",
    "\n",
    "❌ “CNNs understand images like humans.”\n",
    "\n",
    "Reality:\n",
    "- CNNs detect statistical patterns\n",
    "- They have no semantic understanding\n",
    "- They can fail badly on unfamiliar data\n",
    "\n",
    "CNNs are pattern extractors, not intelligent observers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce279f0",
   "metadata": {
    "papermill": {
     "duration": 0.003768,
     "end_time": "2025-12-29T05:05:27.583127",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.579359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Manual Convolution Example\n",
    "\n",
    "Input Image (5×5)\n",
    "\n",
    "$$\n",
    "I = \\begin{bmatrix}\n",
    "1 & 2 & 3 & 0 & 1 \\\\\n",
    "0 & 1 & 2 & 3 & 1 \\\\\n",
    "1 & 0 & 1 & 2 & 0 \\\\\n",
    "2 & 1 & 0 & 1 & 2 \\\\\n",
    "1 & 2 & 1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Filter (3×3)\n",
    "\n",
    "$$\n",
    "F = \\begin{bmatrix}\n",
    "1 & 0 & -1 \\\\\n",
    "1 & 0 & -1 \\\\\n",
    "1 & 0 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "**Step 1: Determine Output Size**\n",
    "\n",
    "Input: $5 \\times 5$, Filter: $3 \\times 3$, Stride = 1, Padding = 0\n",
    "\n",
    "$$\n",
    "\\text{Output Size} = (H - F + 1) \\times (W - F + 1) = (5-3+1) \\times (5-3+1) = 3 \\times 3\n",
    "$$\n",
    "\n",
    "\n",
    "**Step 2: Compute Dot Products for Each Patch**\n",
    "\n",
    "**Top-left patch:**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "0 & 1 & 2 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix} \n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -1 \\\\\n",
    "1 & 0 & -1 \\\\\n",
    "1 & 0 & -1\n",
    "\\end{bmatrix} = -4\n",
    "$$\n",
    "\n",
    "**Top-middle patch:**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & 3 & 0 \\\\\n",
    "1 & 2 & 3 \\\\\n",
    "0 & 1 & 2\n",
    "\\end{bmatrix} \\cdot F = -2\n",
    "$$\n",
    "\n",
    "**Top-right patch:**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "3 & 0 & 1 \\\\\n",
    "2 & 3 & 1 \\\\\n",
    "1 & 2 & 0\n",
    "\\end{bmatrix} \\cdot F = 4\n",
    "$$\n",
    "\n",
    "**Middle-left patch:**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 2 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "2 & 1 & 0\n",
    "\\end{bmatrix} \\cdot F = 0\n",
    "$$\n",
    "\n",
    "**Middle-middle patch:**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "0 & 1 & 2 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix} \\cdot F = -4\n",
    "$$\n",
    "\n",
    "**Middle-right patch:**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & 3 & 1 \\\\\n",
    "1 & 2 & 0 \\\\\n",
    "0 & 1 & 2\n",
    "\\end{bmatrix} \\cdot F = 0\n",
    "$$\n",
    "\n",
    "**Bottom-left patch:**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "2 & 1 & 0 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{bmatrix} \\cdot F = 2\n",
    "$$\n",
    "\n",
    "**Bottom-middle patch:**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 2 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "2 & 1 & 0\n",
    "\\end{bmatrix} \\cdot F = 0\n",
    "$$\n",
    "\n",
    "**Bottom-right patch:**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 0 \\\\\n",
    "0 & 1 & 2 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix} \\cdot F = -1\n",
    "$$\n",
    "\n",
    "**Step 3: Final 3×3 Feature Map**\n",
    "\n",
    "$$\n",
    "\\text{Feature Map} = \\begin{bmatrix}\n",
    "-4 & -2 & 4 \\\\\n",
    "0 & -4 & 0 \\\\\n",
    "2 & 0 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This demonstrates how the filter slides over the image, computes a dot product for each patch, and produces a feature map that highlights where the vertical edge pattern appears.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7d261a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T05:05:27.593264Z",
     "iopub.status.busy": "2025-12-29T05:05:27.592735Z",
     "iopub.status.idle": "2025-12-29T05:05:27.609668Z",
     "shell.execute_reply": "2025-12-29T05:05:27.608329Z"
    },
    "papermill": {
     "duration": 0.025013,
     "end_time": "2025-12-29T05:05:27.612027",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.587014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Map:\n",
      " [[-4. -2.  4.]\n",
      " [ 0. -4.  0.]\n",
      " [ 2.  0. -1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input image (5x5)\n",
    "image = np.array([\n",
    "    [1, 2, 3, 0, 1],\n",
    "    [0, 1, 2, 3, 1],\n",
    "    [1, 0, 1, 2, 0],\n",
    "    [2, 1, 0, 1, 2],\n",
    "    [1, 2, 1, 0, 1]\n",
    "])\n",
    "\n",
    "# Filter (3x3)\n",
    "filter = np.array([\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1]\n",
    "])\n",
    "\n",
    "# Output feature map size\n",
    "H_out = image.shape[0] - filter.shape[0] + 1\n",
    "W_out = image.shape[1] - filter.shape[1] + 1\n",
    "feature_map = np.zeros((H_out, W_out))\n",
    "\n",
    "# Perform convolution\n",
    "for i in range(H_out):\n",
    "    for j in range(W_out):\n",
    "        patch = image[i:i+filter.shape[0], j:j+filter.shape[1]]\n",
    "        feature_map[i, j] = np.sum(patch * filter)  # element-wise multiplication + sum (dot product)\n",
    "\n",
    "print(\"Feature Map:\\n\", feature_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9deed24c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T05:05:27.622453Z",
     "iopub.status.busy": "2025-12-29T05:05:27.621581Z",
     "iopub.status.idle": "2025-12-29T05:05:27.631301Z",
     "shell.execute_reply": "2025-12-29T05:05:27.630303Z"
    },
    "papermill": {
     "duration": 0.017064,
     "end_time": "2025-12-29T05:05:27.633251",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.616187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 2, 3, 0, 1, 0],\n",
       "       [0, 0, 1, 2, 3, 1, 0],\n",
       "       [0, 1, 0, 1, 2, 0, 0],\n",
       "       [0, 2, 1, 0, 1, 2, 0],\n",
       "       [0, 1, 2, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zero padding (adding zeros around the border of the input image before applying convolution.)\n",
    "padded_image = np.pad(image, pad_width=1, mode='constant', constant_values=0)\n",
    "padded_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a7dc75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T05:05:27.643263Z",
     "iopub.status.busy": "2025-12-29T05:05:27.642825Z",
     "iopub.status.idle": "2025-12-29T05:05:27.651702Z",
     "shell.execute_reply": "2025-12-29T05:05:27.650461Z"
    },
    "papermill": {
     "duration": 0.016431,
     "end_time": "2025-12-29T05:05:27.653839",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.637408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Image:\n",
      " [[0 0 0 0 0 0 0]\n",
      " [0 1 2 3 0 1 0]\n",
      " [0 0 1 2 3 1 0]\n",
      " [0 1 0 1 2 0 0]\n",
      " [0 2 1 0 1 2 0]\n",
      " [0 1 2 1 0 1 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "\n",
      "Feature Map after Convolution:\n",
      " [[-3. -4.  0.  3.  3.]\n",
      " [-3. -4. -2.  4.  5.]\n",
      " [-2.  0. -4.  0.  6.]\n",
      " [-3.  2.  0. -1.  3.]\n",
      " [-3.  2.  2. -2.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Output feature map size\n",
    "H_out = padded_image.shape[0] - filter.shape[0] + 1\n",
    "W_out = padded_image.shape[1] - filter.shape[1] + 1\n",
    "feature_map = np.zeros((H_out, W_out))\n",
    "\n",
    "# Perform convolution\n",
    "for i in range(H_out):\n",
    "    for j in range(W_out):\n",
    "        patch = padded_image[i:i+filter.shape[0], j:j+filter.shape[1]]\n",
    "        feature_map[i, j] = np.sum(patch * filter)  # dot product\n",
    "\n",
    "print(\"Padded Image:\\n\", padded_image)\n",
    "print(\"\\nFeature Map after Convolution:\\n\", feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce0758",
   "metadata": {
    "papermill": {
     "duration": 0.00419,
     "end_time": "2025-12-29T05:05:27.663179",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.658989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Key Takeaways from  Day 20\n",
    "\n",
    "- CNNs preserve spatial structure\n",
    "- Convolution extracts local patterns\n",
    "- Filters learn features automatically\n",
    "- Stride controls resolution\n",
    "- Padding preserves size and borders\n",
    "- CNNs scale efficiently for images\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62a7780",
   "metadata": {
    "papermill": {
     "duration": 0.004102,
     "end_time": "2025-12-29T05:05:27.671445",
     "exception": false,
     "start_time": "2025-12-29T05:05:27.667343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"text-align:center; font-size:18px;\">\n",
    "© 2025 Mostafizur Rahman\n",
    "</p>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.237472,
   "end_time": "2025-12-29T05:05:27.995641",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-29T05:05:23.758169",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
