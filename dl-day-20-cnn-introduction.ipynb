{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Day 20: CNN Introduction\n\nWelcome to Day 20!\n\nToday you'll learn:\n\n- What is CNN?\n- Understand why CNNs exist\n- Learn convolution operation step-by-step\n- Understand filters, stride, padding\n- Manually compute a convolution\n- Implement convolution using NumPy\n\nIf you found this notebook helpful, your **<b style=\"color:red;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n\n---","metadata":{}},{"cell_type":"markdown","source":"# What is a Convolutional Neural Network (CNN)?\n\nA Convolutional Neural Network (CNN) is a specialized type of neural network designed to work with grid-structured data, especially images.\n\nExamples of grid-structured data:\n- Images → 2D grid of pixels  \n- Videos → 3D grid (height × width × time)  \n- Spectrograms → time × frequency grid  \n\nCNNs are the standard architecture for computer vision because they preserve and exploit the spatial structure of data.","metadata":{}},{"cell_type":"markdown","source":"## Why Traditional Neural Networks Fail for Images\n\nConsider a grayscale image of size 28 × 28:\n\n- Total pixels:  \n  $$\n  28 \\times 28 = 784\n  $$\n\nIf we use a fully connected layer with just 1,000 neurons:\n\n$$\n784 \\times 1000 = 784{,}000 \\text{ parameters}\n$$\n\nProblems:\n1. **Too many parameters** → slow training, high memory usage  \n2. **No spatial awareness**  \n   - Neighboring pixels are treated the same as distant pixels  \n3. **No translation understanding**  \n   - The same object in a different position looks completely new  \n\nFully connected networks ignore how images are structured.","metadata":{}},{"cell_type":"markdown","source":"## Core Ideas Behind CNNs\n\nCNNs are built on three key assumptions about images. Let’s break down each one carefully.\n\n### 1. Locality\n\nLocality means most important visual information in an image is contained in small, local regions, rather than spread across the entire image.\n\n* **Local region / patch:** A small area of the image, e.g., a 3×3 or 5×5 block of pixels.\n* **Edge:** A boundary where the intensity of pixels changes sharply (e.g., where light meets dark).\n* **Corner:** A point where two edges meet.\n* **Texture:** Repeating patterns in a small region, like stripes or dots.\n\nPixels are meaningful relative to their neighbors, not the entire image. Detecting edges, corners, or textures locally helps the network understand small building blocks of the image.\n\n### 2. Parameter Sharing\n\nParameter sharing means the same set of weights (filter) is applied across multiple positions in the image.\n\n* **Filter (kernel):** A small matrix of numbers that slides over the image to detect specific patterns.\n* **Weights:** Numbers in the filter that the network learns during training.\n* **Feature map:** The result of applying a filter across the image, showing where the pattern occurs.\n\nInstead of learning a separate detector for every location in the image, CNNs learn one filter and reuse it everywhere.\n\n* This drastically reduces the number of parameters (learnable weights), making training more efficient.\n* It also allows the network to recognize the same pattern regardless of its position, a property called **translation invariance**.\n\n### 3. Spatial Hierarchy\n\nSpatial hierarchy means simple patterns combine to form complex structures in a layered manner.\n\n* **Layer:** A level in the neural network that transforms input into more abstract features.\n* **Edges → shapes → object parts → full objects:** This describes how visual features are learned progressively:\n\n  1. Early layers detect simple features (edges, corners)\n  2. Middle layers combine them into shapes or textures\n  3. Deep layers recognize complex objects\n\nCNNs automatically learn a hierarchy of features, building up from local patterns to high-level concepts. This is one of the main reasons CNNs work so well for images.\n\n\n **Summary of New Terms**\n\n| Term                   | Definition                                                    |\n| ---------------------- | ------------------------------------------------------------- |\n| Local region / patch   | Small area of the image (e.g., 3×3 pixels)                    |\n| Edge                   | Boundary of sharp pixel intensity change                      |\n| Corner                 | Intersection point of two edges                               |\n| Texture                | Repeating pattern in a local area                             |\n| Filter / Kernel        | Small learnable matrix applied across the image               |\n| Weights                | Learnable numbers in the filter that detect patterns          |\n| Feature map            | Output of a filter showing where a pattern occurs             |\n| Layer                  | Level in a neural network that transforms input into features |\n| Translation invariance | Ability to detect the same pattern regardless of its location |\n","metadata":{}},{"cell_type":"markdown","source":"## What Makes a Convolutional Neural Network (CNN)?\n\nA Convolutional Neural Network (CNN) is a neural network designed to work with images by understanding what patterns exist and where they appear.\n\nTo understand this, we must clearly define three new terms:\n- Convolution operation  \n- Filter (Kernel)  \n- Feature map  \n\nWe will explain each from scratch.\n\n## 1. Convolution Operation\n\nA convolution is a mathematical operation where a small matrix (filter) slides over an input (like an image) and computes a weighted sum at each position.\n\nInstead of looking at the entire image at once (like fully connected layers), convolution focuses on small local regions.\n\n- Imagine placing a small transparent grid on top of an image.\n- You slide it step by step.\n- At each position, you check how well the grid matches the image underneath.\n- This is different from full matrix multiplication (used in dense layers) because we only focus on small local patches instead of the whole image at once.\n\nThis is how CNNs scan images.\n\nExample\n\nInput image patch:\n\n$$\nX =\n\\begin{bmatrix}\n1 & 2 & 0 \\\\\n0 & 1 & 3 \\\\\n1 & 2 & 1\n\\end{bmatrix}\n$$\n\nFilter (kernel):\n$$\nK =\n\\begin{bmatrix}\n0 & 1 & 0 \\\\\n1 & -4 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}\n$$\n\n- Slide the filter across the image\n- At each position, compute:\n$$\n\\text{sum of } (X \\odot K) = \\sum_{i,j} X_{i,j} \\cdot K_{i,j}\n$$\n\nThe result (feature map) is  one number that tells how strongly the pattern exists at that location.\n\n## 2. Filter (Kernel)\n\nA filter (also called a kernel) is a small learnable matrix of numbers used to detect a specific visual pattern.\n\n- Its job: detect specific patterns (edges, textures, corners) in images.\n- Typical sizes: 3×3, 5×5, 7×7  \n- Learns during training  \n- The same filter is applied across the entire image  \n\nExample:  Filter (Edge Detector)\n\nVertical edge filter (Sobel):\n\n$$\n\\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-2 & 0 & 2 \\\\\n-1 & 0 & 1\n\\end{bmatrix}\n$$\n\n- This filter responds strongly to vertical edges.\n- When applied to an image:\n    - Strong response → vertical edge exists\n    - Weak response → no vertical edge\n\n**Why Filters Matter**\n- One filter → detects one type of pattern  \n- Multiple filters → detect multiple patterns (edges, corners, textures)\n- NN learns the best filter values during training, we don’t handcraft them.\n\n\n\n## 3. Feature Map\n\n**Definition:**  \nA **feature map** is the output produced after applying a filter across the image.\n\n- Each value in the feature map shows **how strongly the filter matched** at that position.\n- Feature maps preserve the **2D structure** of the image.\n\n### Example\n- Input image size: 28×28  \n- Filter size: 3×3  \n- Output feature map size: 26×26  \n\nHigh values in the feature map = strong presence of the pattern.\n\nThink of it as a **heatmap** of detected features.\n\n---\n\n## 4. Spatial Structure\n\n**Definition:**  \n**Spatial structure** means the **relative position of pixels** is preserved.\n\n- CNNs keep the height and width of images intact.\n- Unlike dense layers, CNNs do **not flatten images early**.\n\n### Why This Is Important\n- A pattern at the top of an image remains at the top of the feature map.\n- CNNs know both **what** the pattern is and **where** it appears.\n\n---\n\n## Putting Everything Together\n\n1. Start with an image.\n2. Slide a **filter** over small local regions.\n3. Perform **convolution** at each position.\n4. Produce a **feature map**.\n5. Repeat with multiple filters and layers.\n\nEarly layers detect **edges and textures**.  \nDeeper layers combine them into **shapes and objects**.\n\n---\n\n## One-Line Intuition\n\n> CNNs learn small pattern detectors (filters) that slide across images, creating feature maps that show what patterns exist and where they appear.\n","metadata":{}},{"cell_type":"markdown","source":"Perfect — let’s go **term by term**, explain everything, and include examples so a beginner fully grasps the idea.\n\n---\n\n## 1. Convolution Operation\n\n**Definition:**\nA **convolution** is a mathematical operation where a **small matrix (filter)** slides over an input (like an image) and computes a **weighted sum** at each position.\n\n* Think of it as **scanning a small window across the image** to detect patterns.\n* This is different from **full matrix multiplication** (used in dense layers) because we only focus on **small local patches** instead of the whole image at once.\n\n**Example:**\n\nSmall 3×3 image patch:\n\n[\nX =\n\\begin{bmatrix}\n1 & 2 & 0 \\\n0 & 1 & 3 \\\n1 & 2 & 1\n\\end{bmatrix}\n]\n\nFilter (kernel):\n\n[\nK =\n\\begin{bmatrix}\n0 & 1 & 0 \\\n1 & -4 & 1 \\\n0 & 1 & 0\n\\end{bmatrix}\n]\n\n* Slide the filter across the image\n* At each position, compute:\n  \n* Result → **feature map** highlighting edges, corners, or textures\n\n---\n\n## 2. Filter / Kernel\n\n**Definition:**\nA **filter (or kernel)** is a **small, learnable matrix of numbers** in CNNs.\n\n* Its job: detect **specific patterns** (edges, textures, corners) in images.\n* Typical sizes: 3×3, 5×5, 7×7\n\n**Example:**\n\n* Vertical edge filter (Sobel):\n\n[\n\\begin{bmatrix}\n-1 & 0 & 1 \\\n-2 & 0 & 2 \\\n-1 & 0 & 1\n\\end{bmatrix}\n]\n\n* When applied to an image:\n\n  * Strong response → vertical edge exists\n  * Weak response → no vertical edge\n\n**Key Point:**\nCNN **learns the best filter values** during training — we don’t handcraft them.\n\n---\n\n## 3. Feature Map\n\n**Definition:**\nA **feature map** is the output of applying a filter over an image.\n\n* Shows **where a particular pattern occurs** in the image.\n* Usually a smaller 2D grid than the original image (depending on padding and stride).\n\n**Example:**\n\n* Input image → 28×28\n* Filter → 3×3\n* Feature map → 26×26 (each value indicates **how strongly the pattern is present** at that location)\n\nThink of it as a **heatmap** highlighting the pattern the filter detects.\n\n---\n\n## 4. Spatial Structure\n\n**Definition:**\n**Spatial structure** means the **2D arrangement of pixels** in an image is preserved.\n\n* Unlike dense layers that flatten the image and destroy pixel positions, CNNs maintain **height × width** of the image through feature maps.\n* This allows the network to understand **where patterns occur**, not just **what patterns exist**.\n\n**Example:**\n\n* Horizontal edge at the top of an image → top of feature map lights up\n* Horizontal edge at the bottom → bottom of feature map lights up\n\n---\n\n## 5. Putting it all together\n\n**Step-by-step intuition:**\n\n1. Start with an image:\n\n```\n[ 0 0 0 ]\n[ 0 1 0 ]\n[ 0 0 0 ]\n```\n\n2. Apply a **filter** (like a vertical edge detector) → slide across all positions.\n\n3. Compute **dot products at each position** → produce **feature map**.\n\n4. Feature map shows **where the vertical edges appear**.\n\n* Early layers → simple patterns (edges, corners)\n* Middle layers → combine into shapes, textures\n* Deep layers → detect objects (faces, cars, digits)\n\n---\n\n✅ **Summary in Beginner Terms:**\n\n| Term              | Simple Definition                                        |\n| ----------------- | -------------------------------------------------------- |\n| Convolution       | Sliding a small filter over an image to detect patterns  |\n| Filter / Kernel   | Small learnable matrix that looks for a specific feature |\n| Feature Map       | Output showing where the filter’s pattern occurs         |\n| Spatial Structure | Keeping 2D pixel arrangement intact                      |\n\n---\n\nIf you want, I can **draw a beginner-friendly diagram showing filter sliding over an image and producing a feature map**, which usually makes these abstract ideas very clear.\n\nDo you want me to do that next?\n","metadata":{}},{"cell_type":"markdown","source":"## How CNNs Represent an Image\n\nAn image is represented as a tensor.\n\n- Grayscale image:\n  \\[\n  X \\in \\mathbb{R}^{H \\times W}\n  \\]\n\n- RGB image:\n  \\[\n  X \\in \\mathbb{R}^{H \\times W \\times 3}\n  \\]\n\nCNNs **do not flatten** images at the beginning.  \nThey keep height, width, and channels intact.\n\nThis is a critical design choice.\n\n---\n\n## Convolution Operation (High-Level View)\n\nCNNs use small matrices called **filters** (or kernels), for example:\n\n\\[\nK =\n\\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-2 & 0 & 2 \\\\\n-1 & 0 & 1\n\\end{bmatrix}\n\\]\n\nThis filter:\n- Slides across the image\n- Computes dot products\n- Produces a **feature map**\n\nMathematically:\n\n\\[\n(X * K)(i, j) = \\sum_{m} \\sum_{n} X(i+m, j+n) \\cdot K(m, n)\n\\]\n\nEach filter specializes in detecting a specific pattern (edges, textures, etc.).\n\n---\n\n## CNN vs Fully Connected Networks\n\n| Aspect | Fully Connected NN | CNN |\n|------|-------------------|-----|\n| Input handling | Flattened | Spatially preserved |\n| Parameter count | Very large | Much smaller |\n| Translation awareness | ❌ | ✅ |\n| Scalability to images | Poor | Excellent |\n| Vision performance | Weak | State-of-the-art |\n\n---\n\n## Intuition from Human Vision\n\nCNNs loosely mimic the human visual system:\n- Early layers → detect edges\n- Middle layers → detect shapes\n- Deeper layers → recognize objects\n\nThis hierarchical processing is a **key reason for CNN success**.\n\n---\n\n## Common Applications of CNNs\n\nCNNs are used in:\n- Image classification (ResNet, EfficientNet)\n- Object detection (YOLO, Faster R-CNN)\n- Face recognition\n- Medical image analysis\n- Autonomous vehicle perception\n\nIf your data has **spatial structure**, CNNs are usually the right tool.\n\n---\n\n## A Critical Misconception\n\n❌ *“CNNs understand images like humans.”*\n\nReality:\n- CNNs detect statistical patterns\n- They have no semantic understanding\n- They can fail badly on unfamiliar data\n\nCNNs are **pattern extractors**, not intelligent observers.\n\n---\n\n## Summary\n\n- CNNs are designed for **spatial data**\n- Convolution enables **efficient pattern detection**\n- Parameter sharing makes CNNs scalable\n- Hierarchical features enable strong visual performance\n\nThis explains **why CNNs exist**, not just how they work.\n","metadata":{}},{"cell_type":"markdown","source":"# Why Convolutional Neural Networks (CNNs)?\n\nTraditional neural networks:\n- Flatten images → lose spatial structure\n- Too many parameters\n- Poor scalability for images\n\nCNNs:\n- Preserve spatial relationships\n- Use local connectivity\n- Share parameters (filters)\n- Are translation invariant\n\n> CNNs learn patterns, not pixels.\n","metadata":{}},{"cell_type":"markdown","source":"# Convolution\n\nConvolution is a mathematical operation that:\n- Slides a small matrix (filter / kernel) over an input\n- Computes element-wise multiplication\n- Sums the result to produce a feature map\n\nThis allows the network to detect:\n- Edges\n- Corners\n- Textures\n- Shapes\n","metadata":{}},{"cell_type":"markdown","source":"# Basic Components\n\n- **Input Image**: $H \\times W$\n- **Filter (Kernel)**: $k \\times k$\n- **Stride**: how far the filter moves each step\n- **Padding**: zeros added around the input\n\n### Output Size Formula\n\n$$\n\\text{Output Size} = \\frac{N - K + 2P}{S} + 1\n$$\n\nWhere:\n- $N$ = input size\n- $K$ = kernel size\n- $P$ = padding\n- $S$ = stride\n","metadata":{}},{"cell_type":"markdown","source":"## Manual Convolution Example\n\n### Input Image (5×5)\n\n$$\n\\begin{bmatrix}\n1 & 2 & 3 & 0 & 1 \\\\\n0 & 1 & 2 & 3 & 1 \\\\\n1 & 0 & 1 & 2 & 0 \\\\\n2 & 1 & 0 & 1 & 2 \\\\\n1 & 2 & 1 & 0 & 1\n\\end{bmatrix}\n$$\n\n### Filter (3×3)\n\n$$\n\\begin{bmatrix}\n1 & 0 & -1 \\\\\n1 & 0 & -1 \\\\\n1 & 0 & -1\n\\end{bmatrix}\n$$\n","metadata":{}},{"cell_type":"markdown","source":"# One Convolution Step\n\nTake top-left $3 \\times 3$ region:\n\n$$\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 2 \\\\\n1 & 0 & 1\n\\end{bmatrix}\n$$\n\nMultiply element-wise with filter and sum:\n\n$$\n(1×1)+(2×0)+(3×-1) +\n(0×1)+(1×0)+(2×-1) +\n(1×1)+(0×0)+(1×-1)\n$$\n\n$$\n= 1 - 3 - 2 + 1 - 1 = -4\n$$\n\nThat value becomes **one pixel in the output feature map**.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nimage = np.array([\n    [1,2,3,0,1],\n    [0,1,2,3,1],\n    [1,0,1,2,0],\n    [2,1,0,1,2],\n    [1,2,1,0,1]\n])\n\nkernel = np.array([\n    [1,0,-1],\n    [1,0,-1],\n    [1,0,-1]\n])\n\noutput = np.zeros((3,3))\n\nfor i in range(3):\n    for j in range(3):\n        region = image[i:i+3, j:j+3]\n        output[i,j] = np.sum(region * kernel)\n\noutput\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T10:56:06.480382Z","iopub.execute_input":"2025-12-26T10:56:06.480819Z","iopub.status.idle":"2025-12-26T10:56:06.495698Z","shell.execute_reply.started":"2025-12-26T10:56:06.480788Z","shell.execute_reply":"2025-12-26T10:56:06.494750Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"array([[-4., -2.,  4.],\n       [ 0., -4.,  0.],\n       [ 2.,  0., -1.]])"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"## What Filters Learn\n\n- Vertical edge detector\n- Horizontal edge detector\n- Diagonal patterns\n- Texture patterns\n\nEarly CNN layers:\n> Detect simple patterns\n\nDeeper CNN layers:\n> Combine patterns into objects\n","metadata":{"execution":{"iopub.status.busy":"2025-12-26T10:56:14.527759Z","iopub.execute_input":"2025-12-26T10:56:14.528071Z","iopub.status.idle":"2025-12-26T10:56:14.534781Z","shell.execute_reply.started":"2025-12-26T10:56:14.528049Z","shell.execute_reply":"2025-12-26T10:56:14.533575Z"}}},{"cell_type":"markdown","source":"## Stride\n\nStride controls how much the filter moves each step.\n\n- Stride = 1 → detailed feature map\n- Stride = 2 → smaller feature map\n- Larger stride → more compression, less detail\n\nEffect:\n- Reduces spatial size\n- Increases receptive field\n","metadata":{}},{"cell_type":"markdown","source":"## Padding\n\nPadding adds zeros around the image.\n\n### Why Padding?\n- Prevents shrinking feature maps too fast\n- Preserves border information\n\nTypes:\n- **Valid**: no padding\n- **Same**: output size = input size\n","metadata":{}},{"cell_type":"code","source":"padded_image = np.pad(image, pad_width=1, mode='constant', constant_values=0)\npadded_image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T10:56:50.378056Z","iopub.execute_input":"2025-12-26T10:56:50.378374Z","iopub.status.idle":"2025-12-26T10:56:50.386928Z","shell.execute_reply.started":"2025-12-26T10:56:50.378351Z","shell.execute_reply":"2025-12-26T10:56:50.386021Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"array([[0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 2, 3, 0, 1, 0],\n       [0, 0, 1, 2, 3, 1, 0],\n       [0, 1, 0, 1, 2, 0, 0],\n       [0, 2, 1, 0, 1, 2, 0],\n       [0, 1, 2, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0]])"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## CNN vs Fully Connected Layers\n\n| Aspect | Fully Connected | CNN |\n|------|----------------|-----|\n| Parameters | Very high | Low (shared) |\n| Spatial awareness | ❌ | ✅ |\n| Translation invariant | ❌ | ✅ |\n| Image scalability | Poor | Excellent |\n","metadata":{"execution":{"iopub.status.busy":"2025-12-26T10:57:00.353170Z","iopub.execute_input":"2025-12-26T10:57:00.353627Z","iopub.status.idle":"2025-12-26T10:57:00.362044Z","shell.execute_reply.started":"2025-12-26T10:57:00.353586Z","shell.execute_reply":"2025-12-26T10:57:00.360492Z"}}},{"cell_type":"markdown","source":"# Key Takeaways from  Day 20\n\n- CNNs preserve spatial structure\n- Convolution extracts local patterns\n- Filters learn features automatically\n- Stride controls resolution\n- Padding preserves size and borders\n- CNNs scale efficiently for images\n\n---","metadata":{}},{"cell_type":"markdown","source":"<p style=\"text-align:center; font-size:18px;\">\n© 2025 Mostafizur Rahman\n</p>\n\n","metadata":{}}]}