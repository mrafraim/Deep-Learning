{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-22-cnn-in-pytorch?scriptVersionId=289814155\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"ebe08185","metadata":{"papermill":{"duration":0.005609,"end_time":"2026-01-03T10:38:09.918439","exception":false,"start_time":"2026-01-03T10:38:09.91283","status":"completed"},"tags":[]},"source":["# Day 22: CNN in PyTorch\n","\n","Welcome to Day 22!\n","\n","Today you'll learn:\n","- Load MNIST dataset using PyTorch\n","- Build a simple CNN architecture\n","- Understand tensor shapes at each layer\n","- Train and evaluate the CNN\n","\n","If you found this notebook helpful, your **<b style=\"color:red;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"bb9885a3","metadata":{"papermill":{"duration":0.004135,"end_time":"2026-01-03T10:38:09.926935","exception":false,"start_time":"2026-01-03T10:38:09.9228","status":"completed"},"tags":[]},"source":["# Import Necessary Libraries"]},{"cell_type":"code","execution_count":1,"id":"eaf02bb4","metadata":{"execution":{"iopub.execute_input":"2026-01-03T10:38:09.936693Z","iopub.status.busy":"2026-01-03T10:38:09.936449Z","iopub.status.idle":"2026-01-03T10:38:23.640276Z","shell.execute_reply":"2026-01-03T10:38:23.639655Z"},"papermill":{"duration":13.710867,"end_time":"2026-01-03T10:38:23.642019","exception":false,"start_time":"2026-01-03T10:38:09.931152","status":"completed"},"tags":[]},"outputs":[],"source":["import torch                   # Core PyTorch library: tensor operations + autograd engine\n","import torch.nn as nn          # Neural network modules (layers, loss functions, model base class)\n","import torch.optim as optim    # Optimization algorithms (SGD, Adam, RMSprop, etc.)\n","\n","from torchvision import datasets, transforms  \n","# datasets   → ready-made datasets (MNIST, CIFAR, ImageNet, etc.)\n","# transforms → data preprocessing & augmentation (ToTensor, Normalize, Resize, etc.)\n","\n","from torch.utils.data import DataLoader  \n","# DataLoader → efficient batching, shuffling, and parallel data loading during training\n","\n","import matplotlib.pyplot as plt  # Visualize images"]},{"cell_type":"markdown","id":"f4847cda","metadata":{"papermill":{"duration":0.00422,"end_time":"2026-01-03T10:38:23.650749","exception":false,"start_time":"2026-01-03T10:38:23.646529","status":"completed"},"tags":[]},"source":["# Decide Whether to Use GPU or CPU\n"]},{"cell_type":"code","execution_count":2,"id":"73170c85","metadata":{"execution":{"iopub.execute_input":"2026-01-03T10:38:23.66084Z","iopub.status.busy":"2026-01-03T10:38:23.660329Z","iopub.status.idle":"2026-01-03T10:38:23.746419Z","shell.execute_reply":"2026-01-03T10:38:23.745756Z"},"papermill":{"duration":0.092615,"end_time":"2026-01-03T10:38:23.747871","exception":false,"start_time":"2026-01-03T10:38:23.655256","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"]},{"cell_type":"markdown","id":"093c503e","metadata":{"papermill":{"duration":0.004412,"end_time":"2026-01-03T10:38:23.757501","exception":false,"start_time":"2026-01-03T10:38:23.753089","status":"completed"},"tags":[]},"source":["# MNIST Dataset\n","\n","MNIST stands for Modified National Institute of Standards and Technology dataset.\n","It was created from a larger collection of handwritten digits originally gathered by the U.S. National Institute of Standards and Technology (NIST).\n","\n","Each digit image was carefully cleaned, centered, and resized to 28 × 28 grayscale pixels, making the dataset simple, consistent, and easy to model. With 70,000 images split into 60,000 for training and 10,000 for testing, MNIST became the go-to dataset for teaching and validating image classification algorithms.\n","\n","Because of its simplicity and balance, MNIST is widely used as a first benchmark in machine learning helping researchers and practitioners verify that their models, pipelines, and training logic work correctly.\n","\n","- 70,000 grayscale images\n","- Digits: 0 – 9\n","- Image size: 28 × 28\n","- Single channel (not RGB)\n","\n","Split:\n","- Training: 60,000\n","- Test: 10,000"]},{"cell_type":"markdown","id":"7f64e317","metadata":{"papermill":{"duration":0.004213,"end_time":"2026-01-03T10:38:23.76595","exception":false,"start_time":"2026-01-03T10:38:23.761737","status":"completed"},"tags":[]},"source":["# Preprocessing Pipeline"]},{"cell_type":"code","execution_count":3,"id":"c59c607e","metadata":{"execution":{"iopub.execute_input":"2026-01-03T10:38:23.775473Z","iopub.status.busy":"2026-01-03T10:38:23.775232Z","iopub.status.idle":"2026-01-03T10:38:23.778914Z","shell.execute_reply":"2026-01-03T10:38:23.778251Z"},"papermill":{"duration":0.010138,"end_time":"2026-01-03T10:38:23.780268","exception":false,"start_time":"2026-01-03T10:38:23.77013","status":"completed"},"tags":[]},"outputs":[],"source":["transform = transforms.Compose([                \n","    transforms.ToTensor(),                      # Converts image to tensor, scale [0,1]\n","    transforms.Normalize((0.1307,), (0.3081,))  # Standard MNIST normalization\n","])"]},{"cell_type":"markdown","id":"70ea8a6c","metadata":{"papermill":{"duration":0.004258,"end_time":"2026-01-03T10:38:23.788926","exception":false,"start_time":"2026-01-03T10:38:23.784668","status":"completed"},"tags":[]},"source":["`transforms.Compose([...])`\n","\n","* **Purpose:** Chains multiple image transformations together.\n","* **Why:** You often want to do several preprocessing steps (convert to tensor → normalize → resize, etc.) in one pipeline.\n","* **Flow:** Each image passes through each step in order.\n","\n","\n","`transforms.ToTensor()`\n","\n","* Converts an image from *PIL format or NumPy array → PyTorch tensor\n","* Changes shape: `(H, W)` → `(C, H, W)` (PyTorch expects channel-first)\n","* Scales pixel values from `[0, 255]` → `[0.0, 1.0]` automatically\n","\n","Example\n","\n","```\n","Original pixel: 128  → ToTensor() → 128 / 255 ≈ 0.502\n","```\n","\n","`transforms.Normalize(mean, std)`\n","\n","* Formula:\n","\n","$$\n","x_{\\text{normalized}} = \\frac{x - \\text{mean}}{\\text{std}}\n","$$\n","\n","* For MNIST: `mean = 0.1307`, `std = 0.3081`\n","* **Purpose:**\n","\n","  1. Centers pixel values around 0\n","  2. Scales variance to ~1\n","* **Why it matters:**\n","\n","  * Faster training convergence\n","  * More stable gradients\n","  * Standard practice in deep learning\n","\n","**Example:**\n","\n","* Pixel after `ToTensor()`: 0.5\n","* Normalized:\n","  $$\n","  (0.5 - 0.1307) / 0.3081 ≈ 1.21\n","  $$\n","  \n"]},{"cell_type":"markdown","id":"b9de52b1","metadata":{"papermill":{"duration":0.00413,"end_time":"2026-01-03T10:38:23.797279","exception":false,"start_time":"2026-01-03T10:38:23.793149","status":"completed"},"tags":[]},"source":["```\n","MNIST image file\n","      ↓\n","   PIL Image (image object)       ← \"just a picture\"\n","      ↓ ToTensor()\n","   Tensor                   ← \"numbers the model can learn from\"\n","      ↓ Normalize()\n","   Ready for CNN\n","```"]},{"cell_type":"markdown","id":"7705dea7","metadata":{"papermill":{"duration":0.00409,"end_time":"2026-01-03T10:38:23.805628","exception":false,"start_time":"2026-01-03T10:38:23.801538","status":"completed"},"tags":[]},"source":["*PIL = Python’s way of holding an image before math happens\n","\n","Just like:\n","- Text file → string\n","- CSV file → table\n","- Image file → PIL Image"]},{"cell_type":"markdown","id":"5dce317a","metadata":{"papermill":{"duration":0.004205,"end_time":"2026-01-03T10:38:23.814028","exception":false,"start_time":"2026-01-03T10:38:23.809823","status":"completed"},"tags":[]},"source":["# Load MNIST Dataset with Preprocessing\n"]},{"cell_type":"code","execution_count":4,"id":"fb260fbb","metadata":{"execution":{"iopub.execute_input":"2026-01-03T10:38:23.823564Z","iopub.status.busy":"2026-01-03T10:38:23.823148Z","iopub.status.idle":"2026-01-03T10:38:25.74927Z","shell.execute_reply":"2026-01-03T10:38:25.748455Z"},"papermill":{"duration":1.93263,"end_time":"2026-01-03T10:38:25.750799","exception":false,"start_time":"2026-01-03T10:38:23.818169","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 37.7MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 987kB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 9.26MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 11.0MB/s]\n"]}],"source":["train_dataset = datasets.MNIST(\n","    root=\"./data\",\n","    train=True,\n","    download=True,\n","    transform=transform\n",")\n","\n","test_dataset = datasets.MNIST(\n","    root=\"./data\",\n","    train=False,\n","    download=True,\n","    transform=transform\n",")\n"]},{"cell_type":"markdown","id":"08146abb","metadata":{"papermill":{"duration":0.00485,"end_time":"2026-01-03T10:38:25.760848","exception":false,"start_time":"2026-01-03T10:38:25.755998","status":"completed"},"tags":[]},"source":["# Wrap Datasets in DataLoader for Batching & Shuffling"]},{"cell_type":"code","execution_count":5,"id":"9f7ceb87","metadata":{"execution":{"iopub.execute_input":"2026-01-03T10:38:25.771332Z","iopub.status.busy":"2026-01-03T10:38:25.771105Z","iopub.status.idle":"2026-01-03T10:38:25.775062Z","shell.execute_reply":"2026-01-03T10:38:25.774384Z"},"papermill":{"duration":0.01085,"end_time":"2026-01-03T10:38:25.776444","exception":false,"start_time":"2026-01-03T10:38:25.765594","status":"completed"},"tags":[]},"outputs":[],"source":["train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=64,\n","    shuffle=True)\n","\n","test_loader = DataLoader(\n","    test_dataset, \n","    batch_size=64, \n","    shuffle=False)\n"]},{"cell_type":"markdown","id":"f98f7820","metadata":{"papermill":{"duration":0.004742,"end_time":"2026-01-03T10:38:25.786079","exception":false,"start_time":"2026-01-03T10:38:25.781337","status":"completed"},"tags":[]},"source":["# Let’s See What the Images Look Like\n"]},{"cell_type":"code","execution_count":6,"id":"2e10a8ae","metadata":{"execution":{"iopub.execute_input":"2026-01-03T10:38:25.796741Z","iopub.status.busy":"2026-01-03T10:38:25.796355Z","iopub.status.idle":"2026-01-03T10:38:26.143729Z","shell.execute_reply":"2026-01-03T10:38:26.143039Z"},"papermill":{"duration":0.354358,"end_time":"2026-01-03T10:38:26.145152","exception":false,"start_time":"2026-01-03T10:38:25.790794","status":"completed"},"tags":[]},"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 800x400 with 8 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["images, labels = next(iter(train_loader))\n","\n","plt.figure(figsize=(8,4))\n","for i in range(8):\n","    plt.subplot(2,4,i+1)\n","    plt.imshow(images[i].squeeze(), cmap=\"gray\")\n","    plt.title(f\"Label: {labels[i].item()}\")\n","    plt.axis(\"off\")\n","plt.show()\n"]},{"cell_type":"markdown","id":"d48c8e98","metadata":{"papermill":{"duration":0.005301,"end_time":"2026-01-03T10:38:26.155682","exception":false,"start_time":"2026-01-03T10:38:26.150381","status":"completed"},"tags":[]},"source":["# Check Image Shape\n","CNN expects images as (batch_size, channels, height, width)"]},{"cell_type":"code","execution_count":7,"id":"d3bc2eb9","metadata":{"execution":{"iopub.execute_input":"2026-01-03T10:38:26.166829Z","iopub.status.busy":"2026-01-03T10:38:26.166393Z","iopub.status.idle":"2026-01-03T10:38:26.170467Z","shell.execute_reply":"2026-01-03T10:38:26.169936Z"},"papermill":{"duration":0.011015,"end_time":"2026-01-03T10:38:26.171706","exception":false,"start_time":"2026-01-03T10:38:26.160691","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([64, 1, 28, 28])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["images.shape\n"]},{"cell_type":"markdown","id":"773130bd","metadata":{"papermill":{"duration":0.004916,"end_time":"2026-01-03T10:38:26.181619","exception":false,"start_time":"2026-01-03T10:38:26.176703","status":"completed"},"tags":[]},"source":["# CNN Architecture\n","\n","1. **Convolution:** Extract features (edges, shapes)  \n","2. **ReLU:** Add non-linearity  \n","3. **MaxPooling:** Reduce spatial dimensions  \n","4. **Flatten:** Convert to vector  \n","5. **Fully Connected Layer:** Predict class\n"]},{"cell_type":"markdown","id":"00dc9a64","metadata":{"papermill":{"duration":0.004837,"end_time":"2026-01-03T10:38:26.191496","exception":false,"start_time":"2026-01-03T10:38:26.186659","status":"completed"},"tags":[]},"source":["```\n","Input Image (e.g., 28×28 MNIST)\n","        │\n","        ▼\n","Convolution Layer\n","- Applies learnable filters\n","- Extracts local features (edges, curves, textures)\n","        │\n","        ▼\n","ReLU Activation\n","- Introduces non-linearity\n","- Keeps positive signals, suppresses noise\n","        │\n","        ▼\n","Max Pooling\n","- Downsamples feature maps\n","- Retains most important activations\n","        │\n","        ▼\n","Flatten\n","- Converts 2D feature maps into 1D vector\n","        │\n","        ▼\n","Fully Connected Layer\n","- Combines learned features\n","- Outputs class probabilities\n"]},{"cell_type":"markdown","id":"a29b18ff","metadata":{"papermill":{"duration":0.00485,"end_time":"2026-01-03T10:38:26.201384","exception":false,"start_time":"2026-01-03T10:38:26.196534","status":"completed"},"tags":[]},"source":["# Define CNN"]},{"cell_type":"code","execution_count":8,"id":"5d652977","metadata":{"execution":{"iopub.execute_input":"2026-01-03T10:38:26.212259Z","iopub.status.busy":"2026-01-03T10:38:26.212036Z","iopub.status.idle":"2026-01-03T10:38:26.217418Z","shell.execute_reply":"2026-01-03T10:38:26.21684Z"},"papermill":{"duration":0.012313,"end_time":"2026-01-03T10:38:26.218713","exception":false,"start_time":"2026-01-03T10:38:26.2064","status":"completed"},"tags":[]},"outputs":[],"source":["class SimpleCNN(nn.Module):\n","    def __init__(self):                    # Initialization: setting up the building blocks\n","        super().__init__()\n","\n","        # Convolutional layers\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n","\n","        # Activation function \n","        self.relu = nn.ReLU()\n","        \n","        # Pooling\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(32 * 7 * 7, 128)  # (in_features, out_features)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):                 # Forward Pass: defines how the input flows through the network\n","        x = self.relu(self.conv1(x))  # (N,16,28,28)\n","        x = self.pool(x)              # (N,16,14,14)\n","\n","        x = self.relu(self.conv2(x))  # (N,32,14,14)\n","        x = self.pool(x)              # (N,32,7,7)\n","\n","        x = x.view(x.size(0), -1)     # Flatten\n","        x = self.relu(self.fc1(x))\n","        x = self.fc2(x)               # output of shape (batch_size, 10)\n","\n","        return x\n"]},{"cell_type":"markdown","id":"16c4ef15","metadata":{"papermill":{"duration":0.004961,"end_time":"2026-01-03T10:38:26.228822","exception":false,"start_time":"2026-01-03T10:38:26.223861","status":"completed"},"tags":[]},"source":["## Initialization\n","\n","**Purpose:**\n","\n","This part declares and initializes all the layers your CNN will use.\n","\n","- Think of it as “setting up the building blocks”.\n","- At this point, no data is processed yet, it’s just defining the model’s structure:\n","- Convolutional layers → feature extraction\n","- Pooling → downsampling\n","- ReLU → non-linearity\n","- Fully connected layers → classification"]},{"cell_type":"markdown","id":"74a37c1e","metadata":{"papermill":{"duration":0.005142,"end_time":"2026-01-03T10:38:26.239086","exception":false,"start_time":"2026-01-03T10:38:26.233944","status":"completed"},"tags":[]},"source":["**1. Model Definition & Initialization**\n","\n","```python\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","````\n","* `class SimpleCNN(nn.Module)` defines a new class called SimpleCNN.\n","* `nn.Module` is the base class for all PyTorch models.\n","* `def __init__(self)` Constructor of the class. Runs once when you create an object. `self` → refers to this specific neuron object.\n","* `super().__init__()` initializes internal PyTorch machinery (parameters, gradients, etc.)."]},{"cell_type":"markdown","id":"2c888096","metadata":{"papermill":{"duration":0.004962,"end_time":"2026-01-03T10:38:26.249043","exception":false,"start_time":"2026-01-03T10:38:26.244081","status":"completed"},"tags":[]},"source":["**2. Convolution Layers (Feature Extraction)**\n","\n","```python\n","self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n","self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n","```\n","\n","* **`nn.Conv2d`**: Applies a 2D convolution over an image (or batch of images). Think of it as a filter sliding over the image, detecting features like edges or patterns.\n","* **Parameters explained**:\n","\n","  1. `in_channels`: Number of input channels. MNIST images are grayscale → `1`. If it were RGB → `3`.\n","  2. `out_channels`: Number of filters (feature maps) to learn.\n","\n","     * `conv1` outputs 16 feature maps.\n","     * `conv2` outputs 32 feature maps.\n","  3. `kernel_size=3`: Filter size 3×3.\n","  4. `padding=1`: Adds a 1-pixel border around the input so the output size remains the same after convolution (important to control dimensions).\n","\n","Why convolution?\n","\n","To detect local patterns like edges, curves, and shapes."]},{"cell_type":"markdown","id":"b149d520","metadata":{"papermill":{"duration":0.004911,"end_time":"2026-01-03T10:38:26.258931","exception":false,"start_time":"2026-01-03T10:38:26.25402","status":"completed"},"tags":[]},"source":["**3. Activation Function (ReLU)**\n","\n","```python\n","self.relu = nn.ReLU()\n","```\n","\n","* Applies:\n","  $$\n","  f(x) = \\max(0, x)\n","  $$\n","* Introduces non-linearity\n","* Prevents the network from collapsing into a linear model"]},{"cell_type":"markdown","id":"a568e701","metadata":{"papermill":{"duration":0.004953,"end_time":"2026-01-03T10:38:26.268863","exception":false,"start_time":"2026-01-03T10:38:26.26391","status":"completed"},"tags":[]},"source":["**4. Pooling Layer**\n","\n","```python\n","self.pool = nn.MaxPool2d(2, 2)\n","```\n","\n","* **`nn.MaxPool2d`**: Reduces spatial dimensions (height & width) by taking the maximum value in each window.\n","* **Parameters**:\n","\n","  * `2, 2` → window size 2×2, stride 2 → halves the dimensions.\n","* Helps reduce computation and introduces translation invariance (small shifts in the image don’t change the output much).\n","\n","Example:\n","\n","```\n","28×28 → 14×14 → 7×7\n","```\n","\n","Why pooling?\n","\n","* Reduces computation\n","* Makes features more robust to small shifts\n","* Keeps strongest activations\n"]},{"cell_type":"markdown","id":"dcf38ad4","metadata":{"papermill":{"duration":0.005006,"end_time":"2026-01-03T10:38:26.278855","exception":false,"start_time":"2026-01-03T10:38:26.273849","status":"completed"},"tags":[]},"source":["**5. Fully Connected Layers**\n","\n","```python\n","self.fc1 = nn.Linear(32 * 7 * 7, 128)\n","self.fc2 = nn.Linear(128, 10)\n","```\n","\n","* After convolutions and pooling, the feature maps are flattened into a 1D vector to feed into fully connected (dense) layers.\n","* **Dimensions**:\n","\n","  * Input image: 28×28\n","  * After `conv1` + ReLU → 28×28×16\n","  * After `pool` → 14×14×16\n","  * After `conv2` + ReLU → 14×14×32\n","  * After `pool` → 7×7×32\n","  * Flattened → 32×7×7 = 1568 features → fed into `fc1`\n","* `fc1` reduces 1568 → 128 features.\n","* `fc2` maps 128 → 10, which corresponds to 10 classes in MNIST (digits 0–9)."]},{"cell_type":"markdown","id":"a6e18825","metadata":{"papermill":{"duration":0.004947,"end_time":"2026-01-03T10:38:26.288858","exception":false,"start_time":"2026-01-03T10:38:26.283911","status":"completed"},"tags":[]},"source":["## Forward Pass\n","\n","**Purpose:**\n","\n","This part defines how the input flows through the network, the actual computation.\n","\n","- It applies the layers you defined in a specific order:\n","- Conv → ReLU → Pool\n","- Conv → ReLU → Pool\n","- Flatten → FC → ReLU → FC → Output\n","- Returns the logits (raw scores) for each class.\n","\n","**What `forward()` REALLY is**\n","\n","`forward()` defines how data flows through the network.\n","\n","Think of it as an assembly line:\n","raw pixels → feature extractor → compressor → decision maker.\n","\n","PyTorch will automatically:\n","\n","* call this during training/inference\n","* compute gradients through it (autograd)\n","\n","\n","**Input: `x`**\n","\n","For MNIST:\n","\n","```\n","x shape = (batch_size, 1, 28, 28)\n","```\n","\n","* `batch_size`: number of images processed together\n","* `1`: grayscale channel\n","* `28×28`: image size\n","\n","\n","**1. First Convolution + Activation**\n","\n","```python\n","x = self.relu(self.conv1(x))\n","```\n","\n","What happens\n","\n","* `conv1`: applies 16 different 3×3 filters\n","* Each filter learns something different (edges, curves, strokes)\n","* `ReLU`: removes negative values → keeps useful signal\n","\n","Shape change:\n","\n","```\n","( batch, 1, 28, 28 )\n","→ conv1 →\n","( batch, 16, 28, 28 )\n","→ ReLU →\n","( batch, 16, 28, 28 )\n","```\n","\n","Padding = 1 → spatial size stays 28×28\n","\n","\n","**2️. First Pooling**\n","\n","```python\n","x = self.pool(x)\n","```\n","\n","What pooling does\n","\n","* Takes the max value in every 2×2 block\n","* Reduces spatial size\n","* Keeps the strongest activations\n","\n","Shape change:\n","\n","```\n","( batch, 16, 28, 28 )\n","→ MaxPool(2×2) →\n","( batch, 16, 14, 14 )\n","```\n","\n","This is where the model throws away redundancy.\n","\n","\n","**3️. Second Convolution + Activation**\n","\n","```python\n","x = self.relu(self.conv2(x))\n","```\n","\n","What changes here\n","\n","* Input channels = 16\n","* Output channels = 32\n","* Model now learns higher-level features\n","\n","  * not edges anymore\n","  * combinations of edges → digit parts\n","\n","Shape change:\n","\n","```\n","( batch, 16, 14, 14 )\n","→ conv2 →\n","( batch, 32, 14, 14 )\n","→ ReLU →\n","( batch, 32, 14, 14 )\n","```\n","\n","**4️. Second Pooling**\n","\n","```python\n","x = self.pool(x)\n","```\n","\n","Shape change:\n","\n","```\n","( batch, 32, 14, 14 )\n","→ MaxPool →\n","( batch, 32, 7, 7 )\n","```\n","\n","Now each image is represented by 32 feature maps of size 7×7.\n","\n","This is the end of feature extraction.\n","\n","\n","**5️. Flattening**\n","\n","```python\n","x = x.view(x.size(0), -1)\n","```\n","\n","Why this is needed\n","\n","Fully connected layers expect 2D input:\n","\n","```\n","(batch_size, features)\n","```\n","\n","What this line does\n","\n","* `x.size(0)` → batch size (DON’T touch it)\n","* `-1` → “infer the rest automatically”\n","\n","Shape change:\n","\n","```\n","( batch, 32, 7, 7 )\n","→ flatten →\n","( batch, 32*7*7 )\n","→ ( batch, 1568 )\n","```\n","\n","This number must match `fc1` input size\n","That’s why you defined:\n","\n","```python\n","nn.Linear(32*7*7, 128)\n","```\n","\n","\n","**6️. First Fully Connected Layer**\n","\n","```python\n","x = self.relu(self.fc1(x))\n","```\n","\n","What happens\n","\n","* Compresses 1568 features → 128\n","* ReLU adds non-linearity\n","\n","Shape:\n","\n","```\n","( batch, 1568 )\n","→ fc1 →\n","( batch, 128 )\n","```\n","\n","This is where reasoning happens.\n","\n","\n","**7.Output Layer (No ReLU!)**\n","\n","```python\n","x = self.fc2(x)\n","```\n","\n","Why no ReLU here?\n","\n","Because:\n","\n","* This outputs raw scores (logits). Logits = output of the final fully connected layer before applying softmax or any probability function.\n","* Loss functions like `CrossEntropyLoss` expect logits\n","\n","Shape:\n","\n","```\n","( batch, 128 )\n","→ fc2 →\n","( batch, 10 )\n","```\n","\n","Each row:\n","\n","```\n","[ score_0, score_1, ..., score_9 ]\n","```\n","\n","The highest score = predicted digit.\n","\n","\n","**8️. Return Output**\n","\n","```python\n","return x\n","```\n","\n","This goes to:\n","\n","* loss function (during training)\n","* `argmax` (during inference)"]},{"cell_type":"markdown","id":"66a078d3","metadata":{"papermill":{"duration":0.004977,"end_time":"2026-01-03T10:38:26.298799","exception":false,"start_time":"2026-01-03T10:38:26.293822","status":"completed"},"tags":[]},"source":["# Model, Loss, Optimizer"]},{"cell_type":"code","execution_count":9,"id":"1af8e2ee","metadata":{"execution":{"iopub.execute_input":"2026-01-03T10:38:26.310201Z","iopub.status.busy":"2026-01-03T10:38:26.30955Z","iopub.status.idle":"2026-01-03T10:38:26.524967Z","shell.execute_reply":"2026-01-03T10:38:26.524048Z"},"papermill":{"duration":0.222956,"end_time":"2026-01-03T10:38:26.526733","exception":false,"start_time":"2026-01-03T10:38:26.303777","status":"completed"},"tags":[]},"outputs":[],"source":["model = SimpleCNN().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n"]},{"cell_type":"markdown","id":"6f6f330a","metadata":{"papermill":{"duration":0.005104,"end_time":"2026-01-03T10:38:26.537164","exception":false,"start_time":"2026-01-03T10:38:26.53206","status":"completed"},"tags":[]},"source":["## 1️. Cross-Entropy Loss\n","\n","Cross-Entropy Loss is used for multi-class classification. It measures how close the predicted probabilities are to the true labels.\n","\n","**Step 1: Softmax to get probabilities**\n","\n","For a network output (logits) $z = [z_0, z_1, ..., z_{C-1}]$ for $C$ classes:\n","\n","$$\n","\\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j=0}^{C-1} e^{z_j}}, \\quad i = 0, 1, ..., C-1\n","$$\n","\n","- $\\hat{y}_i$ = predicted probability for class $i$  \n","\n","\n","**Step 2: Cross-Entropy for one sample**\n","\n","If the true label is $y$:\n","\n","$$\n","\\text{Loss} = - \\log(\\hat{y}_y)\n","$$\n","\n","- Only the probability of the true class contributes  \n","- Penalizes incorrect predictions  \n","\n","**Step 3: Average over a batch of $N$ samples**\n","\n","$$\n","\\text{CrossEntropyLoss} = - \\frac{1}{N} \\sum_{n=1}^{N} \\log(\\hat{y}_{y^{(n)}})\n","$$\n","\n","- Smaller loss → better predictions  \n","- Encourages high probability for the correct class  \n","\n","\n","## 2️. Adam Optimizer\n","\n","Adam combines momentum + RMSProp for efficient gradient updates.\n","\n","**Step 1: Compute gradients**\n","\n","$$\n","g_t = \\nabla_\\theta L_t\n","$$\n","\n","- Gradient of the loss w.r.t. parameters $\\theta$ at step $t$  \n","\n","\n","**Step 2: Update biased first and second moments**\n","\n","$$\n","m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n","$$\n","\n","$$\n","v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n","$$\n","\n","- $m_t$ = momentum (moving average of gradients)  \n","- $v_t$ = adaptive scaling (moving average of squared gradients)  \n","\n","\n","**Step 3: Bias correction**\n","\n","$$\n","\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \n","\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n","$$\n","\n","- Corrects initialization bias at early steps  \n","\n","\n","**Step 4: Parameter update**\n","\n","$$\n","\\theta_t = \\theta_{t-1} - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n","$$\n","\n","- $\\eta$ = learning rate  \n","- $\\epsilon$ = small constant to avoid division by zero  \n","- Automatically scales updates per parameter for stable and fast convergence  \n","\n","\n","### Summary Table\n","\n","| Component | Formula | Role |\n","|-----------|--------|------|\n","| Cross-Entropy Loss | $L = -\\frac{1}{N} \\sum_{n=1}^{N} \\log \\hat{y}_{y^{(n)}}$ | Measures how well predictions match true labels |\n","| Adam Optimizer | $\\theta_t = \\theta_{t-1} - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$ | Updates weights efficiently using momentum + adaptive scaling |\n"]},{"cell_type":"markdown","id":"9007ad45","metadata":{"papermill":{"duration":0.005006,"end_time":"2026-01-03T10:38:26.547191","exception":false,"start_time":"2026-01-03T10:38:26.542185","status":"completed"},"tags":[]},"source":["# Training Loop"]},{"cell_type":"code","execution_count":10,"id":"f8fc592e","metadata":{"execution":{"iopub.execute_input":"2026-01-03T10:38:26.559071Z","iopub.status.busy":"2026-01-03T10:38:26.558306Z","iopub.status.idle":"2026-01-03T10:38:26.563517Z","shell.execute_reply":"2026-01-03T10:38:26.562853Z"},"papermill":{"duration":0.012759,"end_time":"2026-01-03T10:38:26.564943","exception":false,"start_time":"2026-01-03T10:38:26.552184","status":"completed"},"tags":[]},"outputs":[],"source":["def train(model, loader):\n","    model.train()                          # Set model to training mode\n","    total_loss = 0                         # Initialize cumulative loss for the epoch\n","\n","    # Loop over each batch of images and labels in the DataLoader\n","    for images, labels in loader:     \n","        images, labels = images.to(device), labels.to(device) # Move data to the appropriate device (CPU or GPU)\n","\n","        optimizer.zero_grad()              # Clear old gradients\n","        outputs = model(images)            # Forward pass: compute predictions from the model\n","        loss = criterion(outputs, labels)  # Compute the loss between predictions and true labels\n","        loss.backward()                    # Backward pass: compute gradients of loss w.r.t. model parameters\n","        optimizer.step()                   # Update model parameters using gradients (Adam)\n","\n","        total_loss += loss.item()          # Accumulate the batch loss (convert tensor to scalar)\n","\n","    return total_loss / len(loader)        # Return average loss over all batches in the epoch\n"]},{"cell_type":"markdown","id":"6b6f2d04","metadata":{"papermill":{"duration":0.004964,"end_time":"2026-01-03T10:38:26.575026","exception":false,"start_time":"2026-01-03T10:38:26.570062","status":"completed"},"tags":[]},"source":["# Evaluation Loop"]},{"cell_type":"code","execution_count":11,"id":"126cc791","metadata":{"execution":{"iopub.execute_input":"2026-01-03T10:38:26.587139Z","iopub.status.busy":"2026-01-03T10:38:26.586434Z","iopub.status.idle":"2026-01-03T10:38:26.591102Z","shell.execute_reply":"2026-01-03T10:38:26.590525Z"},"papermill":{"duration":0.011895,"end_time":"2026-01-03T10:38:26.592374","exception":false,"start_time":"2026-01-03T10:38:26.580479","status":"completed"},"tags":[]},"outputs":[],"source":["def evaluate(model, loader):\n","    model.eval()                                             # Set model to evaluation mode\n","    correct = 0                                              # Counters for correct predictions and total samples\n","    total = 0\n","\n","    # No gradients needed during evaluation → saves memory & computation\n","    with torch.no_grad():\n","        for images, labels in loader:                        # Loop over each batch in the DataLoader\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)                           # Forward pass: compute predictions from the model\n","            predictions = outputs.argmax(dim=1)               # Get predicted class by taking the index of the highest logit\n","\n","            correct += (predictions == labels).sum().item()   # Count correct predictions in this batch\n","            total += labels.size(0)                           # Count total samples processed\n","\n","    return correct / total                                    # Compute overall accuracy\n"]},{"cell_type":"markdown","id":"7a688fc6","metadata":{"papermill":{"duration":0.004968,"end_time":"2026-01-03T10:38:26.602436","exception":false,"start_time":"2026-01-03T10:38:26.597468","status":"completed"},"tags":[]},"source":["# Train the CNN"]},{"cell_type":"code","execution_count":12,"id":"04550aa7","metadata":{"execution":{"iopub.execute_input":"2026-01-03T10:38:26.613848Z","iopub.status.busy":"2026-01-03T10:38:26.6134Z","iopub.status.idle":"2026-01-03T10:39:41.90239Z","shell.execute_reply":"2026-01-03T10:39:41.901675Z"},"papermill":{"duration":75.301755,"end_time":"2026-01-03T10:39:41.909299","exception":false,"start_time":"2026-01-03T10:38:26.607544","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5 | Loss: 0.1551 | Accuracy: 0.9826\n","Epoch 2/5 | Loss: 0.0515 | Accuracy: 0.9868\n","Epoch 3/5 | Loss: 0.0351 | Accuracy: 0.9875\n","Epoch 4/5 | Loss: 0.0263 | Accuracy: 0.9883\n","Epoch 5/5 | Loss: 0.0201 | Accuracy: 0.9880\n"]}],"source":["epochs = 5\n","\n","for epoch in range(epochs):\n","    loss = train(model, train_loader)\n","    acc = evaluate(model, test_loader)\n","\n","    print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss:.4f} | Accuracy: {acc:.4f}\")\n"]},{"cell_type":"markdown","id":"37a23a29","metadata":{"papermill":{"duration":0.005289,"end_time":"2026-01-03T10:39:41.919865","exception":false,"start_time":"2026-01-03T10:39:41.914576","status":"completed"},"tags":[]},"source":["# What the CNN Learned\n","\n","- Conv1 → edges & simple patterns  \n","- Conv2 → digit-specific shapes  \n","- FC layers → combine features → classify\n"]},{"cell_type":"markdown","id":"a5c6c696","metadata":{"papermill":{"duration":0.00515,"end_time":"2026-01-03T10:39:41.930225","exception":false,"start_time":"2026-01-03T10:39:41.925075","status":"completed"},"tags":[]},"source":["# Key Takeaways from Day 22\n","\n","- CNNs work on patterns, not raw numbers  \n","- Input shape must be (N, C, H, W)  \n","- Convolutions extract features hierarchically  \n","- Pooling reduces spatial size  \n","- PyTorch simplifies training & evaluation\n","\n","---"]},{"cell_type":"markdown","id":"df7484ef","metadata":{"papermill":{"duration":0.005126,"end_time":"2026-01-03T10:39:41.940531","exception":false,"start_time":"2026-01-03T10:39:41.935405","status":"completed"},"tags":[]},"source":["<p style=\"text-align:center; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":98.824309,"end_time":"2026-01-03T10:39:45.161919","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-01-03T10:38:06.33761","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}