{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "046cfcc7",
   "metadata": {
    "papermill": {
     "duration": 0.004973,
     "end_time": "2026-01-15T12:24:03.253939",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.248966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Day 31: Regularization in CNN/RNN\n",
    "\n",
    "Welcome to day 31!\n",
    "\n",
    "Today you'll learn: \n",
    "\n",
    "1. Why regularization is necessary\n",
    "2. Dropout: intuition and behavior\n",
    "3. Batch Normalization: why it works\n",
    "4. Regularization in CNNs\n",
    "5. Regularization in RNNs (important differences)\n",
    "6. Practical do’s and don’ts\n",
    "\n",
    "\n",
    "If you found this notebook helpful, your **<b style=\"color:orange;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be6bd1f",
   "metadata": {
    "papermill": {
     "duration": 0.003865,
     "end_time": "2026-01-15T12:24:03.262270",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.258405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Why Regularization Exists\n",
    "\n",
    "Deep networks have:\n",
    "- Millions of parameters\n",
    "- High expressive power\n",
    "- Ability to memorize noise\n",
    "\n",
    "This leads to:\n",
    "- Low training loss\n",
    "- High validation loss\n",
    "\n",
    "→ **Overfitting**\n",
    "\n",
    "Regularization:\n",
    "> Intentionally restricts model freedom to improve generalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc579c4",
   "metadata": {
    "papermill": {
     "duration": 0.003593,
     "end_time": "2026-01-15T12:24:03.269599",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.266006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CNN Overfitting Pattern\n",
    "\n",
    "Typical CNN behavior without regularization:\n",
    "\n",
    "- Training loss ↓↓↓\n",
    "- Validation loss ↓ then ↑\n",
    "- Filters become too specialized\n",
    "- Model memorizes training images\n",
    "\n",
    "CNNs overfit because:\n",
    "- Many filters\n",
    "- Fully connected layers at the end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52611deb",
   "metadata": {
    "papermill": {
     "duration": 0.003899,
     "end_time": "2026-01-15T12:24:03.277187",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.273288",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dropout in CNN\n",
    "\n",
    "Dropout is a regularization technique designed to reduce overfitting by randomly disabling neurons during training.\n",
    "\n",
    "In CNNs, dropout must be used surgically. Blindly applying it hurts spatial learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377857c3",
   "metadata": {
    "papermill": {
     "duration": 0.004047,
     "end_time": "2026-01-15T12:24:03.285361",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.281314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1️. What Problem Dropout Solves\n",
    "\n",
    "Deep CNNs have high representational capacity.\n",
    "\n",
    "This creates a core risk:\n",
    "\n",
    "- Model memorizes training data\n",
    "- Learns fragile feature dependencies\n",
    "- Performs poorly on unseen data\n",
    "\n",
    "This phenomenon is called **overfitting**.\n",
    "\n",
    "Dropout directly attacks:\n",
    "\n",
    "❌ Feature co-adaptation  \n",
    "❌ Over-reliance on specific neurons  \n",
    "❌ Brittle internal representations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7742442",
   "metadata": {
    "papermill": {
     "duration": 0.003801,
     "end_time": "2026-01-15T12:24:03.293127",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.289326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2️. What Dropout Actually Does (Mathematics)\n",
    "\n",
    "Dropout randomly turns off neurons during training.\n",
    "\n",
    "For a neuron output $h$ during training:\n",
    "\n",
    "$$\n",
    "\\tilde{h} = h \\cdot r, \\quad r \\sim \\text{Bernoulli}(p)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $p$ = probability of keeping the neuron active\n",
    "- $r = 1$ → neuron active\n",
    "- $r = 0$ → neuron dropped\n",
    "\n",
    "Important:\n",
    "- Dropout is applied only during training\n",
    "- During inference, all neurons are active\n",
    "- Outputs are scaled automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae05957",
   "metadata": {
    "papermill": {
     "duration": 0.003614,
     "end_time": "2026-01-15T12:24:03.300573",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.296959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 1: What is `h`?\n",
    "\n",
    "`h` is the output of a neuron before dropout.\n",
    "\n",
    "Example:\n",
    "- Input → weights → activation function\n",
    "- Resulting number = `h`\n",
    "\n",
    "So if a neuron fires with value:\n",
    "\n",
    "$h = 6.0$\n",
    "\n",
    "That is its contribution to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f09f1c8",
   "metadata": {
    "papermill": {
     "duration": 0.00377,
     "end_time": "2026-01-15T12:24:03.308101",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.304331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 2: What is `r`?\n",
    "\n",
    "`r` is a random switch.\n",
    "\n",
    "It is drawn from a Bernoulli distribution:\n",
    "\n",
    "$$r ∼ Bernoulli(p)$$\n",
    "\n",
    "Meaning:\n",
    "- r = 1 with probability p (keep neuron)\n",
    "- r = 0 with probability 1 − p (drop neuron)\n",
    "\n",
    "This decision is made:\n",
    "- Independently\n",
    "- For every neuron\n",
    "- At every training step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9080f2a",
   "metadata": {
    "papermill": {
     "duration": 0.003809,
     "end_time": "2026-01-15T12:24:03.315749",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.311940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### What Is a Bernoulli Distribution?\n",
    "\n",
    "A Bernoulli distribution models an experiment with:\n",
    "\n",
    "- Exactly two possible outcomes\n",
    "- One is labeled “success”\n",
    "- One is labeled “failure”\n",
    "\n",
    "That’s it. No middle ground.\n",
    "\n",
    "\n",
    "**Mathematical Definition**\n",
    "\n",
    "A random variable $X$ follows a Bernoulli distribution if:\n",
    "\n",
    "$$\n",
    "X \\sim \\text{Bernoulli}(p)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $p$ = probability of success\n",
    "- $X = 1$ with probability $p$\n",
    "- $X = 0$ with probability $1 - p$\n",
    "\n",
    "So:\n",
    "\n",
    "| Outcome | Value | Probability |\n",
    "|------|------|-------------|\n",
    "| Success | 1 | $p$ |\n",
    "| Failure | 0 | $1 - p$ |\n",
    "\n",
    "\n",
    "**Example A: Coin Flip (Biased)**\n",
    "\n",
    "- Head = 1\n",
    "- Tail = 0\n",
    "- Probability of head = 0.7\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "X \\sim \\text{Bernoulli}(0.7)\n",
    "$$\n",
    "\n",
    "Each flip gives:\n",
    "- 1 (70% of the time)\n",
    "- 0 (30% of the time)\n",
    "\n",
    "\n",
    "**Example B: Light Switch**\n",
    "\n",
    "- ON = 1\n",
    "- OFF = 0\n",
    "\n",
    "No probability involved here unless randomness is added.\n",
    "Bernoulli adds randomness with control.\n",
    "\n",
    "### Bernoulli in Neural Networks\n",
    "\n",
    "In dropout:\n",
    "\n",
    "- Each neuron is treated like a light switch\n",
    "- ON = neuron kept\n",
    "- OFF = neuron dropped\n",
    "\n",
    "The switch is flipped randomly using Bernoulli.\n",
    "\n",
    "For each neuron:\n",
    "\n",
    "$$\n",
    "r \\sim \\text{Bernoulli}(p)\n",
    "$$\n",
    "\n",
    "Meaning:\n",
    "- r = 1 → neuron survives\n",
    "- r = 0 → neuron removed\n",
    "\n",
    "This happens:\n",
    "- For every neuron\n",
    "- For every training batch\n",
    "- Independently\n",
    "\n",
    "\n",
    "**Manual Dropout Example Using Bernoulli**\n",
    "\n",
    "Layer output:\n",
    "\n",
    "$h = [5, 3, 7, 1]$\n",
    "\n",
    "Keep probability:\n",
    "\n",
    "$p = 0.5$\n",
    "\n",
    "Bernoulli draws:\n",
    "\n",
    "$r = [1, 0, 1, 0]$\n",
    "\n",
    "Apply dropout:\n",
    "\n",
    "$$\n",
    "\\tilde{h} = h \\odot r\n",
    "$$\n",
    "\n",
    "Result:\n",
    "\n",
    "$$[5, 0, 7, 0]$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d546d",
   "metadata": {
    "papermill": {
     "duration": 0.003741,
     "end_time": "2026-01-15T12:24:03.323194",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.319453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 3: The Core Equation\n",
    "\n",
    "$$\n",
    "\\tilde{h} = h \\cdot r\n",
    "$$\n",
    "\n",
    "This is not fancy math.\n",
    "\n",
    "It literally means:\n",
    "\n",
    "- If r = 1 → output stays the same\n",
    "- If r = 0 → output becomes zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec84019b",
   "metadata": {
    "papermill": {
     "duration": 0.00403,
     "end_time": "2026-01-15T12:24:03.330941",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.326911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 4: Manual Single-Neuron Example\n",
    "\n",
    "Assume:\n",
    "- Neuron output: $h = 6$\n",
    "- Keep probability: $p = 0.5$\n",
    "\n",
    "Possible outcomes:\n",
    "\n",
    "| r | Calculation | Output |\n",
    "|---|------------|--------|\n",
    "| 1 | 6 × 1 | 6 |\n",
    "| 0 | 6 × 0 | 0 |\n",
    "\n",
    "So during training:\n",
    "- Sometimes the neuron exists\n",
    "- Sometimes it vanishes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c94e7",
   "metadata": {
    "papermill": {
     "duration": 0.003794,
     "end_time": "2026-01-15T12:24:03.338573",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.334779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 5: Manual Multi-Neuron Example (Critical)\n",
    "\n",
    "Assume a layer output:\n",
    "\n",
    "$h = [4, 2, 8, 6]$\n",
    "\n",
    "Let $p = 0.5$\n",
    "\n",
    "Random Bernoulli mask:\n",
    "\n",
    "$r = [1, 0, 1, 0]$\n",
    "\n",
    "Apply dropout:\n",
    "\n",
    "$$\n",
    "\\tilde{h} = h \\odot r\n",
    "$$\n",
    "\n",
    "Result:\n",
    "\n",
    "$$[4, 0, 8, 0]$$\n",
    "\n",
    "Half the neurons are removed for this step only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c79c5",
   "metadata": {
    "papermill": {
     "duration": 0.003671,
     "end_time": "2026-01-15T12:24:03.345979",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.342308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Why Scaling Is Required\n",
    "\n",
    "If we randomly drop neurons, the expected output magnitude decreases.\n",
    "\n",
    "Without correction:\n",
    "\n",
    "- Training sees smaller activations\n",
    "- Inference sees larger activations\n",
    "- Model breaks\n",
    "\n",
    "### Expected Value Explanation\n",
    "\n",
    "Original expected output:\n",
    "\n",
    "$$E[h] = h$$\n",
    "\n",
    "After dropout (no scaling):\n",
    "\n",
    "$$\n",
    "E[\\tilde{h}] = p \\cdot h + (1 - p) \\cdot 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[\\tilde{h}] = p \\cdot h\n",
    "$$\n",
    "\n",
    "So magnitude shrinks by factor p.\n",
    "\n",
    "**Concrete Numeric Example**\n",
    "\n",
    "Let:\n",
    "- h = 10\n",
    "- p = 0.5\n",
    "\n",
    "Expected value:\n",
    "\n",
    "E[\\tilde{h}] = 0.5 × 10 = 5\n",
    "\n",
    "Meaning:\n",
    "\n",
    "- During training, neuron contributes half as much on average\n",
    "\n",
    "\n",
    "**Why This Is a Problem**\n",
    "\n",
    "During training:\n",
    "- Network learns using average signal ≈ 5\n",
    "\n",
    "During inference (no dropout):\n",
    "- All neurons active\n",
    "- Output = 10\n",
    "\n",
    "Distribution mismatch:\n",
    "- Training sees small activations\n",
    "- Inference sees larger activations\n",
    "- Leads to unstable predictions\n",
    "\n",
    "### Inverted Dropout: The Fix (Used in PyTorch)\n",
    "\n",
    "Instead of scaling at inference, we scale during training.\n",
    "\n",
    "$$\n",
    "\\tilde{h} = \\frac{h \\cdot r}{p}\n",
    "$$\n",
    "\n",
    "\n",
    "**Expected Value With Inverted Dropout**\n",
    "\n",
    "Possible outcomes now:\n",
    "\n",
    "\n",
    "| r | Probability | Output |\n",
    "|--|--|--|\n",
    "| 1 | $p$ | $h / p$ |\n",
    "| 0 | $1 − p$ | 0 |\n",
    "\n",
    "**Expected Value Calculation**\n",
    "\n",
    "$$\n",
    "E[\\tilde{h}] = p \\cdot \\frac{h}{p} + (1 - p) \\cdot 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[\\tilde{h}] = h\n",
    "$$\n",
    "\n",
    "Matches original neuron output\n",
    "\n",
    "**Numeric Example**\n",
    "\n",
    "Let:\n",
    "- $h = 10$\n",
    "- $p = 0.5$\n",
    "\n",
    "Case 1: \n",
    "\n",
    "$r = 1$  \n",
    "$Output = 10 / 0.5 = 20$\n",
    "\n",
    "Case 2: \n",
    "\n",
    "$r = 0$  \n",
    "$Output = 0$  \n",
    "\n",
    "Expected value:\n",
    "\n",
    "$0.5 × 20 + 0.5 × 0 = 10$ \n",
    "\n",
    "\n",
    "### What “Outputs Are Scaled Automatically” Really Means\n",
    "\n",
    "When you write:\n",
    "\n",
    "`nn.Dropout(p=0.5)`\n",
    "\n",
    "PyTorch:\n",
    "- Applies Bernoulli mask\n",
    "- Divides by p during training\n",
    "- Does NOTHING during inference\n",
    "\n",
    "You never see the scaling, but it’s there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f772a1",
   "metadata": {
    "papermill": {
     "duration": 0.003624,
     "end_time": "2026-01-15T12:24:03.353284",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.349660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3️.  How Dropout Actually Helps\n",
    "\n",
    "Dropout exists to prevent co-adaptation between neurons.\n",
    "\n",
    "Co-adaptation happens when:\n",
    "- Neuron A becomes useful only because neuron B exists\n",
    "- Neuron B depends on neuron A to work correctly\n",
    "\n",
    "This creates fragile feature learning.\n",
    "\n",
    "If either neuron fails, the prediction collapses.\n",
    "\n",
    "\n",
    "### What Dropout Does During Training\n",
    "\n",
    "During every training step:\n",
    "\n",
    "- Random neurons are temporarily removed\n",
    "- The network structure changes every batch\n",
    "- Forward and backward passes use a different sub-network\n",
    "\n",
    "Example:\n",
    "\n",
    "- Batch 1: Neurons A, C active\n",
    "- Batch 2: Neurons B, D active\n",
    "- Batch 3: Neurons A, B active\n",
    "\n",
    "No neuron is guaranteed to be present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815d0a22",
   "metadata": {
    "papermill": {
     "duration": 0.003875,
     "end_time": "2026-01-15T12:24:03.361358",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.357483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Why This Is Equivalent to Training Many Sub-Networks\n",
    "\n",
    "Because neurons are randomly removed:\n",
    "- The model never trains as a single fixed architecture\n",
    "- It trains thousands of smaller networks\n",
    "- All networks share the same weights\n",
    "\n",
    "This behaves like an ensemble:\n",
    "- But without training separate models\n",
    "- And without extra memory cost\n",
    "\n",
    "### Why Neurons Become More Robust\n",
    "\n",
    "Since any neuron can disappear:\n",
    "- No neuron can rely on a specific partner\n",
    "- Each neuron must learn independently useful features\n",
    "\n",
    "Instead of learning:\n",
    "\n",
    "Feature = Neuron A AND Neuron B\n",
    "\n",
    "The network learns:\n",
    "\n",
    "Feature = Neuron A OR Neuron B OR Neuron C\n",
    "\n",
    "This creates redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda91d5",
   "metadata": {
    "papermill": {
     "duration": 0.004568,
     "end_time": "2026-01-15T12:24:03.369615",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.365047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### What This Achieves\n",
    "\n",
    "Dropout forces the model to learn:\n",
    "- Multiple ways to represent the same pattern\n",
    "- Backup features instead of brittle shortcuts\n",
    "\n",
    "Results:\n",
    "- Better generalization\n",
    "- Reduced overfitting\n",
    "- More stable performance on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26825448",
   "metadata": {
    "papermill": {
     "duration": 0.003782,
     "end_time": "2026-01-15T12:24:03.377323",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.373541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4️. Why Dropout Is Tricky in CNNs\n",
    "\n",
    "Dropout behaves very differently in CNNs compared to fully connected networks. This is not accidental, it comes from how CNNs represent information.\n",
    "\n",
    "### How CNNs Represent Information\n",
    "\n",
    "CNNs rely on three structural ideas:\n",
    "\n",
    "- **Local spatial correlations**  \n",
    "  Nearby pixels (or tokens) are strongly related.\n",
    "\n",
    "- **Shared convolutional filters**  \n",
    "  The same filter detects the same pattern everywhere.\n",
    "\n",
    "- **Structured feature maps**  \n",
    "  Activations form grids (height × width × channels), not flat vectors.\n",
    "\n",
    "This structure is the strength of CNNs.\n",
    "\n",
    "### What Dropout Does That Causes Trouble\n",
    "\n",
    "Dropout randomly removes individual activations.\n",
    "\n",
    "In early convolution layers, this means:\n",
    "- Random pixels in feature maps are erased\n",
    "- Local continuity is broken\n",
    "- Partial edges or textures disappear\n",
    "\n",
    "**Example (edge detection):**\n",
    "\n",
    "Original feature map:\n",
    "\n",
    "████████<br>\n",
    "████████<br>\n",
    "████████\n",
    "\n",
    "After dropout:\n",
    "\n",
    "███ ███<br>\n",
    "█ █████<br>\n",
    "████ ██\n",
    "\n",
    "Edges become fragmented.\n",
    "\n",
    "\n",
    "### Why This Hurts Early CNN Layers\n",
    "\n",
    "Early convolution layers learn:\n",
    "- Edges\n",
    "- Corners\n",
    "- Textures\n",
    "- Simple shapes\n",
    "\n",
    "These features require spatial consistency.\n",
    "\n",
    "Dropping random neurons early:\n",
    "- Destroys local patterns\n",
    "- Makes filters harder to learn\n",
    "- Slows convergence\n",
    "- Reduces representation quality\n",
    "\n",
    "In short:\n",
    "> Dropout fights against what early CNN layers are trying to learn.\n",
    "\n",
    "### Why Dropout Works Better in Later Layers\n",
    "\n",
    "Later CNN layers (especially fully connected layers):\n",
    "- Represent abstract concepts\n",
    "- No longer depend on precise spatial layout\n",
    "- Behave like standard dense networks\n",
    "\n",
    "Examples:\n",
    "- “Catness”\n",
    "- “Face-like structure”\n",
    "- “Positive sentiment”\n",
    "\n",
    "Here:\n",
    "- Co-adaptation becomes a real risk\n",
    "- Dropout helps prevent over-reliance on specific neurons\n",
    "\n",
    "\n",
    "### Practical Rule Used in Real Systems\n",
    "\n",
    "- Avoid dropout in early convolution layers\n",
    "- Dropout is usually NOT needed in convolutional layers and if you use it, use very little, very carefully.\n",
    "- Use dropout in:\n",
    "  - Fully connected layers\n",
    "  - Classification heads\n",
    "  - Dense decision layers\n",
    "\n",
    "\n",
    "### Industry Reality Check\n",
    "\n",
    "Modern CNN architectures often:\n",
    "- Use Batch Normalization instead of dropout\n",
    "- Use data augmentation for regularization\n",
    "- Apply dropout only near the output\n",
    "\n",
    "That’s why you rarely see heavy dropout in ResNet, EfficientNet, etc.\n",
    "\n",
    "\n",
    "\n",
    "> CNNs depend on spatial structure. Dropout destroys spatial structure.\n",
    "\n",
    "So:\n",
    "\n",
    "> Dropout is a poor regularizer for early CNN layers but a good regularizer for dense decision layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aad967",
   "metadata": {
    "papermill": {
     "duration": 0.003707,
     "end_time": "2026-01-15T12:24:03.385594",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.381887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5️. Where to Use Dropout in CNN\n",
    "\n",
    "- After Fully Connected (FC) layers\n",
    "- After Global Average Pooling\n",
    "- Late-stage convolution blocks (light dropout)\n",
    "\n",
    "Avoid\n",
    "- First conv layer\n",
    "- Aggressive dropout in early feature extraction\n",
    "\n",
    "Typical Values\n",
    "\n",
    "| Layer Type | Dropout Rate |\n",
    "|----------|-------------|\n",
    "| FC Layers | 0.3 – 0.5 |\n",
    "| Conv Blocks | 0.1 – 0.3 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b315a6d4",
   "metadata": {
    "papermill": {
     "duration": 0.0042,
     "end_time": "2026-01-15T12:24:03.394041",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.389841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Example: Dropout in a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2484cc86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:24:03.403767Z",
     "iopub.status.busy": "2026-01-15T12:24:03.403387Z",
     "iopub.status.idle": "2026-01-15T12:24:07.835247Z",
     "shell.execute_reply": "2026-01-15T12:24:07.834209Z"
    },
    "papermill": {
     "duration": 4.439603,
     "end_time": "2026-01-15T12:24:07.837559",
     "exception": false,
     "start_time": "2026-01-15T12:24:03.397956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional layer:\n",
    "        # Input: 1 channel (grayscale), Output: 16 channels, Kernel: 3x3\n",
    "        # No padding, so output spatial size = (28-3+1)=26\n",
    "        self.conv = nn.Conv2d(1, 16, 3)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        # Randomly zeroes 30% of neurons during training\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        # Fully connected layer:\n",
    "        # Input features = 16 channels * 26 * 26 pixels (flattened)\n",
    "        # Output features = 10 classes\n",
    "        self.fc = nn.Linear(16*26*26, 10)  # assuming input 28x28\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Apply ReLU activation function\n",
    "        # F.relu is functional (stateless) version\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Flatten 4D tensor (B, C, H, W) -> 2D tensor (B, features)\n",
    "        # Necessary for feeding into fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply dropout (only active during training)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Fully connected layer to produce logits for 10 classes\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc0890",
   "metadata": {
    "papermill": {
     "duration": 0.004307,
     "end_time": "2026-01-15T12:24:07.845996",
     "exception": false,
     "start_time": "2026-01-15T12:24:07.841689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Dropout:\n",
    "\n",
    "- `ON` during `model.train()`\n",
    "- `OFF`during `model.eval()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7902d504",
   "metadata": {
    "papermill": {
     "duration": 0.003684,
     "end_time": "2026-01-15T12:24:07.853516",
     "exception": false,
     "start_time": "2026-01-15T12:24:07.849832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Batch Normalization in CNN\n",
    "\n",
    "Batch Normalization (BatchNorm) is a technique to normalize the inputs of each layer in a neural network. It is widely used in CNNs to stabilize and accelerate training.\n",
    "\n",
    "For an input activation $x$ in a mini-batch:\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\mu$ = mean of the mini-batch  \n",
    "- $\\sigma^2$ = variance of the mini-batch  \n",
    "- $\\epsilon$ = small constant to avoid division by zero\n",
    "\n",
    "**Manual Example**\n",
    "\n",
    "Consider a mini-batch of 4 activations from a single neuron/channel:\n",
    "\n",
    "$$\n",
    "x = [2, 4, 6, 8]\n",
    "$$\n",
    "\n",
    "We'll apply Batch Normalization with a small $\\epsilon = 10^{-5}$, and assume learnable parameters:\n",
    "\n",
    "$$\n",
    "\\gamma = 2, \\quad \\beta = 1\n",
    "$$\n",
    "\n",
    "### Step 1: Compute Mini-Batch Mean\n",
    "\n",
    "The mean $\\mu$ is:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i\n",
    "$$\n",
    "\n",
    "Here, $N=4$:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{2 + 4 + 6 + 8}{4} = \\frac{20}{4} = 5\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3791b9",
   "metadata": {
    "papermill": {
     "duration": 0.003618,
     "end_time": "2026-01-15T12:24:07.860929",
     "exception": false,
     "start_time": "2026-01-15T12:24:07.857311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"text-align:center; font-size:18px;\">\n",
    "© 2026 Mostafizur Rahman\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e6ab6a",
   "metadata": {
    "papermill": {
     "duration": 0.003631,
     "end_time": "2026-01-15T12:24:07.868388",
     "exception": false,
     "start_time": "2026-01-15T12:24:07.864757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.379019,
   "end_time": "2026-01-15T12:24:10.360854",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-15T12:23:59.981835",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
