{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-31-regularization-in-cnn-rnn?scriptVersionId=293819189\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"2c3f37a7","metadata":{"papermill":{"duration":0.010859,"end_time":"2026-01-25T10:24:41.858112","exception":false,"start_time":"2026-01-25T10:24:41.847253","status":"completed"},"tags":[]},"source":["# Day 31: Regularization in CNN/RNN\n","\n","Welcome to day 31!\n","\n","Today you'll learn: \n","\n","1. Why regularization is necessary\n","2. Dropout: intuition and behavior\n","3. Batch Normalization: why it works\n","4. Regularization in CNNs\n","5. Regularization in RNNs (important differences)\n","6. Practical do’s and don’ts\n","\n","\n","If you found this notebook helpful, your **<b style=\"color:orange;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"b06be07d","metadata":{"papermill":{"duration":0.00939,"end_time":"2026-01-25T10:24:41.877093","exception":false,"start_time":"2026-01-25T10:24:41.867703","status":"completed"},"tags":[]},"source":["#  Why Regularization Exists\n","\n","Deep networks have:\n","- Millions of parameters\n","- High expressive power\n","- Ability to memorize noise\n","\n","This leads to:\n","- Low training loss\n","- High validation loss\n","\n","→ **Overfitting**\n","\n","Regularization:\n","> Intentionally restricts model freedom to improve generalization\n"]},{"cell_type":"markdown","id":"8395af40","metadata":{"papermill":{"duration":0.009812,"end_time":"2026-01-25T10:24:41.897087","exception":false,"start_time":"2026-01-25T10:24:41.887275","status":"completed"},"tags":[]},"source":["# PART A: Regularization in CNNs"]},{"cell_type":"markdown","id":"3062efa1","metadata":{"papermill":{"duration":0.009556,"end_time":"2026-01-25T10:24:41.917051","exception":false,"start_time":"2026-01-25T10:24:41.907495","status":"completed"},"tags":[]},"source":["# CNN Overfitting Pattern\n","\n","Typical CNN behavior without regularization:\n","\n","- Training loss ↓↓↓\n","- Validation loss ↓ then ↑\n","- Filters become too specialized\n","- Model memorizes training images\n","\n","CNNs overfit because:\n","- Many filters\n","- Fully connected layers at the end\n"]},{"cell_type":"markdown","id":"7c6e5eff","metadata":{"papermill":{"duration":0.010005,"end_time":"2026-01-25T10:24:41.93652","exception":false,"start_time":"2026-01-25T10:24:41.926515","status":"completed"},"tags":[]},"source":["# Dropout in CNN\n","\n","Dropout is a **regularization technique** designed to reduce overfitting by randomly disabling neurons during training.\n","\n","In CNNs, dropout must be used surgically. Blindly applying it hurts spatial learning.\n"]},{"cell_type":"markdown","id":"f59002c1","metadata":{"papermill":{"duration":0.009413,"end_time":"2026-01-25T10:24:41.955541","exception":false,"start_time":"2026-01-25T10:24:41.946128","status":"completed"},"tags":[]},"source":["## 1️. What Problem Dropout Solves\n","\n","Deep CNNs have high representational capacity.\n","\n","This creates a core risk:\n","\n","- Model memorizes training data\n","- Learns fragile feature dependencies\n","- Performs poorly on unseen data\n","\n","This phenomenon is called **overfitting**.\n","\n","Dropout directly attacks:\n","\n","❌ Feature co-adaptation  \n","❌ Over-reliance on specific neurons  \n","❌ Brittle internal representations\n"]},{"cell_type":"markdown","id":"c5bdccd4","metadata":{"papermill":{"duration":0.009344,"end_time":"2026-01-25T10:24:41.974381","exception":false,"start_time":"2026-01-25T10:24:41.965037","status":"completed"},"tags":[]},"source":["## 2️. What Dropout Actually Does (Mathematics)\n","\n","Dropout randomly turns off neurons during training.\n","\n","For a neuron output $h$ during training:\n","\n","$$\n","\\tilde{h} = h \\cdot r, \\quad r \\sim \\text{Bernoulli}(p)\n","$$\n","\n","Where:\n","- $p$ = probability of keeping the neuron active\n","- $r = 1$ → neuron active\n","- $r = 0$ → neuron dropped\n","\n","Important:\n","- Dropout is applied only during training\n","- During inference, all neurons are active\n","- Outputs are scaled automatically"]},{"cell_type":"markdown","id":"17f66dbc","metadata":{"papermill":{"duration":0.009422,"end_time":"2026-01-25T10:24:41.993238","exception":false,"start_time":"2026-01-25T10:24:41.983816","status":"completed"},"tags":[]},"source":["### Step 1: What is `h`?\n","\n","`h` is the output of a neuron before dropout.\n","\n","Example:\n","- Input → weights → activation function\n","- Resulting number = `h`\n","\n","So if a neuron fires with value:\n","\n","$h = 6.0$\n","\n","That is its contribution to the next layer."]},{"cell_type":"markdown","id":"ccd89a2f","metadata":{"papermill":{"duration":0.009539,"end_time":"2026-01-25T10:24:42.01223","exception":false,"start_time":"2026-01-25T10:24:42.002691","status":"completed"},"tags":[]},"source":["### Step 2: What is `r`?\n","\n","`r` is a random switch.\n","\n","It is drawn from a Bernoulli distribution:\n","\n","$$r ∼ Bernoulli(p)$$\n","\n","Meaning:\n","- r = 1 with probability p (keep neuron)\n","- r = 0 with probability 1 − p (drop neuron)\n","\n","This decision is made:\n","- Independently\n","- For every neuron\n","- At every training step\n"]},{"cell_type":"markdown","id":"e82372b9","metadata":{"papermill":{"duration":0.009526,"end_time":"2026-01-25T10:24:42.031199","exception":false,"start_time":"2026-01-25T10:24:42.021673","status":"completed"},"tags":[]},"source":["---\n","\n","### What Is a Bernoulli Distribution?\n","\n","A Bernoulli distribution models an experiment with:\n","\n","- Exactly two possible outcomes\n","- One is labeled “success”\n","- One is labeled “failure”\n","\n","That’s it. No middle ground.\n","\n","\n","**Mathematical Definition**\n","\n","A random variable $X$ follows a Bernoulli distribution if:\n","\n","$$\n","X \\sim \\text{Bernoulli}(p)\n","$$\n","\n","Where:\n","- $p$ = probability of success\n","- $X = 1$ with probability $p$\n","- $X = 0$ with probability $1 - p$\n","\n","So:\n","\n","| Outcome | Value | Probability |\n","|------|------|-------------|\n","| Success | 1 | $p$ |\n","| Failure | 0 | $1 - p$ |\n","\n","\n","**Example A: Coin Flip (Biased)**\n","\n","- Head = 1\n","- Tail = 0\n","- Probability of head = 0.7\n","\n","Then:\n","\n","$$\n","X \\sim \\text{Bernoulli}(0.7)\n","$$\n","\n","Each flip gives:\n","- 1 (70% of the time)\n","- 0 (30% of the time)\n","\n","\n","**Example B: Light Switch**\n","\n","- ON = 1\n","- OFF = 0\n","\n","No probability involved here unless randomness is added.\n","Bernoulli adds randomness with control.\n","\n","### Bernoulli in Neural Networks\n","\n","In dropout:\n","\n","- Each neuron is treated like a light switch\n","- ON = neuron kept\n","- OFF = neuron dropped\n","\n","The switch is flipped randomly using Bernoulli.\n","\n","For each neuron:\n","\n","$$\n","r \\sim \\text{Bernoulli}(p)\n","$$\n","\n","Meaning:\n","- r = 1 → neuron survives\n","- r = 0 → neuron removed\n","\n","This happens:\n","- For every neuron\n","- For every training batch\n","- Independently\n","\n","\n","**Manual Dropout Example Using Bernoulli**\n","\n","Layer output:\n","\n","$h = [5, 3, 7, 1]$\n","\n","Keep probability:\n","\n","$p = 0.5$\n","\n","Bernoulli draws:\n","\n","$r = [1, 0, 1, 0]$\n","\n","Apply dropout:\n","\n","$$\n","\\tilde{h} = h \\odot r\n","$$\n","\n","Result:\n","\n","$$[5, 0, 7, 0]$$\n","\n","\n","---"]},{"cell_type":"markdown","id":"d4fc26c8","metadata":{"papermill":{"duration":0.010298,"end_time":"2026-01-25T10:24:42.052612","exception":false,"start_time":"2026-01-25T10:24:42.042314","status":"completed"},"tags":[]},"source":["### Step 3: The Core Equation\n","\n","$$\n","\\tilde{h} = h \\cdot r\n","$$\n","\n","This is not fancy math.\n","\n","It literally means:\n","\n","- If r = 1 → output stays the same\n","- If r = 0 → output becomes zero\n"]},{"cell_type":"markdown","id":"95b2aa20","metadata":{"papermill":{"duration":0.00926,"end_time":"2026-01-25T10:24:42.07136","exception":false,"start_time":"2026-01-25T10:24:42.0621","status":"completed"},"tags":[]},"source":["### Step 4: Manual Single-Neuron Example\n","\n","Assume:\n","- Neuron output: $h = 6$\n","- Keep probability: $p = 0.5$\n","\n","Possible outcomes:\n","\n","| r | Calculation | Output |\n","|---|------------|--------|\n","| 1 | 6 × 1 | 6 |\n","| 0 | 6 × 0 | 0 |\n","\n","So during training:\n","- Sometimes the neuron exists\n","- Sometimes it vanishes\n"]},{"cell_type":"markdown","id":"7eb3423e","metadata":{"papermill":{"duration":0.009359,"end_time":"2026-01-25T10:24:42.090108","exception":false,"start_time":"2026-01-25T10:24:42.080749","status":"completed"},"tags":[]},"source":["### Step 5: Manual Multi-Neuron Example (Critical)\n","\n","Assume a layer output:\n","\n","$h = [4, 2, 8, 6]$\n","\n","Let $p = 0.5$\n","\n","Random Bernoulli mask:\n","\n","$r = [1, 0, 1, 0]$\n","\n","Apply dropout:\n","\n","$$\n","\\tilde{h} = h \\odot r\n","$$\n","\n","Result:\n","\n","$$[4, 0, 8, 0]$$\n","\n","Half the neurons are removed for this step only.\n"]},{"cell_type":"markdown","id":"f1cf0118","metadata":{"papermill":{"duration":0.009427,"end_time":"2026-01-25T10:24:42.108833","exception":false,"start_time":"2026-01-25T10:24:42.099406","status":"completed"},"tags":[]},"source":["### Why Scaling Is Required\n","\n","If we randomly drop neurons, the expected output magnitude decreases.\n","\n","Without correction:\n","\n","- Training sees smaller activations\n","- Inference sees larger activations\n","- Model breaks\n","\n","### Expected Value Explanation\n","\n","Original expected output:\n","\n","$$E[h] = h$$\n","\n","After dropout (no scaling):\n","\n","$$\n","E[\\tilde{h}] = p \\cdot h + (1 - p) \\cdot 0\n","$$\n","\n","$$\n","E[\\tilde{h}] = p \\cdot h\n","$$\n","\n","So magnitude shrinks by factor p.\n","\n","**Concrete Numeric Example**\n","\n","Let:\n","- h = 10\n","- p = 0.5\n","\n","Expected value:\n","\n","$$\n","E[\\tilde{h}] = 0.5 × 10 = 5\n","$$\n","Meaning:\n","\n","- During training, neuron contributes half as much on average\n","\n","\n","**Why This Is a Problem**\n","\n","During training:\n","- Network learns using average signal ≈ 5\n","\n","During inference (no dropout):\n","- All neurons active\n","- Output = 10\n","\n","Distribution mismatch:\n","- Training sees small activations\n","- Inference sees larger activations\n","- Leads to unstable predictions\n","\n","### Inverted Dropout: The Fix (Used in PyTorch)\n","\n","Instead of scaling at inference, we scale during training.\n","\n","$$\n","\\tilde{h} = \\frac{h \\cdot r}{p}\n","$$\n","\n","\n","**Expected Value With Inverted Dropout**\n","\n","Possible outcomes now:\n","\n","\n","| r | Probability | Output |\n","|--|--|--|\n","| 1 | $p$ | $h / p$ |\n","| 0 | $1 − p$ | 0 |\n","\n","**Expected Value Calculation**\n","\n","$$\n","E[\\tilde{h}] = p \\cdot \\frac{h}{p} + (1 - p) \\cdot 0\n","$$\n","\n","$$\n","E[\\tilde{h}] = h\n","$$\n","\n","Matches original neuron output\n","\n","**Numeric Example**\n","\n","Let:\n","- $h = 10$\n","- $p = 0.5$\n","\n","Case 1: \n","\n","$r = 1$  \n","$Output = 10 / 0.5 = 20$\n","\n","Case 2: \n","\n","$r = 0$  \n","$Output = 0$  \n","\n","Expected value:\n","\n","$0.5 × 20 + 0.5 × 0 = 10$ \n","\n","\n","### What “Outputs Are Scaled Automatically” Really Means\n","\n","When you write:\n","\n","`nn.Dropout(p=0.5)`\n","\n","PyTorch:\n","- Applies Bernoulli mask\n","- Divides by p during training\n","- Does NOTHING during inference\n","\n","You never see the scaling, but it’s there.\n"]},{"cell_type":"markdown","id":"cb1b9bb8","metadata":{"papermill":{"duration":0.009259,"end_time":"2026-01-25T10:24:42.127466","exception":false,"start_time":"2026-01-25T10:24:42.118207","status":"completed"},"tags":[]},"source":["## 3️.  How Dropout Actually Helps\n","\n","Dropout exists to prevent co-adaptation between neurons.\n","\n","Co-adaptation happens when:\n","- Neuron A becomes useful only because neuron B exists\n","- Neuron B depends on neuron A to work correctly\n","\n","This creates fragile feature learning.\n","\n","If either neuron fails, the prediction collapses.\n","\n","\n","### What Dropout Does During Training\n","\n","During every training step:\n","\n","- Random neurons are temporarily removed\n","- The network structure changes every batch\n","- Forward and backward passes use a different sub-network\n","\n","Example:\n","\n","- Batch 1: Neurons A, C active\n","- Batch 2: Neurons B, D active\n","- Batch 3: Neurons A, B active\n","\n","No neuron is guaranteed to be present."]},{"cell_type":"markdown","id":"90400e63","metadata":{"papermill":{"duration":0.009396,"end_time":"2026-01-25T10:24:42.146234","exception":false,"start_time":"2026-01-25T10:24:42.136838","status":"completed"},"tags":[]},"source":["### Why This Is Equivalent to Training Many Sub-Networks\n","\n","Because neurons are randomly removed:\n","- The model never trains as a single fixed architecture\n","- It trains thousands of smaller networks\n","- All networks share the same weights\n","\n","This behaves like an ensemble:\n","- But without training separate models\n","- And without extra memory cost\n","\n","### Why Neurons Become More Robust\n","\n","Since any neuron can disappear:\n","- No neuron can rely on a specific partner\n","- Each neuron must learn independently useful features\n","\n","Instead of learning:\n","\n","Feature = Neuron A AND Neuron B\n","\n","The network learns:\n","\n","Feature = Neuron A OR Neuron B OR Neuron C\n","\n","This creates redundancy."]},{"cell_type":"markdown","id":"fc83bbdd","metadata":{"papermill":{"duration":0.009286,"end_time":"2026-01-25T10:24:42.164784","exception":false,"start_time":"2026-01-25T10:24:42.155498","status":"completed"},"tags":[]},"source":["### What This Achieves\n","\n","Dropout forces the model to learn:\n","- Multiple ways to represent the same pattern\n","- Backup features instead of brittle shortcuts\n","\n","Results:\n","- Better generalization\n","- Reduced overfitting\n","- More stable performance on unseen data"]},{"cell_type":"markdown","id":"a3e2c6c6","metadata":{"papermill":{"duration":0.009372,"end_time":"2026-01-25T10:24:42.183555","exception":false,"start_time":"2026-01-25T10:24:42.174183","status":"completed"},"tags":[]},"source":["## 4️. Why Dropout Is Tricky in CNNs\n","\n","Dropout behaves very differently in CNNs compared to fully connected networks. This is not accidental, it comes from how CNNs represent information.\n","\n","### How CNNs Represent Information\n","\n","CNNs rely on three structural ideas:\n","\n","- **Local spatial correlations**  \n","  Nearby pixels (or tokens) are strongly related.\n","\n","- **Shared convolutional filters**  \n","  The same filter detects the same pattern everywhere.\n","\n","- **Structured feature maps**  \n","  Activations form grids (height × width × channels), not flat vectors.\n","\n","This structure is the strength of CNNs.\n","\n","### What Dropout Does That Causes Trouble\n","\n","Dropout randomly removes individual activations.\n","\n","In early convolution layers, this means:\n","- Random pixels in feature maps are erased\n","- Local continuity is broken\n","- Partial edges or textures disappear\n","\n","**Example (edge detection):**\n","\n","Original feature map:\n","\n","████████<br>\n","████████<br>\n","████████\n","\n","After dropout:\n","\n","███ ███<br>\n","█ █████<br>\n","████ ██\n","\n","Edges become fragmented.\n","\n","\n","### Why This Hurts Early CNN Layers\n","\n","Early convolution layers learn:\n","- Edges\n","- Corners\n","- Textures\n","- Simple shapes\n","\n","These features require spatial consistency.\n","\n","Dropping random neurons early:\n","- Destroys local patterns\n","- Makes filters harder to learn\n","- Slows convergence\n","- Reduces representation quality\n","\n","In short:\n","> Dropout fights against what early CNN layers are trying to learn.\n","\n","### Why Dropout Works Better in Later Layers\n","\n","Later CNN layers (especially fully connected layers):\n","- Represent abstract concepts\n","- No longer depend on precise spatial layout\n","- Behave like standard dense networks\n","\n","Examples:\n","- “Catness”\n","- “Face-like structure”\n","- “Positive sentiment”\n","\n","Here:\n","- Co-adaptation becomes a real risk\n","- Dropout helps prevent over-reliance on specific neurons\n","\n","\n","### Practical Rule Used in Real Systems\n","\n","- Avoid dropout in early convolution layers\n","- Dropout is usually NOT needed in convolutional layers and if you use it, use very little, very carefully.\n","- Use dropout in:\n","  - Fully connected layers\n","  - Classification heads\n","  - Dense decision layers\n","\n","\n","### Industry Reality Check\n","\n","Modern CNN architectures often:\n","- Use Batch Normalization instead of dropout\n","- Use data augmentation for regularization\n","- Apply dropout only near the output\n","\n","That’s why you rarely see heavy dropout in ResNet, EfficientNet, etc.\n","\n","\n","\n","> CNNs depend on spatial structure. Dropout destroys spatial structure.\n","\n","So:\n","\n","> Dropout is a poor regularizer for early CNN layers but a good regularizer for dense decision layers.\n"]},{"cell_type":"markdown","id":"e159868b","metadata":{"papermill":{"duration":0.009582,"end_time":"2026-01-25T10:24:42.202817","exception":false,"start_time":"2026-01-25T10:24:42.193235","status":"completed"},"tags":[]},"source":["## 5️. Where to Use Dropout in CNN\n","\n","- After Fully Connected (FC) layers\n","- After Global Average Pooling\n","- Late-stage convolution blocks (light dropout)\n","\n","Avoid\n","- First conv layer\n","- Aggressive dropout in early feature extraction\n","\n","Typical Values\n","\n","| Layer Type | Dropout Rate |\n","|----------|-------------|\n","| FC Layers | 0.3 – 0.5 |\n","| Conv Blocks | 0.1 – 0.3 |\n"]},{"cell_type":"markdown","id":"87321416","metadata":{"papermill":{"duration":0.009403,"end_time":"2026-01-25T10:24:42.22212","exception":false,"start_time":"2026-01-25T10:24:42.212717","status":"completed"},"tags":[]},"source":["## 7. Example: Dropout in a CNN"]},{"cell_type":"code","execution_count":1,"id":"887f9574","metadata":{"execution":{"iopub.execute_input":"2026-01-25T10:24:42.242852Z","iopub.status.busy":"2026-01-25T10:24:42.24251Z","iopub.status.idle":"2026-01-25T10:24:46.972552Z","shell.execute_reply":"2026-01-25T10:24:46.971517Z"},"papermill":{"duration":4.743533,"end_time":"2026-01-25T10:24:46.974974","exception":false,"start_time":"2026-01-25T10:24:42.231441","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Define a simple CNN model\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        # Convolutional layer:\n","        # Input: 1 channel (grayscale), Output: 16 channels, Kernel: 3x3\n","        # No padding, so output spatial size = (28-3+1)=26 # assuming input 28x28\n","        self.conv = nn.Conv2d(1, 16, 3)\n","        \n","        # Dropout layer for regularization\n","        # Randomly zeroes 30% of neurons during training\n","        self.dropout = nn.Dropout(p=0.3)\n","        \n","        # Fully connected layer:\n","        # Input features = 16 channels * 26 * 26 pixels (flattened)\n","        # Output features = 10 classes\n","        self.fc = nn.Linear(16*26*26, 10)  \n","\n","    def forward(self, x):\n","        # Apply convolution\n","        x = self.conv(x)\n","        \n","        # Apply ReLU activation function\n","        # F.relu is functional (stateless) version\n","        x = F.relu(x)\n","        \n","        # Flatten 4D tensor (B, C, H, W) -> 2D tensor (B, features)\n","        # Necessary for feeding into fully connected layer\n","        x = x.view(x.size(0), -1)\n","        \n","        # Apply dropout (only active during training)\n","        x = self.dropout(x)\n","        \n","        # Fully connected layer to produce logits for 10 classes\n","        x = self.fc(x)\n","        \n","        return x\n"]},{"cell_type":"markdown","id":"da2241d6","metadata":{"papermill":{"duration":0.009577,"end_time":"2026-01-25T10:24:46.994524","exception":false,"start_time":"2026-01-25T10:24:46.984947","status":"completed"},"tags":[]},"source":["Dropout:\n","\n","- `ON` during `model.train()`\n","- `OFF`during `model.eval()`"]},{"cell_type":"markdown","id":"a1ae83a7","metadata":{"papermill":{"duration":0.009463,"end_time":"2026-01-25T10:24:47.01353","exception":false,"start_time":"2026-01-25T10:24:47.004067","status":"completed"},"tags":[]},"source":["# Batch Normalization in CNN\n","\n","**Batch Normalization (BatchNorm)** is a technique to normalize the inputs of each layer in a neural network. It is widely used in CNNs to stabilize and accelerate training.\n","\n","For an input activation $x$ in a mini-batch:\n","\n","$$\n","\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n","$$\n","\n","Where:  \n","- $\\mu$ = mean of the mini-batch  \n","- $\\sigma^2$ = variance of the mini-batch  \n","- $\\epsilon$ = small constant to avoid division by zero\n","\n","After normalization, activations have zero mean and unit variance, which can limit the network’s expressive power. BatchNorm introduces two learnable parameters $\\gamma$ and $\\beta$ to allow the network to scale and shift the normalized values:\n","\n","- $\\gamma$ controls the **strength (variance)** of a feature  \n","- $\\beta$ controls the **position (mean)** of a feature  \n","- They allow BatchNorm to represent the identity function if needed  \n","- Prevent normalization from restricting what the network can learn  \n","\n","Without scale and shift, BatchNorm would stabilize training but reduce model capacity.\n","\n","$$\n","y = \\gamma \\hat{x} + \\beta\n","$$\n","\n","- $\\gamma$ = scale factor  \n","- $\\beta$ = shift factor  \n","\n","This ensures the network can still represent the identity transformation if needed.\n","\n"]},{"cell_type":"markdown","id":"f5f98431","metadata":{"papermill":{"duration":0.009367,"end_time":"2026-01-25T10:24:47.032331","exception":false,"start_time":"2026-01-25T10:24:47.022964","status":"completed"},"tags":[]},"source":["## 1. Manual Example\n","\n","Consider a mini-batch of 4 activations from a single neuron/channel:\n","\n","$$\n","x = [2, 4, 6, 8]\n","$$\n","\n","We'll apply Batch Normalization with a small $\\epsilon = 10^{-5}$, and assume learnable parameters:\n","\n","$$\n","\\gamma = 2, \\quad \\beta = 1\n","$$\n","\n","### Step 1: Compute Mini-Batch Mean\n","\n","The mean $\\mu$ is:\n","\n","$$\n","\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i\n","$$\n","\n","Here, $N=4$:\n","\n","$$\n","\\mu = \\frac{2 + 4 + 6 + 8}{4} = \\frac{20}{4} = 5\n","$$\n"]},{"cell_type":"markdown","id":"eb90692a","metadata":{"papermill":{"duration":0.009367,"end_time":"2026-01-25T10:24:47.051748","exception":false,"start_time":"2026-01-25T10:24:47.042381","status":"completed"},"tags":[]},"source":["### Step 2: Compute Mini-Batch Variance\n","\n","The variance $\\sigma^2$ is:\n","\n","$$\n","\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu)^2\n","$$\n","\n","$$\n","\\sigma^2 = \\frac{(2-5)^2 + (4-5)^2 + (6-5)^2 + (8-5)^2}{4}\n","$$\n","\n","$$\n","\\sigma^2 = \\frac{(-3)^2 + (-1)^2 + 1^2 + 3^2}{4} = \\frac{9 + 1 + 1 + 9}{4} = \\frac{20}{4} = 5\n","$$\n"]},{"cell_type":"markdown","id":"cbd2f371","metadata":{"papermill":{"duration":0.009729,"end_time":"2026-01-25T10:24:47.073009","exception":false,"start_time":"2026-01-25T10:24:47.06328","status":"completed"},"tags":[]},"source":["### Step 3: Normalize the Activations\n","\n","Normalized activations $\\hat{x}$:\n","\n","$$\n","\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n","$$\n","\n","$$\n","\\hat{x} = \\frac{[2,4,6,8] - 5}{\\sqrt{5 + 10^{-5}}} \\approx \\frac{[-3, -1, 1, 3]}{\\sqrt{5}} \\approx [-1.34, -0.45, 0.45, 1.34]\n","$$\n"]},{"cell_type":"markdown","id":"d38d5ea2","metadata":{"papermill":{"duration":0.009676,"end_time":"2026-01-25T10:24:47.092192","exception":false,"start_time":"2026-01-25T10:24:47.082516","status":"completed"},"tags":[]},"source":["### Step 4: Scale and Shift with $\\gamma$ and $\\beta$\n","\n","Finally, apply learnable parameters:\n","\n","$$\n","y_i = \\gamma \\hat{x}_i + \\beta\n","$$\n","\n","$$\n","y = 2 \\cdot [-1.34, -0.45, 0.45, 1.34] + 1 \\approx [-1.68, 0.1, 1.9, 3.68]\n","$$\n"]},{"cell_type":"markdown","id":"4d0fbf5f","metadata":{"papermill":{"duration":0.009443,"end_time":"2026-01-25T10:24:47.111721","exception":false,"start_time":"2026-01-25T10:24:47.102278","status":"completed"},"tags":[]},"source":["### Result\n","\n","Original mini-batch: $[2, 4, 6, 8]$  \n","Normalized & scaled mini-batch: $[-1.68, 0.1, 1.9, 3.68]$\n","\n","- The activations are now centered, scaled, and shifted.\n","- BatchNorm has stabilized the input distribution while keeping learnable flexibility.\n"]},{"cell_type":"markdown","id":"ae42ac88","metadata":{"papermill":{"duration":0.009501,"end_time":"2026-01-25T10:24:47.130754","exception":false,"start_time":"2026-01-25T10:24:47.121253","status":"completed"},"tags":[]},"source":["## 2. How BatchNorm Works\n","\n","1. Normalizes activations across the mini-batch.  \n","2. Centers and scales each feature to have zero mean and unit variance.  \n","3. Introduces learnable parameters ($\\gamma$ and $\\beta$) to retain representational flexibility.  \n","4. Integrates seamlessly with convolutional layers by normalizing across channels for each spatial location.\n"]},{"cell_type":"markdown","id":"751a8c78","metadata":{"papermill":{"duration":0.009394,"end_time":"2026-01-25T10:24:47.149691","exception":false,"start_time":"2026-01-25T10:24:47.140297","status":"completed"},"tags":[]},"source":["## 3. Benefits of BatchNorm\n","\n","1. **Stabilizes Gradients**  \n","   - By keeping activations in a consistent range, gradients do not explode or vanish.  \n","   - This makes deeper networks trainable.\n","\n","2. **Allows Higher Learning Rates**  \n","   - Reduces the risk of divergence, enabling faster convergence.\n","\n","3. **Reduces Internal Covariate Shift**  \n","   - The distribution of inputs to each layer becomes more stable during training, which improves learning efficiency.\n","\n","4. **Acts as a Mild Regularizer**  \n","   - Slight noise from mini-batch statistics reduces overfitting, sometimes reducing the need for dropout.\n","\n","5. **Improves Generalization**  \n","   - Normalization smoothens the optimization landscape, making training more robust.\n"]},{"cell_type":"markdown","id":"61120bdc","metadata":{"papermill":{"duration":0.009491,"end_time":"2026-01-25T10:24:47.168745","exception":false,"start_time":"2026-01-25T10:24:47.159254","status":"completed"},"tags":[]},"source":["## 4. CNN Batch Normalization Placement\n","\n","The correct and most common placement of BatchNorm in CNNs is:\n","\n","$$\n","\\text{Conv} \\;\\rightarrow\\; \\text{BatchNorm} \\;\\rightarrow\\; \\text{Activation}\n","$$\n","\n","Example with ReLU:\n","\n","$$\n","y = \\text{ReLU}(\\text{BN}(\\text{Conv}(x)))\n","$$\n","\n","This is the default choice in modern CNN architectures.\n"]},{"cell_type":"markdown","id":"97de9025","metadata":{"papermill":{"duration":0.009407,"end_time":"2026-01-25T10:24:47.187662","exception":false,"start_time":"2026-01-25T10:24:47.178255","status":"completed"},"tags":[]},"source":["### Why BatchNorm Is Placed After Convolution\n","\n","A convolution layer produces raw feature maps with unstable distributions during training.\n","\n","BatchNorm:\n","- Normalizes these feature maps\n","- Stabilizes their distribution\n","- Makes the activation function behave predictably\n","\n","Placing BN before activation ensures the nonlinearity receives normalized inputs.\n"]},{"cell_type":"markdown","id":"2194a9ef","metadata":{"papermill":{"duration":0.009571,"end_time":"2026-01-25T10:24:47.206896","exception":false,"start_time":"2026-01-25T10:24:47.197325","status":"completed"},"tags":[]},"source":["### Why NOT Place BatchNorm After Activation\n","\n","Bad pattern:\n","\n","$$\n","\\text{Conv} \\;\\rightarrow\\; \\text{Activation} \\;\\rightarrow\\; \\text{BatchNorm}\n","$$\n","\n","Reasons:\n","- Activations like ReLU clip negative values\n","- This distorts the distribution\n","- BatchNorm then normalizes a biased signal\n","- Empirically worse convergence\n"]},{"cell_type":"markdown","id":"83accb8e","metadata":{"papermill":{"duration":0.009515,"end_time":"2026-01-25T10:24:47.225847","exception":false,"start_time":"2026-01-25T10:24:47.216332","status":"completed"},"tags":[]},"source":["## 5. Exact Computation Order in CNN\n","\n","For an input tensor $x$:\n","\n","1. Convolution:\n","$$\n","z = W * x + b\n","$$\n","\n","2. Batch Normalization:\n","$$\n","\\hat{z} = \\frac{z - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n","$$\n","\n","$$\n","z_{\\text{BN}} = \\gamma \\hat{z} + \\beta\n","$$\n","\n","3. Activation:\n","$$\n","y = \\text{ReLU}(z_{\\text{BN}})\n","$$"]},{"cell_type":"markdown","id":"83c43df9","metadata":{"papermill":{"duration":0.009371,"end_time":"2026-01-25T10:24:47.244641","exception":false,"start_time":"2026-01-25T10:24:47.23527","status":"completed"},"tags":[]},"source":["## 6. Channel-wise Normalization in CNNs\n","\n","For CNNs, BatchNorm is applied per channel, not per pixel.\n","\n","Given tensor shape:\n","$$\n","(N, C, H, W)\n","$$\n","\n","BatchNorm computes:\n","- Mean $\\mu_c$\n","- Variance $\\sigma_c^2$\n","\n","Across:\n","$$\n","N \\times H \\times W\n","$$\n","\n","For each channel $c$ independently.\n"]},{"cell_type":"markdown","id":"08acffaf","metadata":{"papermill":{"duration":0.009434,"end_time":"2026-01-25T10:24:47.263565","exception":false,"start_time":"2026-01-25T10:24:47.254131","status":"completed"},"tags":[]},"source":["## 7. Real Architecture Examples\n","\n","**VGG-BN**\n","\n","$$\n","\\text{Conv} \\rightarrow \\text{BN} \\rightarrow \\text{ReLU}\n","$$\n","\n","**ResNet (Post-activation)**\n","\n","$$\n","\\text{Conv} \\rightarrow \\text{BN} \\rightarrow \\text{ReLU}\n","$$\n","\n","**ResNet (Pre-activation variant)**\n","\n","$$\n","\\text{BN} \\rightarrow \\text{ReLU} \\rightarrow \\text{Conv}\n","$$\n","\n","Pre-activation ResNet is a special case, not the default.\n"]},{"cell_type":"markdown","id":"10f3ebbd","metadata":{"papermill":{"duration":0.009376,"end_time":"2026-01-25T10:24:47.282375","exception":false,"start_time":"2026-01-25T10:24:47.272999","status":"completed"},"tags":[]},"source":["## 8. BatchNorm + Bias Redundancy\n","\n","When using BatchNorm:\n","\n","**Do NOT use bias in Conv layers**\n","\n","Reason:\n","\n","$$\n","\\beta \\text{ in BN replaces bias}\n","$$\n","\n","Practical rule:\n","- `Conv2d(bias=False)`\n","- `BatchNorm2d(...)`\n"]},{"cell_type":"markdown","id":"ebb69842","metadata":{"papermill":{"duration":0.009461,"end_time":"2026-01-25T10:24:47.301325","exception":false,"start_time":"2026-01-25T10:24:47.291864","status":"completed"},"tags":[]},"source":["## 9. Summary: CNN BatchNorm Placement Rules\n","\n","Default:\n","\n","$$\n","\\text{Conv} \\rightarrow \\text{BN} \\rightarrow \\text{Activation}\n","$$\n","\n","Avoid:\n","\n","$$\n","\\text{Conv} \\rightarrow \\text{Activation} \\rightarrow \\text{BN}\n","$$\n","\n","- BN normalizes feature maps, not activations  \n","- Always disable Conv bias when using BN  \n","- Enables deeper networks and higher learning rates\n"]},{"cell_type":"markdown","id":"13eba469","metadata":{"papermill":{"duration":0.00953,"end_time":"2026-01-25T10:24:47.3204","exception":false,"start_time":"2026-01-25T10:24:47.31087","status":"completed"},"tags":[]},"source":["## 10. Example: BatchNorm in CNN"]},{"cell_type":"code","execution_count":2,"id":"26ff17bf","metadata":{"execution":{"iopub.execute_input":"2026-01-25T10:24:47.341173Z","iopub.status.busy":"2026-01-25T10:24:47.3407Z","iopub.status.idle":"2026-01-25T10:24:47.348598Z","shell.execute_reply":"2026-01-25T10:24:47.347625Z"},"papermill":{"duration":0.020621,"end_time":"2026-01-25T10:24:47.350485","exception":false,"start_time":"2026-01-25T10:24:47.329864","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# CNN with BatchNorm Example\n","class CNNWithBN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # Conv layer 1: input 3 channels, output 16 channels, kernel 3x3\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n","        # BatchNorm for conv1 output channels\n","        self.bn1 = nn.BatchNorm2d(16)\n","        \n","        # Conv layer 2: input 16 channels, output 32 channels\n","        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(32)\n","        \n","        # Fully connected layer: flatten 32*8*8 -> 10 classes\n","        self.fc = nn.Linear(32*8*8, 10)\n","\n","    def forward(self, x):\n","        # Conv -> BatchNorm -> ReLU\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        x = F.relu(self.bn2(self.conv2(x)))\n","        \n","        # Flatten for FC layer\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x"]},{"cell_type":"markdown","id":"335eb63b","metadata":{"papermill":{"duration":0.009603,"end_time":"2026-01-25T10:24:47.370077","exception":false,"start_time":"2026-01-25T10:24:47.360474","status":"completed"},"tags":[]},"source":["Learning Notes:\n","\n","- BatchNorm is applied after Conv, before ReLU.\n","- `nn.BatchNorm2d(num_features)` → number of channels.\n","- Normalizes activations per channel across the batch, stabilizing training."]},{"cell_type":"markdown","id":"1f9051b0","metadata":{"papermill":{"duration":0.009472,"end_time":"2026-01-25T10:24:47.389213","exception":false,"start_time":"2026-01-25T10:24:47.379741","status":"completed"},"tags":[]},"source":["# PART B: Regularization in RNNs"]},{"cell_type":"markdown","id":"2adba76b","metadata":{"papermill":{"duration":0.009625,"end_time":"2026-01-25T10:24:47.408343","exception":false,"start_time":"2026-01-25T10:24:47.398718","status":"completed"},"tags":[]},"source":["RNNs are designed to handle sequence data, e.g., text, time series, or speech.  \n","Unlike regular feedforward networks, RNNs maintain a memory of previous inputs using hidden states.\n","\n","**RNN Structure**\n","\n","At each time step $t$, the RNN computes:\n","\n","$$\n","h_t = f(W_x x_t + W_h h_{t-1} + b)\n","$$\n","\n","- $x_t$ = input at time step $t$  \n","- $h_{t-1}$ = hidden state from previous time step  \n","- $W_x, W_h$ = weights for input and hidden state  \n","- $b$ = bias  \n","- $f$ = activation function (usually $\\tanh$ or ReLU)\n","\n","Output can be:\n","\n","$$\n","y_t = g(W_y h_t + c)\n","$$\n","\n","- $y_t$ = output at time step $t$  \n","- $g$ = activation for output (softmax for classification)\n","\n","Suppose a simple RNN with:\n","- 1 input neuron, 1 hidden neuron  \n","- $W_x = 0.5$, $W_h = 0.8$, $b = 0$  \n","- Activation $f = \\tanh$  \n","- Sequence $x = [1.0, 2.0]$  \n","- Initial hidden state $h_0 = 0$\n","\n","Time Step 1:\n","\n","$$\n","h_1 = \\tanh(W_x x_1 + W_h h_0) = \\tanh(0.5*1 + 0.8*0) = \\tanh(0.5) \\approx 0.462\n","$$\n","\n","Time Step 2:\n","\n","$$\n","h_2 = \\tanh(W_x x_2 + W_h h_1) = \\tanh(0.5*2 + 0.8*0.462) = \\tanh(1.3696) \\approx 0.878\n","$$\n","\n","So hidden states are $h = [0.462, 0.878]$.\n"]},{"cell_type":"markdown","id":"fe0f137a","metadata":{"papermill":{"duration":0.00946,"end_time":"2026-01-25T10:24:47.427767","exception":false,"start_time":"2026-01-25T10:24:47.418307","status":"completed"},"tags":[]},"source":["# Why RNN Regularization Is Tricky\n","\n","RNNs:\n","- Share weights across time\n","- Depend on hidden state\n","- Sensitive to noise\n","\n","Naive dropout:\n","- Breaks temporal consistency\n","- Destroys memory\n"]},{"cell_type":"markdown","id":"7c308a3f","metadata":{"papermill":{"duration":0.009519,"end_time":"2026-01-25T10:24:47.446953","exception":false,"start_time":"2026-01-25T10:24:47.437434","status":"completed"},"tags":[]},"source":["# Dropout in RNNs\n","\n","- RNNs can overfit small datasets because they reuse the same weights for all time steps.  \n","- Dropout is a way to regularize and improve generalization.\n","\n","⚠ Important: Hidden states are temporally dependent.  \n","- Randomly dropping hidden state values at each time step independently destroys sequence information.  \n","- **Solution:** use the same dropout mask across all time steps (recurrent dropout).\n"]},{"cell_type":"markdown","id":"d8866ac7","metadata":{"papermill":{"duration":0.009607,"end_time":"2026-01-25T10:24:47.466041","exception":false,"start_time":"2026-01-25T10:24:47.456434","status":"completed"},"tags":[]},"source":["## 1. Standard Dropout Concept\n","\n","For a layer with input vector $x$, standard dropout randomly sets a fraction $p$ of activations to zero:\n","\n","$$\n","\\tilde{x}_i =\n","\\begin{cases} \n","0 & \\text{with probability } p \\\\\n","\\frac{x_i}{1-p} & \\text{with probability } 1-p\n","\\end{cases}\n","$$\n","\n","- $p$ = dropout rate (e.g., 0.2 means 20% of neurons are dropped)\n","- Scaling by $1/(1-p)$ ensures expectation is unchanged\n"]},{"cell_type":"markdown","id":"75e04306","metadata":{"papermill":{"duration":0.010053,"end_time":"2026-01-25T10:24:47.485601","exception":false,"start_time":"2026-01-25T10:24:47.475548","status":"completed"},"tags":[]},"source":["## 2. Dropout in RNNs (Vanilla RNN/LSTM/GRU)\n","\n","Key difference: hidden states are temporally correlated.  \n","\n","- Standard dropout at each time step can destroy temporal patterns.  \n","- **Solution:** apply dropout to inputs and/or hidden states consistently across time steps (same mask for all time steps in a sequence).\n","\n","Common patterns:\n","\n","1. **Input Dropout**: Apply dropout to $x_t$ (the inputs at each time step).  \n","2. **Recurrent Dropout**: Apply dropout to $h_{t-1}$ (hidden state) using the same mask across all $t$.  \n","3. **Output Dropout**: Optional dropout after $h_t$ before feeding to final layer.\n"]},{"cell_type":"markdown","id":"50cc45dd","metadata":{"papermill":{"duration":0.00943,"end_time":"2026-01-25T10:24:47.504546","exception":false,"start_time":"2026-01-25T10:24:47.495116","status":"completed"},"tags":[]},"source":["## 3. Vanilla RNN with Dropout (Step-by-Step)\n","\n","RNN update (simplified):\n","\n","$$\n","h_t = \\tanh(W_x x_t + W_h h_{t-1} + b)\n","$$\n","\n","With **dropout on input**:\n","\n","$$\n","\\tilde{x}_t = \\text{Dropout}(x_t)\n","$$\n","\n","$$\n","h_t = \\tanh(W_x \\tilde{x}_t + W_h h_{t-1} + b)\n","$$\n","\n","With **recurrent dropout**:\n","\n","$$\n","\\tilde{h}_{t-1} = \\text{Dropout}(h_{t-1})\n","$$\n","\n","$$\n","h_t = \\tanh(W_x x_t + W_h \\tilde{h}_{t-1} + b)\n","$$\n"]},{"cell_type":"markdown","id":"c200b798","metadata":{"papermill":{"duration":0.00957,"end_time":"2026-01-25T10:24:47.523536","exception":false,"start_time":"2026-01-25T10:24:47.513966","status":"completed"},"tags":[]},"source":["## 4. Example: RNN with Input & Recurrent Dropout\n"]},{"cell_type":"markdown","id":"ad11cd86","metadata":{"papermill":{"duration":0.009447,"end_time":"2026-01-25T10:24:47.542609","exception":false,"start_time":"2026-01-25T10:24:47.533162","status":"completed"},"tags":[]},"source":["**Step 1: RNN Setup**\n","\n","- Sequence: $x = [1.0, 2.0]$ (2 time steps)  \n","- Single hidden unit: $h_t$  \n","- Initial hidden state: $h_0 = 0$  \n","- Weights: $W_x = 0.5$, $W_h = 0.8$, bias $b = 0$  \n","- Activation: $\\tanh$  \n","- Dropout rate: $p = 0.5$  \n","- Apply:\n","  - Input dropout\n","  - Recurrent dropout (same mask across time steps)\n"]},{"cell_type":"markdown","id":"8bf636fb","metadata":{"papermill":{"duration":0.009508,"end_time":"2026-01-25T10:24:47.561615","exception":false,"start_time":"2026-01-25T10:24:47.552107","status":"completed"},"tags":[]},"source":["**Step 2: Generate Dropout Masks**\n","\n","- Input dropout mask (same for all steps): $m_x = [1, 0]$  \n","- Recurrent dropout mask (same for all steps): $m_h = [1]$  \n","\n","Scaling factor: $1/(1-p) = 1/0.5 = 2$  \n"]},{"cell_type":"markdown","id":"f65dff86","metadata":{"papermill":{"duration":0.010142,"end_time":"2026-01-25T10:24:47.581231","exception":false,"start_time":"2026-01-25T10:24:47.571089","status":"completed"},"tags":[]},"source":["**Step 3: Apply Dropout to Inputs**\n","\n","$$\n","\\tilde{x}_t = x_t \\cdot m_x / (1-p)\n","$$\n","\n","- Time step 1: $\\tilde{x}_1 = 1.0 * 1 * 2 = 2.0$  \n","- Time step 2: $\\tilde{x}_2 = 2.0 * 0 * 2 = 0.0$\n"]},{"cell_type":"markdown","id":"f0df21b0","metadata":{"papermill":{"duration":0.009555,"end_time":"2026-01-25T10:24:47.600388","exception":false,"start_time":"2026-01-25T10:24:47.590833","status":"completed"},"tags":[]},"source":["**Step 4: Apply Dropout to Hidden State**\n","\n","Initial hidden: $h_0 = 0$  \n","Recurrent dropout mask: $m_h = 1$  \n","Scaled: $h_0 \\cdot 1 / 0.5 = 0$  \n","\n","So initial hidden used in computation: $h_0^{drop} = 0$\n"]},{"cell_type":"markdown","id":"ea36b611","metadata":{"papermill":{"duration":0.009416,"end_time":"2026-01-25T10:24:47.619452","exception":false,"start_time":"2026-01-25T10:24:47.610036","status":"completed"},"tags":[]},"source":["**Step 5: Compute Hidden States**\n","\n","Time step 1:\n","\n","$$\n","h_1 = \\tanh(W_x \\tilde{x}_1 + W_h h_0^{drop} + b)\n","= \\tanh(0.5*2.0 + 0.8*0 + 0)\n","= \\tanh(1.0) \\approx 0.761\n","$$\n","\n","Time step 2:\n","\n","$$\n","h_1^{drop} = h_1 * m_h / (1-p) = 0.761 * 1 / 0.5 = 1.522\n","$$\n","\n","$$\n","h_2 = \\tanh(W_x \\tilde{x}_2 + W_h h_1^{drop} + b)\n","= \\tanh(0.5*0 + 0.8*1.522 + 0)\n","= \\tanh(1.218) \\approx 0.839\n","$$\n"]},{"cell_type":"markdown","id":"c4084b28","metadata":{"papermill":{"duration":0.009575,"end_time":"2026-01-25T10:24:47.638571","exception":false,"start_time":"2026-01-25T10:24:47.628996","status":"completed"},"tags":[]},"source":["**Step 6: Summary of Computation**\n","\n","| Time Step | Input $x_t$ | Input Dropout $\\tilde{x}_t$ | Hidden $h_{t-1}^{drop}$ | Hidden $h_t$ |\n","|-----------|------------|----------------------------|-------------------------|-------------|\n","| 1         | 1.0        | 2.0                        | 0.0                     | 0.761       |\n","| 2         | 2.0        | 0.0                        | 1.522                   | 0.839       |\n","\n","Key observations:\n","\n","- Input dropout sets some inputs to zero (scaled up others).  \n","- Recurrent dropout scales hidden states consistently across steps.  \n","- Temporal patterns are preserved while regularizing the network.\n"]},{"cell_type":"markdown","id":"60b9578a","metadata":{"papermill":{"duration":0.009681,"end_time":"2026-01-25T10:24:47.658346","exception":false,"start_time":"2026-01-25T10:24:47.648665","status":"completed"},"tags":[]},"source":["## 5. CNN Dropout vs RNN Dropout\n"]},{"cell_type":"markdown","id":"27047276","metadata":{"papermill":{"duration":0.009361,"end_time":"2026-01-25T10:24:47.677357","exception":false,"start_time":"2026-01-25T10:24:47.667996","status":"completed"},"tags":[]},"source":["### 1. **Where Dropout is Applied**\n","\n","**CNN:**\n","- Applied to activations of fully connected layers or feature maps of convolutional layers.  \n","- Usually after Conv + Activation, or between FC layers.  \n","- Each neuron (or feature map) is dropped independently for every forward pass.\n","\n","**RNN:**\n","- Applied to input vectors ($x_t$), hidden states ($h_t$), or output before final layer.  \n","- Recurrent dropout is shared across all time steps to preserve temporal correlations.  \n","- Independent dropout per time step on hidden state usually breaks sequence learning."]},{"cell_type":"markdown","id":"0282d2d5","metadata":{"papermill":{"duration":0.009506,"end_time":"2026-01-25T10:24:47.696565","exception":false,"start_time":"2026-01-25T10:24:47.687059","status":"completed"},"tags":[]},"source":["### 2. **Temporal Dependency**\n","\n","**CNN:**\n","- No temporal dependency between inputs; dropping neurons is independent.  \n","- Random dropout every forward pass works fine.\n","\n","**RNN:**\n","- Hidden states are sequentially dependent ($h_t$ depends on $h_{t-1}$).  \n","- Random dropout per time step on $h_t$ can destroy temporal patterns.  \n","- Use same mask across sequence for hidden state (recurrent dropout)."]},{"cell_type":"markdown","id":"d896fec8","metadata":{"papermill":{"duration":0.009351,"end_time":"2026-01-25T10:24:47.715472","exception":false,"start_time":"2026-01-25T10:24:47.706121","status":"completed"},"tags":[]},"source":["### 3. **Scaling and Implementation**\n","\n","**CNN:**\n","- Standard inverted dropout: scale kept activations by $1/(1-p)$ during training.  \n","- Dropout mask regenerated every forward pass.\n","\n","**RNN:**\n","- Input dropout: same as CNN.  \n","- Recurrent dropout: mask is fixed for entire sequence, scaled by $1/(1-p)$.  \n","- Frameworks like PyTorch apply dropout between layers for multi-layer RNNs.\n"]},{"cell_type":"markdown","id":"9dac4031","metadata":{"papermill":{"duration":0.009516,"end_time":"2026-01-25T10:24:47.734439","exception":false,"start_time":"2026-01-25T10:24:47.724923","status":"completed"},"tags":[]},"source":["### 4. **Effect on Training**\n","\n","**CNN:**\n","- Reduces co-adaptation between neurons, helps generalization.  \n","- Works well in deep feedforward and convolutional architectures.\n","\n","**RNN:**\n","- Prevents overfitting to short sequences.  \n","- Preserves sequence information when applied correctly.  \n","- Must carefully balance dropout rate: too high can destroy memory of previous time steps.\n"]},{"cell_type":"markdown","id":"1248d0fb","metadata":{"papermill":{"duration":0.009847,"end_time":"2026-01-25T10:24:47.754311","exception":false,"start_time":"2026-01-25T10:24:47.744464","status":"completed"},"tags":[]},"source":["### **Summary Table**\n","\n","| Aspect                     | CNN Dropout                           | RNN Dropout                              |\n","|-----------------------------|--------------------------------------|-----------------------------------------|\n","| Applied To                  | FC layers, Conv activations          | Input $x_t$, Hidden $h_t$, Output       |\n","| Temporal Dependency          | None                                  | High, hidden states depend on $t-1$    |\n","| Dropout Mask per Pass       | Yes, independent                       | Input: independent; Hidden: same mask across sequence |\n","| Scaling                     | $1/(1-p)$                             | $1/(1-p)$ (input & recurrent separately) |\n","| Effect on Network           | Reduces neuron co-adaptation          | Preserves sequence, regularizes memory  |\n","| Common Pitfall              | None significant                       | Different mask per time step → destroys temporal info |\n"]},{"cell_type":"markdown","id":"127976e2","metadata":{"papermill":{"duration":0.009533,"end_time":"2026-01-25T10:24:47.773461","exception":false,"start_time":"2026-01-25T10:24:47.763928","status":"completed"},"tags":[]},"source":["---\n","\n","### Understanding Dropout Scaling $1/(1-p)$*\n","\n","**Step 1: Setup**\n","\n","Suppose a neuron has activation value:\n","\n","$$\n","x = 4.0\n","$$\n","\n","We apply dropout rate $p = 0.5$ (50% chance to drop the neuron during training).  \n","\n","Without scaling, if the neuron is kept, its value is $x=4$; if dropped, $x=0$.\n","\n","**Step 2: Why Scaling is Needed**\n","\n","Dropout randomly drops neurons.  \n","- If we don’t scale, the expected value of the neuron during training decreases.  \n","\n","Expected value without scaling:\n","\n","$$\n","E[x_{\\text{drop}}] = (1-p) \\cdot x + p \\cdot 0 = 0.5 * 4 + 0.5 * 0 = 2\n","$$\n","\n","Notice: The expected activation drops from $4$ to $2$ — the network sees smaller activations during training.\n","\n","**Step 3: Apply Inverted Dropout Scaling**\n","\n","To keep the expected value same, scale kept neurons by $1/(1-p)$:\n","\n","$$\n","\\tilde{x} =\n","\\begin{cases} \n","0 & \\text{with probability } p \\\\\n","x / (1-p) & \\text{with probability } 1-p\n","\\end{cases}\n","$$\n","\n","Here, $1/(1-p) = 1/0.5 = 2$.\n","\n","**Step 4: Compute Scaled Dropout Example**\n","\n","- With probability $p = 0.5$, neuron is dropped: $\\tilde{x} = 0$  \n","- With probability $1-p = 0.5$, neuron is kept: $\\tilde{x} = x/(1-p) = 4/0.5 = 8$  \n","\n","Expected value with scaling:\n","\n","$$\n","E[\\tilde{x}] = 0.5*0 + 0.5*8 = 4\n","$$\n","\n","Now the expected activation is the same as original $x$, preventing the network from seeing smaller values during training.\n","\n","**Step 5: Summary Table**\n","\n","| Dropout Event | Without Scaling | With Scaling $1/(1-p)$ |\n","|---------------|----------------|------------------------|\n","| Neuron dropped | 0              | 0                      |\n","| Neuron kept    | 4              | 8                      |\n","| Expected value | 2              | 4 ✅                   |\n","\n","Key idea: Scaling ensures training activations match inference activations.\n","\n","### Dropout Scaling $1/(1-p)$ in CNN vs RNN\n","\n","### 1. CNNs\n","\n","- Dropout is applied independently to activations of neurons (or feature maps).  \n","- Example: after a fully connected layer or convolution + activation:\n","  $$ \\tilde{x}_i = \n","    \\begin{cases} \n","    0 & \\text{with probability } p \\\\\n","    x_i / (1-p) & \\text{with probability } 1-p\n","    \\end{cases} $$\n","- Each neuron’s mask is regenerated every forward pass.  \n","- Scaling ensures expected activation remains same, so the network sees similar values during training and inference.\n","\n","\n","### 2. RNNs\n","\n","- Dropout is applied to **input $x_t$, hidden states $h_t$, or output**.  \n","- Input dropout: same as CNN, applied independently per neuron (mask can be same across sequence or per step depending on framework).  \n","- Recurrent dropout (on $h_{t-1}$):\n","  - Mask is shared across all time steps to preserve temporal patterns.  \n","  - Still uses scaling $1/(1-p)$:\n","    $$ \\tilde{h}_{t-1} = h_{t-1} \\cdot \\text{mask} / (1-p) $$\n","- Scaling ensures hidden states have same expected magnitude, preventing the network from shrinking memory over time.\n","\n","\n","### 3. Key Similarities and Differences\n","\n","| Aspect                | CNN Dropout                | RNN Dropout                      |\n","|-----------------------|----------------------------|----------------------------------|\n","| Scaling               | $1/(1-p)$ (inverted)      | $1/(1-p)$ (inverted)            |\n","| Mask per forward pass | Yes, independent          | Input: independent; Recurrent: same across time steps |\n","| Temporal dependency    | None                      | Hidden state depends on previous steps |\n","| Purpose               | Prevent co-adaptation      | Prevent overfitting while preserving temporal memory |\n","\n","\n","---\n"]},{"cell_type":"markdown","id":"737b45ce","metadata":{"papermill":{"duration":0.009336,"end_time":"2026-01-25T10:24:47.792354","exception":false,"start_time":"2026-01-25T10:24:47.783018","status":"completed"},"tags":[]},"source":["## 6. Example: Dropout in RNN"]},{"cell_type":"code","execution_count":3,"id":"0a319018","metadata":{"execution":{"iopub.execute_input":"2026-01-25T10:24:47.813771Z","iopub.status.busy":"2026-01-25T10:24:47.812661Z","iopub.status.idle":"2026-01-25T10:24:47.819964Z","shell.execute_reply":"2026-01-25T10:24:47.819098Z"},"papermill":{"duration":0.020167,"end_time":"2026-01-25T10:24:47.821938","exception":false,"start_time":"2026-01-25T10:24:47.801771","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","# RNN with input dropout example\n","class RNNWithDropout(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, dropout_p=0.5):\n","        super().__init__()\n","        # Single-layer RNN\n","        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size,\n","                          num_layers=1, batch_first=True, dropout=0.0)\n","        \n","        # Dropout layer applied to input sequence\n","        self.input_dropout = nn.Dropout(dropout_p)\n","        \n","        # Fully connected output layer\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        # Apply dropout to input sequence\n","        x = self.input_dropout(x)\n","        \n","        # RNN forward pass\n","        out, h_n = self.rnn(x)\n","        \n","        # Take output of last time step for prediction\n","        out = self.fc(out[:, -1, :])\n","        return out"]},{"cell_type":"markdown","id":"b5fa5da6","metadata":{"papermill":{"duration":0.009955,"end_time":"2026-01-25T10:24:47.841719","exception":false,"start_time":"2026-01-25T10:24:47.831764","status":"completed"},"tags":[]},"source":["Learning Notes:\n","\n","- Dropout prevents overfitting by randomly zeroing input features.\n","- For RNN hidden states, recurrent dropout should be mask-shared across time steps.\n","- `nn.RNN(..., dropout=...)` only works for multi-layer RNNs. Single-layer recurrent dropout must be applied manually."]},{"cell_type":"markdown","id":"90c4d251","metadata":{"papermill":{"duration":0.00928,"end_time":"2026-01-25T10:24:47.860503","exception":false,"start_time":"2026-01-25T10:24:47.851223","status":"completed"},"tags":[]},"source":["# Normalization in RNNs\n","\n","Unlike CNNs, Batch Normalization in RNNs is not straightforward because:\n","\n","- **Hidden states are sequentially dependent:** $h_t$ depends on $h_{t-1}$  \n","- Standard BatchNorm (per time step) can break temporal patterns \n","- **Solution:** specialized techniques are used to normalize RNNs.\n"]},{"cell_type":"markdown","id":"6ec35ebd","metadata":{"papermill":{"duration":0.009328,"end_time":"2026-01-25T10:24:47.879394","exception":false,"start_time":"2026-01-25T10:24:47.870066","status":"completed"},"tags":[]},"source":["## 1. Approaches for Normalization in RNNs\n","\n","1. **Batch Normalization (BN) on inputs**\n","   - Apply BN only on input vectors $x_t$ at each time step:\n","     $$\n","     \\hat{x}_t = \\frac{x_t - \\mu_{\\text{batch}}}{\\sqrt{\\sigma^2_{\\text{batch}} + \\epsilon}}\n","     $$\n","   - Then feed $\\hat{x}_t$ into RNN: $h_t = f(W_x \\hat{x}_t + W_h h_{t-1} + b)$\n","   - Safe because input does not depend on hidden states\n","\n","2. **Layer Normalization (LN) on hidden states**\n","   - Normalize within a hidden state vector, not across batch:\n","     $$\n","     \\hat{h}_t = \\frac{h_t - \\mu_{h_t}}{\\sqrt{\\sigma^2_{h_t} + \\epsilon}}\n","     $$\n","   - $\\mu_{h_t}$ and $\\sigma^2_{h_t}$ computed across hidden units, not across batch\n","   - Works well because it preserves temporal dependencies\n","\n","3. **Recurrent BatchNorm / Variants**\n","   - Some research applies BN to hidden-to-hidden transitions carefully, but usually LayerNorm is preferred in RNNs.\n"]},{"cell_type":"markdown","id":"f0f0ad24","metadata":{"papermill":{"duration":0.009666,"end_time":"2026-01-25T10:24:47.8987","exception":false,"start_time":"2026-01-25T10:24:47.889034","status":"completed"},"tags":[]},"source":["## 2. Why LayerNorm is Preferred in RNNs\n","\n","- BatchNorm depends on batch statistics, which vary per time step → can destabilize RNN  \n","- LayerNorm normalizes across hidden units of a single time step, not across batch → stable for sequences  \n","- Works for LSTM, GRU, Vanilla RNN \n","- Often combined with dropout for regularization\n"]},{"cell_type":"markdown","id":"73e980ef","metadata":{"papermill":{"duration":0.009274,"end_time":"2026-01-25T10:24:47.91738","exception":false,"start_time":"2026-01-25T10:24:47.908106","status":"completed"},"tags":[]},"source":["## 3. Example: LayerNorm in RNN Hidden State\n","\n","Suppose hidden state vector at time $t$:\n","\n","$$\n","h_t = [0.5, 1.0, -0.5]\n","$$\n","\n","1. Compute mean and variance across units:\n","$$\n","\\mu_{h_t} = (0.5 + 1.0 - 0.5)/3 = 0.333\n","$$\n","$$\n","\\sigma^2_{h_t} = \\frac{(0.5-0.333)^2 + (1-0.333)^2 + (-0.5-0.333)^2}{3} \\approx 0.555\n","$$\n","\n","2. Normalize:\n","$$\n","\\hat{h}_t = \\frac{h_t - \\mu_{h_t}}{\\sqrt{\\sigma^2_{h_t} + \\epsilon}} \\approx [-0.23, 0.89, -0.67]\n","$$\n","\n","3. Apply learnable $\\gamma, \\beta$:\n","$$\n","h_t^{LN} = \\gamma \\hat{h}_t + \\beta\n","$$\n","\n","Output is normalized per time step, temporal dependency preserved.\n"]},{"cell_type":"markdown","id":"3b6b4f36","metadata":{"papermill":{"duration":0.009296,"end_time":"2026-01-25T10:24:47.936196","exception":false,"start_time":"2026-01-25T10:24:47.9269","status":"completed"},"tags":[]},"source":["## 4. Normalization (CNN vs RNN)\n","| Concept                  | CNN                 | RNN                       |\n","|---------------------------|-------------------|---------------------------|\n","| BatchNorm placement       | After Conv/FC, before activation | Only safe on inputs ($x_t$) |\n","| Hidden state normalization| Not needed         | LayerNorm preferred       |\n","| Temporal dependency       | None               | Must preserve sequence    |\n","| Scaling                   | Learnable $\\gamma, \\beta$ | Same, per hidden state vector |\n","| Regularization            | Dropout often used | Dropout + LayerNorm       |\n"]},{"cell_type":"markdown","id":"975a297d","metadata":{"papermill":{"duration":0.010448,"end_time":"2026-01-25T10:24:47.95597","exception":false,"start_time":"2026-01-25T10:24:47.945522","status":"completed"},"tags":[]},"source":["## 5. Example"]},{"cell_type":"code","execution_count":4,"id":"c83ac043","metadata":{"execution":{"iopub.execute_input":"2026-01-25T10:24:47.977311Z","iopub.status.busy":"2026-01-25T10:24:47.976947Z","iopub.status.idle":"2026-01-25T10:24:47.985079Z","shell.execute_reply":"2026-01-25T10:24:47.983923Z"},"papermill":{"duration":0.021148,"end_time":"2026-01-25T10:24:47.987203","exception":false,"start_time":"2026-01-25T10:24:47.966055","status":"completed"},"tags":[]},"outputs":[],"source":["# BatchNorm on RNN Input \n","\n","import torch\n","import torch.nn as nn\n","\n","class RNNWithBN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super().__init__()\n","        # BatchNorm applied to input features\n","        self.bn = nn.BatchNorm1d(input_size)\n","        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        b, seq_len, f = x.size()\n","        \n","        # Reshape for BatchNorm: (batch*seq_len, input_size)\n","        x_reshaped = x.contiguous().view(-1, f)\n","        x_norm = self.bn(x_reshaped)\n","        \n","        # Reshape back to (batch, seq_len, input_size)\n","        x = x_norm.view(b, seq_len, f)\n","        \n","        # RNN forward\n","        out, h_n = self.rnn(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n"]},{"cell_type":"markdown","id":"387d014c","metadata":{"papermill":{"duration":0.00992,"end_time":"2026-01-25T10:24:48.007064","exception":false,"start_time":"2026-01-25T10:24:47.997144","status":"completed"},"tags":[]},"source":["Learning Notes:\n","\n","- BatchNorm is safe on inputs $x_t$ but not on hidden states.\n","- Must reshape sequence for BatchNorm1d because it expects `(batch, features)`."]},{"cell_type":"code","execution_count":5,"id":"f6ff3856","metadata":{"execution":{"iopub.execute_input":"2026-01-25T10:24:48.028512Z","iopub.status.busy":"2026-01-25T10:24:48.028177Z","iopub.status.idle":"2026-01-25T10:24:48.035841Z","shell.execute_reply":"2026-01-25T10:24:48.034754Z"},"papermill":{"duration":0.0214,"end_time":"2026-01-25T10:24:48.038075","exception":false,"start_time":"2026-01-25T10:24:48.016675","status":"completed"},"tags":[]},"outputs":[],"source":["# LayerNorm on RNN Hidden States\n","\n","import torch\n","import torch.nn as nn\n","\n","class RNNWithLN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super().__init__()\n","        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n","        \n","        # LayerNorm normalizes hidden state across features per time step\n","        self.ln = nn.LayerNorm(hidden_size)\n","        \n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        # RNN forward\n","        out, h_n = self.rnn(x)\n","        \n","        # Apply LayerNorm to hidden states at all time steps\n","        out = self.ln(out)\n","        \n","        # Take last time step output\n","        out = self.fc(out[:, -1, :])\n","        return out\n"]},{"cell_type":"markdown","id":"4bb4116a","metadata":{"papermill":{"duration":0.01147,"end_time":"2026-01-25T10:24:48.059426","exception":false,"start_time":"2026-01-25T10:24:48.047956","status":"completed"},"tags":[]},"source":["Learning Notes:\n","\n","- LayerNorm is preferred for RNN hidden states.\n","- Normalizes across hidden units per time step, preserving temporal dependencies.\n","- Works for Vanilla RNN, LSTM, and GRU."]},{"cell_type":"markdown","id":"aedfb14c","metadata":{"papermill":{"duration":0.009693,"end_time":"2026-01-25T10:24:48.079019","exception":false,"start_time":"2026-01-25T10:24:48.069326","status":"completed"},"tags":[]},"source":["# Key Takeaways from Day 31\n","\n","- Regularization fights overfitting, not training loss\n","- Dropout simulates ensemble learning\n","- BatchNorm stabilizes CNN training\n","- RNNs need sequence-aware regularization\n","- LayerNorm > BatchNorm for RNNs\n","- Bad regularization can harm learning\n","\n","---"]},{"cell_type":"markdown","id":"0dd2516c","metadata":{"papermill":{"duration":0.00952,"end_time":"2026-01-25T10:24:48.098057","exception":false,"start_time":"2026-01-25T10:24:48.088537","status":"completed"},"tags":[]},"source":["<p style=\"text-align:center; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":11.340651,"end_time":"2026-01-25T10:24:49.732065","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-01-25T10:24:38.391414","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}