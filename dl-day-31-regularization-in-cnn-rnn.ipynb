{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-31-regularization-in-cnn-rnn?scriptVersionId=293361456\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"6103dee1","metadata":{"papermill":{"duration":0.005538,"end_time":"2026-01-22T18:02:13.514399","exception":false,"start_time":"2026-01-22T18:02:13.508861","status":"completed"},"tags":[]},"source":["# Day 31: Regularization in CNN/RNN\n","\n","Welcome to day 31!\n","\n","Today you'll learn: \n","\n","1. Why regularization is necessary\n","2. Dropout: intuition and behavior\n","3. Batch Normalization: why it works\n","4. Regularization in CNNs\n","5. Regularization in RNNs (important differences)\n","6. Practical do’s and don’ts\n","\n","\n","If you found this notebook helpful, your **<b style=\"color:orange;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"bb603c92","metadata":{"papermill":{"duration":0.003979,"end_time":"2026-01-22T18:02:13.522749","exception":false,"start_time":"2026-01-22T18:02:13.51877","status":"completed"},"tags":[]},"source":["#  Why Regularization Exists\n","\n","Deep networks have:\n","- Millions of parameters\n","- High expressive power\n","- Ability to memorize noise\n","\n","This leads to:\n","- Low training loss\n","- High validation loss\n","\n","→ **Overfitting**\n","\n","Regularization:\n","> Intentionally restricts model freedom to improve generalization\n"]},{"cell_type":"markdown","id":"74d3d3cd","metadata":{"papermill":{"duration":0.003938,"end_time":"2026-01-22T18:02:13.530702","exception":false,"start_time":"2026-01-22T18:02:13.526764","status":"completed"},"tags":[]},"source":["# CNN Overfitting Pattern\n","\n","Typical CNN behavior without regularization:\n","\n","- Training loss ↓↓↓\n","- Validation loss ↓ then ↑\n","- Filters become too specialized\n","- Model memorizes training images\n","\n","CNNs overfit because:\n","- Many filters\n","- Fully connected layers at the end\n"]},{"cell_type":"markdown","id":"b777009f","metadata":{"papermill":{"duration":0.003877,"end_time":"2026-01-22T18:02:13.538491","exception":false,"start_time":"2026-01-22T18:02:13.534614","status":"completed"},"tags":[]},"source":["# Dropout in CNN\n","\n","Dropout is a **regularization technique** designed to reduce overfitting by randomly disabling neurons during training.\n","\n","In CNNs, dropout must be used surgically. Blindly applying it hurts spatial learning.\n"]},{"cell_type":"markdown","id":"41fd45fd","metadata":{"papermill":{"duration":0.004363,"end_time":"2026-01-22T18:02:13.546909","exception":false,"start_time":"2026-01-22T18:02:13.542546","status":"completed"},"tags":[]},"source":["## 1️. What Problem Dropout Solves\n","\n","Deep CNNs have high representational capacity.\n","\n","This creates a core risk:\n","\n","- Model memorizes training data\n","- Learns fragile feature dependencies\n","- Performs poorly on unseen data\n","\n","This phenomenon is called **overfitting**.\n","\n","Dropout directly attacks:\n","\n","❌ Feature co-adaptation  \n","❌ Over-reliance on specific neurons  \n","❌ Brittle internal representations\n"]},{"cell_type":"markdown","id":"ef09d350","metadata":{"papermill":{"duration":0.004013,"end_time":"2026-01-22T18:02:13.555189","exception":false,"start_time":"2026-01-22T18:02:13.551176","status":"completed"},"tags":[]},"source":["## 2️. What Dropout Actually Does (Mathematics)\n","\n","Dropout randomly turns off neurons during training.\n","\n","For a neuron output $h$ during training:\n","\n","$$\n","\\tilde{h} = h \\cdot r, \\quad r \\sim \\text{Bernoulli}(p)\n","$$\n","\n","Where:\n","- $p$ = probability of keeping the neuron active\n","- $r = 1$ → neuron active\n","- $r = 0$ → neuron dropped\n","\n","Important:\n","- Dropout is applied only during training\n","- During inference, all neurons are active\n","- Outputs are scaled automatically"]},{"cell_type":"markdown","id":"1459b95e","metadata":{"papermill":{"duration":0.00387,"end_time":"2026-01-22T18:02:13.56305","exception":false,"start_time":"2026-01-22T18:02:13.55918","status":"completed"},"tags":[]},"source":["### Step 1: What is `h`?\n","\n","`h` is the output of a neuron before dropout.\n","\n","Example:\n","- Input → weights → activation function\n","- Resulting number = `h`\n","\n","So if a neuron fires with value:\n","\n","$h = 6.0$\n","\n","That is its contribution to the next layer."]},{"cell_type":"markdown","id":"70deff34","metadata":{"papermill":{"duration":0.003898,"end_time":"2026-01-22T18:02:13.570925","exception":false,"start_time":"2026-01-22T18:02:13.567027","status":"completed"},"tags":[]},"source":["### Step 2: What is `r`?\n","\n","`r` is a random switch.\n","\n","It is drawn from a Bernoulli distribution:\n","\n","$$r ∼ Bernoulli(p)$$\n","\n","Meaning:\n","- r = 1 with probability p (keep neuron)\n","- r = 0 with probability 1 − p (drop neuron)\n","\n","This decision is made:\n","- Independently\n","- For every neuron\n","- At every training step\n"]},{"cell_type":"markdown","id":"819fbb16","metadata":{"papermill":{"duration":0.004115,"end_time":"2026-01-22T18:02:13.578997","exception":false,"start_time":"2026-01-22T18:02:13.574882","status":"completed"},"tags":[]},"source":["---\n","\n","### What Is a Bernoulli Distribution?\n","\n","A Bernoulli distribution models an experiment with:\n","\n","- Exactly two possible outcomes\n","- One is labeled “success”\n","- One is labeled “failure”\n","\n","That’s it. No middle ground.\n","\n","\n","**Mathematical Definition**\n","\n","A random variable $X$ follows a Bernoulli distribution if:\n","\n","$$\n","X \\sim \\text{Bernoulli}(p)\n","$$\n","\n","Where:\n","- $p$ = probability of success\n","- $X = 1$ with probability $p$\n","- $X = 0$ with probability $1 - p$\n","\n","So:\n","\n","| Outcome | Value | Probability |\n","|------|------|-------------|\n","| Success | 1 | $p$ |\n","| Failure | 0 | $1 - p$ |\n","\n","\n","**Example A: Coin Flip (Biased)**\n","\n","- Head = 1\n","- Tail = 0\n","- Probability of head = 0.7\n","\n","Then:\n","\n","$$\n","X \\sim \\text{Bernoulli}(0.7)\n","$$\n","\n","Each flip gives:\n","- 1 (70% of the time)\n","- 0 (30% of the time)\n","\n","\n","**Example B: Light Switch**\n","\n","- ON = 1\n","- OFF = 0\n","\n","No probability involved here unless randomness is added.\n","Bernoulli adds randomness with control.\n","\n","### Bernoulli in Neural Networks\n","\n","In dropout:\n","\n","- Each neuron is treated like a light switch\n","- ON = neuron kept\n","- OFF = neuron dropped\n","\n","The switch is flipped randomly using Bernoulli.\n","\n","For each neuron:\n","\n","$$\n","r \\sim \\text{Bernoulli}(p)\n","$$\n","\n","Meaning:\n","- r = 1 → neuron survives\n","- r = 0 → neuron removed\n","\n","This happens:\n","- For every neuron\n","- For every training batch\n","- Independently\n","\n","\n","**Manual Dropout Example Using Bernoulli**\n","\n","Layer output:\n","\n","$h = [5, 3, 7, 1]$\n","\n","Keep probability:\n","\n","$p = 0.5$\n","\n","Bernoulli draws:\n","\n","$r = [1, 0, 1, 0]$\n","\n","Apply dropout:\n","\n","$$\n","\\tilde{h} = h \\odot r\n","$$\n","\n","Result:\n","\n","$$[5, 0, 7, 0]$$\n","\n","\n","---"]},{"cell_type":"markdown","id":"2cf2c01b","metadata":{"papermill":{"duration":0.004879,"end_time":"2026-01-22T18:02:13.587844","exception":false,"start_time":"2026-01-22T18:02:13.582965","status":"completed"},"tags":[]},"source":["### Step 3: The Core Equation\n","\n","$$\n","\\tilde{h} = h \\cdot r\n","$$\n","\n","This is not fancy math.\n","\n","It literally means:\n","\n","- If r = 1 → output stays the same\n","- If r = 0 → output becomes zero\n"]},{"cell_type":"markdown","id":"b651e81a","metadata":{"papermill":{"duration":0.008414,"end_time":"2026-01-22T18:02:13.60297","exception":false,"start_time":"2026-01-22T18:02:13.594556","status":"completed"},"tags":[]},"source":["### Step 4: Manual Single-Neuron Example\n","\n","Assume:\n","- Neuron output: $h = 6$\n","- Keep probability: $p = 0.5$\n","\n","Possible outcomes:\n","\n","| r | Calculation | Output |\n","|---|------------|--------|\n","| 1 | 6 × 1 | 6 |\n","| 0 | 6 × 0 | 0 |\n","\n","So during training:\n","- Sometimes the neuron exists\n","- Sometimes it vanishes\n"]},{"cell_type":"markdown","id":"2419ecc7","metadata":{"papermill":{"duration":0.004551,"end_time":"2026-01-22T18:02:13.619028","exception":false,"start_time":"2026-01-22T18:02:13.614477","status":"completed"},"tags":[]},"source":["### Step 5: Manual Multi-Neuron Example (Critical)\n","\n","Assume a layer output:\n","\n","$h = [4, 2, 8, 6]$\n","\n","Let $p = 0.5$\n","\n","Random Bernoulli mask:\n","\n","$r = [1, 0, 1, 0]$\n","\n","Apply dropout:\n","\n","$$\n","\\tilde{h} = h \\odot r\n","$$\n","\n","Result:\n","\n","$$[4, 0, 8, 0]$$\n","\n","Half the neurons are removed for this step only.\n"]},{"cell_type":"markdown","id":"428cb6e2","metadata":{"papermill":{"duration":0.006511,"end_time":"2026-01-22T18:02:13.6306","exception":false,"start_time":"2026-01-22T18:02:13.624089","status":"completed"},"tags":[]},"source":["### Why Scaling Is Required\n","\n","If we randomly drop neurons, the expected output magnitude decreases.\n","\n","Without correction:\n","\n","- Training sees smaller activations\n","- Inference sees larger activations\n","- Model breaks\n","\n","### Expected Value Explanation\n","\n","Original expected output:\n","\n","$$E[h] = h$$\n","\n","After dropout (no scaling):\n","\n","$$\n","E[\\tilde{h}] = p \\cdot h + (1 - p) \\cdot 0\n","$$\n","\n","$$\n","E[\\tilde{h}] = p \\cdot h\n","$$\n","\n","So magnitude shrinks by factor p.\n","\n","**Concrete Numeric Example**\n","\n","Let:\n","- h = 10\n","- p = 0.5\n","\n","Expected value:\n","\n","$$\n","E[\\tilde{h}] = 0.5 × 10 = 5\n","$$\n","Meaning:\n","\n","- During training, neuron contributes half as much on average\n","\n","\n","**Why This Is a Problem**\n","\n","During training:\n","- Network learns using average signal ≈ 5\n","\n","During inference (no dropout):\n","- All neurons active\n","- Output = 10\n","\n","Distribution mismatch:\n","- Training sees small activations\n","- Inference sees larger activations\n","- Leads to unstable predictions\n","\n","### Inverted Dropout: The Fix (Used in PyTorch)\n","\n","Instead of scaling at inference, we scale during training.\n","\n","$$\n","\\tilde{h} = \\frac{h \\cdot r}{p}\n","$$\n","\n","\n","**Expected Value With Inverted Dropout**\n","\n","Possible outcomes now:\n","\n","\n","| r | Probability | Output |\n","|--|--|--|\n","| 1 | $p$ | $h / p$ |\n","| 0 | $1 − p$ | 0 |\n","\n","**Expected Value Calculation**\n","\n","$$\n","E[\\tilde{h}] = p \\cdot \\frac{h}{p} + (1 - p) \\cdot 0\n","$$\n","\n","$$\n","E[\\tilde{h}] = h\n","$$\n","\n","Matches original neuron output\n","\n","**Numeric Example**\n","\n","Let:\n","- $h = 10$\n","- $p = 0.5$\n","\n","Case 1: \n","\n","$r = 1$  \n","$Output = 10 / 0.5 = 20$\n","\n","Case 2: \n","\n","$r = 0$  \n","$Output = 0$  \n","\n","Expected value:\n","\n","$0.5 × 20 + 0.5 × 0 = 10$ \n","\n","\n","### What “Outputs Are Scaled Automatically” Really Means\n","\n","When you write:\n","\n","`nn.Dropout(p=0.5)`\n","\n","PyTorch:\n","- Applies Bernoulli mask\n","- Divides by p during training\n","- Does NOTHING during inference\n","\n","You never see the scaling, but it’s there.\n"]},{"cell_type":"markdown","id":"93c267b0","metadata":{"papermill":{"duration":0.009369,"end_time":"2026-01-22T18:02:13.645442","exception":false,"start_time":"2026-01-22T18:02:13.636073","status":"completed"},"tags":[]},"source":["## 3️.  How Dropout Actually Helps\n","\n","Dropout exists to prevent co-adaptation between neurons.\n","\n","Co-adaptation happens when:\n","- Neuron A becomes useful only because neuron B exists\n","- Neuron B depends on neuron A to work correctly\n","\n","This creates fragile feature learning.\n","\n","If either neuron fails, the prediction collapses.\n","\n","\n","### What Dropout Does During Training\n","\n","During every training step:\n","\n","- Random neurons are temporarily removed\n","- The network structure changes every batch\n","- Forward and backward passes use a different sub-network\n","\n","Example:\n","\n","- Batch 1: Neurons A, C active\n","- Batch 2: Neurons B, D active\n","- Batch 3: Neurons A, B active\n","\n","No neuron is guaranteed to be present."]},{"cell_type":"markdown","id":"84102760","metadata":{"papermill":{"duration":0.006904,"end_time":"2026-01-22T18:02:13.657575","exception":false,"start_time":"2026-01-22T18:02:13.650671","status":"completed"},"tags":[]},"source":["### Why This Is Equivalent to Training Many Sub-Networks\n","\n","Because neurons are randomly removed:\n","- The model never trains as a single fixed architecture\n","- It trains thousands of smaller networks\n","- All networks share the same weights\n","\n","This behaves like an ensemble:\n","- But without training separate models\n","- And without extra memory cost\n","\n","### Why Neurons Become More Robust\n","\n","Since any neuron can disappear:\n","- No neuron can rely on a specific partner\n","- Each neuron must learn independently useful features\n","\n","Instead of learning:\n","\n","Feature = Neuron A AND Neuron B\n","\n","The network learns:\n","\n","Feature = Neuron A OR Neuron B OR Neuron C\n","\n","This creates redundancy."]},{"cell_type":"markdown","id":"4ddc4e80","metadata":{"papermill":{"duration":0.007975,"end_time":"2026-01-22T18:02:13.670241","exception":false,"start_time":"2026-01-22T18:02:13.662266","status":"completed"},"tags":[]},"source":["### What This Achieves\n","\n","Dropout forces the model to learn:\n","- Multiple ways to represent the same pattern\n","- Backup features instead of brittle shortcuts\n","\n","Results:\n","- Better generalization\n","- Reduced overfitting\n","- More stable performance on unseen data"]},{"cell_type":"markdown","id":"a74b78a4","metadata":{"papermill":{"duration":0.009577,"end_time":"2026-01-22T18:02:13.685389","exception":false,"start_time":"2026-01-22T18:02:13.675812","status":"completed"},"tags":[]},"source":["## 4️. Why Dropout Is Tricky in CNNs\n","\n","Dropout behaves very differently in CNNs compared to fully connected networks. This is not accidental, it comes from how CNNs represent information.\n","\n","### How CNNs Represent Information\n","\n","CNNs rely on three structural ideas:\n","\n","- **Local spatial correlations**  \n","  Nearby pixels (or tokens) are strongly related.\n","\n","- **Shared convolutional filters**  \n","  The same filter detects the same pattern everywhere.\n","\n","- **Structured feature maps**  \n","  Activations form grids (height × width × channels), not flat vectors.\n","\n","This structure is the strength of CNNs.\n","\n","### What Dropout Does That Causes Trouble\n","\n","Dropout randomly removes individual activations.\n","\n","In early convolution layers, this means:\n","- Random pixels in feature maps are erased\n","- Local continuity is broken\n","- Partial edges or textures disappear\n","\n","**Example (edge detection):**\n","\n","Original feature map:\n","\n","████████<br>\n","████████<br>\n","████████\n","\n","After dropout:\n","\n","███ ███<br>\n","█ █████<br>\n","████ ██\n","\n","Edges become fragmented.\n","\n","\n","### Why This Hurts Early CNN Layers\n","\n","Early convolution layers learn:\n","- Edges\n","- Corners\n","- Textures\n","- Simple shapes\n","\n","These features require spatial consistency.\n","\n","Dropping random neurons early:\n","- Destroys local patterns\n","- Makes filters harder to learn\n","- Slows convergence\n","- Reduces representation quality\n","\n","In short:\n","> Dropout fights against what early CNN layers are trying to learn.\n","\n","### Why Dropout Works Better in Later Layers\n","\n","Later CNN layers (especially fully connected layers):\n","- Represent abstract concepts\n","- No longer depend on precise spatial layout\n","- Behave like standard dense networks\n","\n","Examples:\n","- “Catness”\n","- “Face-like structure”\n","- “Positive sentiment”\n","\n","Here:\n","- Co-adaptation becomes a real risk\n","- Dropout helps prevent over-reliance on specific neurons\n","\n","\n","### Practical Rule Used in Real Systems\n","\n","- Avoid dropout in early convolution layers\n","- Dropout is usually NOT needed in convolutional layers and if you use it, use very little, very carefully.\n","- Use dropout in:\n","  - Fully connected layers\n","  - Classification heads\n","  - Dense decision layers\n","\n","\n","### Industry Reality Check\n","\n","Modern CNN architectures often:\n","- Use Batch Normalization instead of dropout\n","- Use data augmentation for regularization\n","- Apply dropout only near the output\n","\n","That’s why you rarely see heavy dropout in ResNet, EfficientNet, etc.\n","\n","\n","\n","> CNNs depend on spatial structure. Dropout destroys spatial structure.\n","\n","So:\n","\n","> Dropout is a poor regularizer for early CNN layers but a good regularizer for dense decision layers.\n"]},{"cell_type":"markdown","id":"08e9e428","metadata":{"papermill":{"duration":0.0065,"end_time":"2026-01-22T18:02:13.702557","exception":false,"start_time":"2026-01-22T18:02:13.696057","status":"completed"},"tags":[]},"source":["## 5️. Where to Use Dropout in CNN\n","\n","- After Fully Connected (FC) layers\n","- After Global Average Pooling\n","- Late-stage convolution blocks (light dropout)\n","\n","Avoid\n","- First conv layer\n","- Aggressive dropout in early feature extraction\n","\n","Typical Values\n","\n","| Layer Type | Dropout Rate |\n","|----------|-------------|\n","| FC Layers | 0.3 – 0.5 |\n","| Conv Blocks | 0.1 – 0.3 |\n"]},{"cell_type":"markdown","id":"1a92479b","metadata":{"papermill":{"duration":0.004833,"end_time":"2026-01-22T18:02:13.712982","exception":false,"start_time":"2026-01-22T18:02:13.708149","status":"completed"},"tags":[]},"source":["## 7. Example: Dropout in a CNN"]},{"cell_type":"code","execution_count":1,"id":"49ffcf7b","metadata":{"execution":{"iopub.execute_input":"2026-01-22T18:02:13.724961Z","iopub.status.busy":"2026-01-22T18:02:13.724606Z","iopub.status.idle":"2026-01-22T18:02:18.994159Z","shell.execute_reply":"2026-01-22T18:02:18.993107Z"},"papermill":{"duration":5.278157,"end_time":"2026-01-22T18:02:18.996521","exception":false,"start_time":"2026-01-22T18:02:13.718364","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Define a simple CNN model\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        # Convolutional layer:\n","        # Input: 1 channel (grayscale), Output: 16 channels, Kernel: 3x3\n","        # No padding, so output spatial size = (28-3+1)=26 # assuming input 28x28\n","        self.conv = nn.Conv2d(1, 16, 3)\n","        \n","        # Dropout layer for regularization\n","        # Randomly zeroes 30% of neurons during training\n","        self.dropout = nn.Dropout(p=0.3)\n","        \n","        # Fully connected layer:\n","        # Input features = 16 channels * 26 * 26 pixels (flattened)\n","        # Output features = 10 classes\n","        self.fc = nn.Linear(16*26*26, 10)  \n","\n","    def forward(self, x):\n","        # Apply convolution\n","        x = self.conv(x)\n","        \n","        # Apply ReLU activation function\n","        # F.relu is functional (stateless) version\n","        x = F.relu(x)\n","        \n","        # Flatten 4D tensor (B, C, H, W) -> 2D tensor (B, features)\n","        # Necessary for feeding into fully connected layer\n","        x = x.view(x.size(0), -1)\n","        \n","        # Apply dropout (only active during training)\n","        x = self.dropout(x)\n","        \n","        # Fully connected layer to produce logits for 10 classes\n","        x = self.fc(x)\n","        \n","        return x\n"]},{"cell_type":"markdown","id":"4116894a","metadata":{"papermill":{"duration":0.004662,"end_time":"2026-01-22T18:02:19.005714","exception":false,"start_time":"2026-01-22T18:02:19.001052","status":"completed"},"tags":[]},"source":["Dropout:\n","\n","- `ON` during `model.train()`\n","- `OFF`during `model.eval()`"]},{"cell_type":"markdown","id":"f69bb670","metadata":{"papermill":{"duration":0.003967,"end_time":"2026-01-22T18:02:19.013786","exception":false,"start_time":"2026-01-22T18:02:19.009819","status":"completed"},"tags":[]},"source":["# Batch Normalization in CNN\n","\n","**Batch Normalization (BatchNorm)** is a technique to normalize the inputs of each layer in a neural network. It is widely used in CNNs to stabilize and accelerate training.\n","\n","For an input activation $x$ in a mini-batch:\n","\n","$$\n","\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n","$$\n","\n","Where:  \n","- $\\mu$ = mean of the mini-batch  \n","- $\\sigma^2$ = variance of the mini-batch  \n","- $\\epsilon$ = small constant to avoid division by zero\n","\n","After normalization, activations have zero mean and unit variance, which can limit the network’s expressive power. BatchNorm introduces two learnable parameters $\\gamma$ and $\\beta$ to allow the network to scale and shift the normalized values:\n","\n","- $\\gamma$ controls the **strength (variance)** of a feature  \n","- $\\beta$ controls the **position (mean)** of a feature  \n","- They allow BatchNorm to represent the identity function if needed  \n","- Prevent normalization from restricting what the network can learn  \n","\n","Without scale and shift, BatchNorm would stabilize training but reduce model capacity.\n","\n","$$\n","y = \\gamma \\hat{x} + \\beta\n","$$\n","\n","- $\\gamma$ = scale factor  \n","- $\\beta$ = shift factor  \n","\n","This ensures the network can still represent the identity transformation if needed.\n","\n"]},{"cell_type":"markdown","id":"9b54b9dd","metadata":{"papermill":{"duration":0.005307,"end_time":"2026-01-22T18:02:19.024081","exception":false,"start_time":"2026-01-22T18:02:19.018774","status":"completed"},"tags":[]},"source":["## 1. Manual Example\n","\n","Consider a mini-batch of 4 activations from a single neuron/channel:\n","\n","$$\n","x = [2, 4, 6, 8]\n","$$\n","\n","We'll apply Batch Normalization with a small $\\epsilon = 10^{-5}$, and assume learnable parameters:\n","\n","$$\n","\\gamma = 2, \\quad \\beta = 1\n","$$\n","\n","### Step 1: Compute Mini-Batch Mean\n","\n","The mean $\\mu$ is:\n","\n","$$\n","\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i\n","$$\n","\n","Here, $N=4$:\n","\n","$$\n","\\mu = \\frac{2 + 4 + 6 + 8}{4} = \\frac{20}{4} = 5\n","$$\n"]},{"cell_type":"markdown","id":"fb9dbb60","metadata":{"papermill":{"duration":0.004111,"end_time":"2026-01-22T18:02:19.032393","exception":false,"start_time":"2026-01-22T18:02:19.028282","status":"completed"},"tags":[]},"source":["### Step 2: Compute Mini-Batch Variance\n","\n","The variance $\\sigma^2$ is:\n","\n","$$\n","\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu)^2\n","$$\n","\n","$$\n","\\sigma^2 = \\frac{(2-5)^2 + (4-5)^2 + (6-5)^2 + (8-5)^2}{4}\n","$$\n","\n","$$\n","\\sigma^2 = \\frac{(-3)^2 + (-1)^2 + 1^2 + 3^2}{4} = \\frac{9 + 1 + 1 + 9}{4} = \\frac{20}{4} = 5\n","$$\n"]},{"cell_type":"markdown","id":"ecbfa512","metadata":{"papermill":{"duration":0.00429,"end_time":"2026-01-22T18:02:19.041213","exception":false,"start_time":"2026-01-22T18:02:19.036923","status":"completed"},"tags":[]},"source":["### Step 3: Normalize the Activations\n","\n","Normalized activations $\\hat{x}$:\n","\n","$$\n","\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n","$$\n","\n","$$\n","\\hat{x} = \\frac{[2,4,6,8] - 5}{\\sqrt{5 + 10^{-5}}} \\approx \\frac{[-3, -1, 1, 3]}{\\sqrt{5}} \\approx [-1.34, -0.45, 0.45, 1.34]\n","$$\n"]},{"cell_type":"markdown","id":"0c7b0937","metadata":{"papermill":{"duration":0.003898,"end_time":"2026-01-22T18:02:19.049094","exception":false,"start_time":"2026-01-22T18:02:19.045196","status":"completed"},"tags":[]},"source":["### Step 4: Scale and Shift with $\\gamma$ and $\\beta$\n","\n","Finally, apply learnable parameters:\n","\n","$$\n","y_i = \\gamma \\hat{x}_i + \\beta\n","$$\n","\n","$$\n","y = 2 \\cdot [-1.34, -0.45, 0.45, 1.34] + 1 \\approx [-1.68, 0.1, 1.9, 3.68]\n","$$\n"]},{"cell_type":"markdown","id":"c816266a","metadata":{"papermill":{"duration":0.003964,"end_time":"2026-01-22T18:02:19.05709","exception":false,"start_time":"2026-01-22T18:02:19.053126","status":"completed"},"tags":[]},"source":["### Result\n","\n","Original mini-batch: $[2, 4, 6, 8]$  \n","Normalized & scaled mini-batch: $[-1.68, 0.1, 1.9, 3.68]$\n","\n","- The activations are now centered, scaled, and shifted.\n","- BatchNorm has stabilized the input distribution while keeping learnable flexibility.\n"]},{"cell_type":"markdown","id":"5ec44501","metadata":{"papermill":{"duration":0.004315,"end_time":"2026-01-22T18:02:19.065341","exception":false,"start_time":"2026-01-22T18:02:19.061026","status":"completed"},"tags":[]},"source":["## 2. How BatchNorm Works\n","\n","1. Normalizes activations across the mini-batch.  \n","2. Centers and scales each feature to have zero mean and unit variance.  \n","3. Introduces learnable parameters ($\\gamma$ and $\\beta$) to retain representational flexibility.  \n","4. Integrates seamlessly with convolutional layers by normalizing across channels for each spatial location.\n"]},{"cell_type":"markdown","id":"73725fa6","metadata":{"papermill":{"duration":0.00406,"end_time":"2026-01-22T18:02:19.0737","exception":false,"start_time":"2026-01-22T18:02:19.06964","status":"completed"},"tags":[]},"source":["## 3. Benefits of BatchNorm\n","\n","1. **Stabilizes Gradients**  \n","   - By keeping activations in a consistent range, gradients do not explode or vanish.  \n","   - This makes deeper networks trainable.\n","\n","2. **Allows Higher Learning Rates**  \n","   - Reduces the risk of divergence, enabling faster convergence.\n","\n","3. **Reduces Internal Covariate Shift**  \n","   - The distribution of inputs to each layer becomes more stable during training, which improves learning efficiency.\n","\n","4. **Acts as a Mild Regularizer**  \n","   - Slight noise from mini-batch statistics reduces overfitting, sometimes reducing the need for dropout.\n","\n","5. **Improves Generalization**  \n","   - Normalization smoothens the optimization landscape, making training more robust.\n"]},{"cell_type":"markdown","id":"1d2e3a3d","metadata":{"papermill":{"duration":0.003936,"end_time":"2026-01-22T18:02:19.081653","exception":false,"start_time":"2026-01-22T18:02:19.077717","status":"completed"},"tags":[]},"source":["## 4. CNN Batch Normalization Placement\n","\n","The correct and most common placement of BatchNorm in CNNs is:\n","\n","$$\n","\\text{Conv} \\;\\rightarrow\\; \\text{BatchNorm} \\;\\rightarrow\\; \\text{Activation}\n","$$\n","\n","Example with ReLU:\n","\n","$$\n","y = \\text{ReLU}(\\text{BN}(\\text{Conv}(x)))\n","$$\n","\n","This is the default choice in modern CNN architectures.\n"]},{"cell_type":"markdown","id":"44e75e96","metadata":{"papermill":{"duration":0.004158,"end_time":"2026-01-22T18:02:19.089913","exception":false,"start_time":"2026-01-22T18:02:19.085755","status":"completed"},"tags":[]},"source":["<p style=\"text-align:center; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":11.54889,"end_time":"2026-01-22T18:02:21.662803","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-01-22T18:02:10.113913","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}