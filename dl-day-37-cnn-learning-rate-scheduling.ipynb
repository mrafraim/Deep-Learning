{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-37-cnn-learning-rate-scheduling?scriptVersionId=299228868\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"902bb410","metadata":{"papermill":{"duration":0.007536,"end_time":"2026-02-22T04:57:25.026473","exception":false,"start_time":"2026-02-22T04:57:25.018937","status":"completed"},"tags":[]},"source":["# Day 37: CNN Learning Rate Scheduling\n","StepLR · ReduceLROnPlateau · CosineAnnealing · LR Phase Intuition\n","\n","Welcome to Day 37!\n","\n","What You’ll Learn Today:\n","\n","1. Why fixed learning rates are suboptimal\n","2. Learning rate decay intuition\n","3. StepLR\n","4. ReduceLROnPlateau\n","5. CosineAnnealing\n","6. When to use which\n","\n","If you found this notebook helpful, your **<b style=\"color:skyblue;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"9794dede","metadata":{"papermill":{"duration":0.006021,"end_time":"2026-02-22T04:57:25.040894","exception":false,"start_time":"2026-02-22T04:57:25.034873","status":"completed"},"tags":[]},"source":["# Why Fixed Learning Rate Fails\n","\n","Recall the basic gradient descent update:\n","\n","$$\n","\\boxed{\\theta_{t+1} = \\theta_t - \\alpha \\nabla L(\\theta_t)}\n","$$\n","\n","Where:\n","- $\\theta_t$ → current parameters  \n","- $\\alpha$ → learning rate  \n","- $\\nabla L$ → gradient (direction of steepest increase)\n","\n","We subtract the gradient to move **downhill**."]},{"cell_type":"markdown","id":"8e32922c","metadata":{"papermill":{"duration":0.006217,"end_time":"2026-02-22T04:57:25.053247","exception":false,"start_time":"2026-02-22T04:57:25.04703","status":"completed"},"tags":[]},"source":["## What the Learning Rate Really Controls\n","\n","The learning rate $\\alpha$ determines **step size**.\n","\n","It answers:\n","\n","> “How far should we move in the gradient direction?”"]},{"cell_type":"markdown","id":"db578748","metadata":{"papermill":{"duration":0.006078,"end_time":"2026-02-22T04:57:25.065371","exception":false,"start_time":"2026-02-22T04:57:25.059293","status":"completed"},"tags":[]},"source":["## If $\\alpha$ Is Large\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\textbf{large step}\n","$$\n","\n","### Pros\n","- Fast movement across loss surface  \n","- Escapes sharp local minima  \n","- Explores wider regions  \n","\n","### Cons\n","- Can overshoot minima  \n","- Oscillates around optimum  \n","- May diverge  \n","\n","Large LR is good for:\n","- Early training\n","- Rough exploration"]},{"cell_type":"markdown","id":"9c02c4b6","metadata":{"papermill":{"duration":0.006115,"end_time":"2026-02-22T04:57:25.077646","exception":false,"start_time":"2026-02-22T04:57:25.071531","status":"completed"},"tags":[]},"source":["## If $\\alpha$ Is Small\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\textbf{small step}\n","$$\n","\n","### Pros\n","- Stable convergence  \n","- Precise fine-tuning  \n","- Less oscillation  \n","\n","### Cons\n","- Very slow training  \n","- Can get stuck in sharp minima  \n","- Poor exploration  \n","\n","Small LR is good for:\n","- Late training\n","- Fine-grained optimization"]},{"cell_type":"markdown","id":"8f65ff95","metadata":{"papermill":{"duration":0.006119,"end_time":"2026-02-22T04:57:25.089961","exception":false,"start_time":"2026-02-22T04:57:25.083842","status":"completed"},"tags":[]},"source":["## Core Problem\n","\n","Training has two different phases:\n","\n","1️. Exploration phase  \n","2️. Refinement phase  \n","\n","A single fixed $\\alpha$ cannot optimize both:\n","\n","- If large → unstable at end\n","- If small → painfully slow at start "]},{"cell_type":"markdown","id":"9cb602fd","metadata":{"papermill":{"duration":0.006089,"end_time":"2026-02-22T04:57:25.102267","exception":false,"start_time":"2026-02-22T04:57:25.096178","status":"completed"},"tags":[]},"source":["## Geometric Intuition\n","\n","Think of descending a mountain:\n","\n","- At the top → you want big jumps  \n","- Near the bottom → you want tiny careful steps  \n","\n","Using one fixed step size is inefficient."]},{"cell_type":"markdown","id":"ea6aa4a6","metadata":{"papermill":{"duration":0.006237,"end_time":"2026-02-22T04:57:25.114662","exception":false,"start_time":"2026-02-22T04:57:25.108425","status":"completed"},"tags":[]},"source":["## Strategic Conclusion\n","\n","> Learning rate must change over time.\n","\n","That’s why we use:\n","- Step decay  \n","- Exponential decay  \n","- Cosine annealing  \n","- OneCycle  \n","- Warmup schedules  \n","\n","Fixed LR is simple but strategically weak for deep networks."]},{"cell_type":"markdown","id":"4918c464","metadata":{"papermill":{"duration":0.006035,"end_time":"2026-02-22T04:57:25.127","exception":false,"start_time":"2026-02-22T04:57:25.120965","status":"completed"},"tags":[]},"source":["# Optimization Has Distinct Phases\n","\n","Training a neural network is **not uniform**.  \n","The geometry of the loss surface and gradient magnitudes change over time."]},{"cell_type":"markdown","id":"94f496cf","metadata":{"papermill":{"duration":0.005982,"end_time":"2026-02-22T04:57:25.139011","exception":false,"start_time":"2026-02-22T04:57:25.133029","status":"completed"},"tags":[]},"source":["## 1️. Exploration Phase (Early Training)\n","\n","- Loss decreases rapidly  \n","- Gradients are large  \n","- Parameters are far from optimum  \n","\n","Update rule:\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\nabla L(\\theta_t)\n","$$\n","\n","Since $\\|\\nabla L\\|$ is large:\n","\n","- Large $\\alpha$ → fast movement across surface  \n","- Helps escape sharp or poor local regions  \n","- Encourages broader exploration  \n","\n","Goal here:\n","> Move quickly toward a promising basin."]},{"cell_type":"markdown","id":"aa656103","metadata":{"papermill":{"duration":0.006057,"end_time":"2026-02-22T04:57:25.151273","exception":false,"start_time":"2026-02-22T04:57:25.145216","status":"completed"},"tags":[]},"source":["## 2️. Transition Phase (Approaching Minimum)\n","\n","- Parameters enter a valley (basin)  \n","- Gradients shrink but fluctuate  \n","- Curvature becomes important  \n","\n","If $\\alpha$ remains large:\n","\n","- Updates overshoot the minimum  \n","- Oscillations occur across valley walls  \n","\n","Mathematically:\n","\n","Large $\\alpha$ × small but varying $\\nabla L$ → unstable zig-zag behavior  \n","\n","Goal here:\n","> Reduce step size to stabilize descent.\n"]},{"cell_type":"markdown","id":"079c785b","metadata":{"papermill":{"duration":0.005991,"end_time":"2026-02-22T04:57:25.163414","exception":false,"start_time":"2026-02-22T04:57:25.157423","status":"completed"},"tags":[]},"source":["## 3️. Fine Convergence Phase (Late Training)\n","\n","- Gradients are small  \n","- Surface curvature dominates  \n","- Small adjustments refine solution  \n","\n","Now we need:\n","\n","$$\n","\\alpha \\text{ very small}\n","$$\n","\n","Why?\n","\n","Because near minimum:\n","\n","$$\n","\\nabla L \\approx 0\n","$$\n","\n","Large $\\alpha$ would:\n","- Bounce around minimum  \n","- Prevent precise convergence  \n","\n","Goal here:\n","> Minimize noise and fine-tune parameters."]},{"cell_type":"markdown","id":"89d90d66","metadata":{"papermill":{"duration":0.005992,"end_time":"2026-02-22T04:57:25.175468","exception":false,"start_time":"2026-02-22T04:57:25.169476","status":"completed"},"tags":[]},"source":["## Fixed Learning Rate Fails\n","\n","A single $\\alpha$ cannot satisfy all phases:\n","\n","| Phase | Ideal LR |\n","|-------|----------|\n","| Exploration | Large |\n","| Transition | Medium |\n","| Fine Convergence | Small |\n","\n","If LR is fixed:\n","\n","- Large LR → never truly settles  \n","- Small LR → wastes early training potential  "]},{"cell_type":"markdown","id":"c97deadb","metadata":{"papermill":{"duration":0.006007,"end_time":"2026-02-22T04:57:25.187428","exception":false,"start_time":"2026-02-22T04:57:25.181421","status":"completed"},"tags":[]},"source":["## Core Insight\n","\n","Optimization is **dynamic**.\n","\n","The learning rate should:\n","- Start large  \n","- Gradually decrease  \n","- Become very small near convergence  \n","\n","That’s why we use:\n","\n","- Step decay  \n","- Cosine annealing  \n","- OneCycle policy  \n","- Warmup + decay  \n","\n","> No scheduling = stuck oscillating near the minimum instead of converging smoothly."]},{"cell_type":"markdown","id":"8926667a","metadata":{"papermill":{"duration":0.006023,"end_time":"2026-02-22T04:57:25.199552","exception":false,"start_time":"2026-02-22T04:57:25.193529","status":"completed"},"tags":[]},"source":["# What is Learning Rate Scheduling?\n","\n","**Learning rate scheduling** is the strategy of changing the learning rate during training instead of keeping it constant.\n","\n","Instead of using a fixed learning rate:\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\nabla L\n","$$\n","\n","we use a time-dependent learning rate:\n","\n","$$\n","\\boxed{\\theta_{t+1} = \\theta_t - \\alpha_t \\nabla L}\n","$$\n","\n","where $ \\alpha_t $ changes over time.\n"]},{"cell_type":"markdown","id":"2363cf9b","metadata":{"papermill":{"duration":0.006198,"end_time":"2026-02-22T04:57:25.211835","exception":false,"start_time":"2026-02-22T04:57:25.205637","status":"completed"},"tags":[]},"source":["## Why Do We Need It?\n","\n","Training usually happens in different phases:\n","\n","### 1️. Early Phase > Exploration\n","- Loss decreases rapidly  \n","- Gradients are large  \n","- Larger learning rate helps move quickly  \n","\n","### 2️. Middle Phase > Transition\n","- Approaching a good region  \n","- Oscillations may start  \n","- Learning rate should reduce  \n","\n","### 3️. Final Phase > Fine Convergence\n","- Very small improvements  \n","- Requires precise adjustments  \n","- Small learning rate is necessary  \n","\n","A single fixed learning rate cannot work optimally across all these phases."]},{"cell_type":"markdown","id":"2f2ea0f3","metadata":{"papermill":{"duration":0.006203,"end_time":"2026-02-22T04:57:25.224462","exception":false,"start_time":"2026-02-22T04:57:25.218259","status":"completed"},"tags":[]},"source":["## What Scheduling Does\n","\n","Learning rate scheduling:\n","\n","- Starts large → enables fast progress  \n","- Gradually reduces → improves stability  \n","- Prevents oscillation near minimum  \n","- Often improves final accuracy  \n"]},{"cell_type":"markdown","id":"0e0ccedf","metadata":{"papermill":{"duration":0.006164,"end_time":"2026-02-22T04:57:25.236936","exception":false,"start_time":"2026-02-22T04:57:25.230772","status":"completed"},"tags":[]},"source":["## Simple Example\n","\n","### Without Scheduling\n","\n","LR = 0.01 for all 30 epochs\n","\n","Result:\n","- Fast initial progress  \n","- Later oscillation around the minimum  \n","\n","### With Scheduling\n","\n","Epoch 1–10   → 0.01<br>\n","Epoch 11–20  → 0.001<br>\n","Epoch 21–30  → 0.0001\n","\n","Result:\n","- Early: Large steps  \n","- Later: Small, precise steps  \n","- Smoother convergence  "]},{"cell_type":"markdown","id":"356f4fb6","metadata":{"papermill":{"duration":0.006105,"end_time":"2026-02-22T04:57:25.24918","exception":false,"start_time":"2026-02-22T04:57:25.243075","status":"completed"},"tags":[]},"source":["## Common Types of LR Scheduling\n","\n","- Step Decay  \n","- Exponential Decay  \n","- Cosine Annealing  \n","- Reduce on Plateau  \n","- Warmup + Decay  \n","- Cyclical Learning Rate  \n","\n","> Learning rate scheduling is the technique of dynamically adjusting the learning rate during training to improve convergence speed and stability."]},{"cell_type":"markdown","id":"d5e836f6","metadata":{"papermill":{"duration":0.006041,"end_time":"2026-02-22T04:57:25.261347","exception":false,"start_time":"2026-02-22T04:57:25.255306","status":"completed"},"tags":[]},"source":["# StepLR - Discrete Learning Rate Decay\n","\n","StepLR is a learning rate scheduler that reduces the learning rate by a fixed factor (`gamma`) after a fixed number of epochs (`step_size`).\n","\n","It implements **piecewise-constant decay***.\n","\n","Instead of smoothly decreasing the learning rate, it drops it in sudden steps.\n","\n","### StepLR Formula\n","\n","$$\n","\\boxed{\\alpha_t = \\alpha_0 \\cdot \\gamma^{\\left\\lfloor \\frac{t}{\\text{step\\_size}} \\right\\rfloor}}\n","$$\n","\n","Where:\n","\n","- $\\alpha_t$ → learning rate at epoch $t$\n","- $ t $ → Current epoch  \n","- $\\alpha_0$ → initial learning rate\n","- $\\gamma$ → decay factor (e.g., 0.1)\n","- $\\text{step\\_size}$ → number of epochs before decay\n","- $\\lfloor \\cdot \\rfloor$ → floor function (round down)\n","\n","**Intuition**\n","\n","Think of StepLR as:\n","\n","> \"Train normally for some time → then suddenly reduce step size → repeat.\"\n","\n","It allows:\n","\n","- Fast learning early  \n","- More careful learning later  "]},{"cell_type":"markdown","id":"dd270f5f","metadata":{"papermill":{"duration":0.006101,"end_time":"2026-02-22T04:57:25.273562","exception":false,"start_time":"2026-02-22T04:57:25.267461","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"color:orange;text-align:center;\">*What is Piecewise-Constant Decay? (Optional)</p>\n","\n","**Piecewise-constant decay** is a function that:\n","\n","- Stays constant for a period of time  \n","- Then suddenly drops to a new constant value  \n","- Repeats this pattern  \n","\n","It does not change smoothly.  \n","It changes in discrete jumps.\n","\n","\n","Imagine a staircase:\n","\n","\n","Level 1  ──────────<br>\n","↓<br>\n","Level 2  ──────────<br>\n","↓<br>\n","Level 3  ──────────\n","\n","\n","Each flat region is constant.  \n","Each drop happens at specific intervals.\n","\n","That staircase shape = piecewise-constant behavior.\n","\n","### Mathematical View (StepLR Example)\n","\n","For StepLR:\n","\n","$$\n","\\alpha_t = \\alpha_0 \\cdot \\gamma^{\\left\\lfloor \\frac{t}{\\text{step\\_size}} \\right\\rfloor}\n","$$\n","\n","The learning rate:\n","\n","- Remains constant within each interval  \n","- Changes only when $ t $ crosses multiples of `step_size`\n","\n","### Simple Example\n","\n","Let:\n","\n","- $ \\alpha_0 = 0.01 $\n","- $ \\gamma = 0.1 $\n","- $ \\text{step\\_size} = 5 $\n","\n","Then:\n","\n","| Epoch | Learning Rate |\n","|-------|---------------|\n","| 0–4   | 0.01          |\n","| 5–9   | 0.001         |\n","| 10–14 | 0.0001        |\n","\n","Notice:\n","\n","- Within each range → LR is constant  \n","- At epoch 5 and 10 → LR drops suddenly  \n","\n","That is **piecewise-constant decay**.\n","\n","\n","### Why It's Called \"Piecewise\"\n","\n","- \"Piecewise\" → defined in separate intervals (pieces)\n","- \"Constant\" → value does not change within each interval\n","- \"Decay\" → value decreases over time\n","\n","> Piecewise-constant decay is a step-like schedule where a value remains constant for fixed intervals and then decreases abruptly at predefined points.\n","\n","---"]},{"cell_type":"markdown","id":"9fd19cf1","metadata":{"papermill":{"duration":0.006373,"end_time":"2026-02-22T04:57:25.286115","exception":false,"start_time":"2026-02-22T04:57:25.279742","status":"completed"},"tags":[]},"source":["## What the Floor Function Does\n","\n","$$\n","\\left\\lfloor \\frac{t}{10} \\right\\rfloor\n","$$\n","\n","If `step_size = 10`:\n","\n","| Epoch | t/10 | Floor | Power of γ |\n","|-------|------|-------|------------|\n","| 1–10  | <1   | 0     | γ⁰ = 1 |\n","| 11–20 | 1–2  | 1     | γ¹ |\n","| 21–30 | 2–3  | 2     | γ² |\n","\n","This creates the **staircase pattern**."]},{"cell_type":"markdown","id":"cb3c2610","metadata":{"papermill":{"duration":0.006121,"end_time":"2026-02-22T04:57:25.298396","exception":false,"start_time":"2026-02-22T04:57:25.292275","status":"completed"},"tags":[]},"source":["## Manual Example\n","\n","Given:\n","\n","- $\\alpha_0 = 0.01$\n","- $\\gamma = 0.1$\n","- step_size = 10\n","\n","\n","### Epoch 1:\n","\n","$$\n","\\alpha_1 = 0.01 \\cdot 0.1^{\\lfloor 1/10 \\rfloor}\n","$$\n","\n","$$\n","= 0.01 \\cdot 0.1^0\n","$$\n","\n","$$\n","= 0.01 \\cdot 1\n","$$\n","\n","$$\n","= 0.01\n","$$\n","\n","\n","### Epoch 10:\n","\n","$$\n","\\alpha_{10} = 0.01 \\cdot 0.1^{\\lfloor 10/10 \\rfloor}\n","$$\n","\n","$$\n","= 0.01 \\cdot 0.1^1\n","$$\n","\n","$$\n","= 0.001\n","$$\n","\n","So decay happens right after epoch 10.\n","\n","\n","### Epoch 15:\n","\n","$$\n","\\alpha_{15} = 0.01 \\cdot 0.1^{\\lfloor 15/10 \\rfloor}\n","$$\n","\n","$$\n","= 0.01 \\cdot 0.1^1\n","$$\n","\n","$$\n","= 0.001\n","$$\n","\n","Learning rate remains constant until next boundary.\n","\n","\n","### Epoch 25:\n","\n","$$\n","\\alpha_{25} = 0.01 \\cdot 0.1^{\\lfloor 25/10 \\rfloor}\n","$$\n","\n","$$\n","= 0.01 \\cdot 0.1^2\n","$$\n","\n","$$\n","= 0.0001\n","$$\n","\n","\n","### Final Schedule\n","\n","| Epoch | LR |\n","|--------|------|\n","| 1–9 | 0.01 |\n","| 10–19 | 0.001 |\n","| 20–29 | 0.0001 |\n","\n","Notice the **sudden drops**."]},{"cell_type":"markdown","id":"d2d7e793","metadata":{"papermill":{"duration":0.006266,"end_time":"2026-02-22T04:57:25.311042","exception":false,"start_time":"2026-02-22T04:57:25.304776","status":"completed"},"tags":[]},"source":["## How to Choose `gamma`\n","\n","`gamma` controls how aggressively you reduce the learning rate:\n","\n","$$\n","\\alpha_{\\text{new}} = \\alpha_{\\text{old}} \\cdot \\gamma\n","$$\n","\n","So the real question is:\n","\n","> How big should each decay jump be?\n","\n","Let’s approach this strategically.\n","\n","\n","### 1️. First-Principles View\n","\n","Learning rate controls step size:\n","\n","$$\n","\\Delta \\theta = -\\alpha \\nabla L\n","$$\n","\n","Reducing LR changes:\n","\n","- Exploration intensity  \n","- Convergence speed  \n","- Stability  \n","\n","`gamma` determines how sharply you transition from exploration → refinement.\n","\n","\n","### 2️. Standard Practical Values\n","\n","Most commonly used values:\n","\n","| Gamma | Effect |\n","|-------|--------|\n","| 0.1   | Strong decay (10× smaller) |\n","| 0.5   | Moderate decay (2× smaller) |\n","| 0.8–0.9 | Gentle decay |\n","\n","### Rule of thumb:\n","- Computer vision (ImageNet-style): **0.1**\n","- Smaller datasets: **0.5**\n","- Fine-tuning: **0.1 or smaller**\n","\n","\n","### 3. Strategic Way to Choose Gamma\n","\n","Instead of guessing, think in terms of training phases.\n","\n","Ask:\n","\n","1. How far am I from convergence?\n","2. Do I need sharp stabilization or gradual refinement?\n","\n","#### Case A: Large Model, Large Dataset\n","\n","Use:\n","$$\n","\\gamma = 0.1\n","$$\n","\n","Why?\n","\n","- You want clear phase transitions.\n","- Early phase = aggressive learning.\n","- Later phase = precise convergence.\n","\n","Strong drop is fine because gradients are stable.\n","\n","#### Case B: Small Dataset / Noisy Gradients\n","\n","Use:\n","$$\n","\\gamma = 0.5\n","$$\n","\n","Why?\n","\n","- Large abrupt drops may stall learning.\n","- Gentler decay avoids optimizer shock.\n","\n","#### Case C: Fine-Tuning Pretrained Model\n","\n","Often:\n","$$\n","\\gamma = 0.1 \\text{ or } 0.2\n","$$\n","\n","Because:\n","\n","- You are already near a good minimum.\n","- You want fast stabilization.\n","\n","### 4️. Quantitative Thinking (Better Way)\n","\n","Decide how small you want LR at the end.\n","\n","If:\n","\n","- Initial LR = $0.01$\n","- Final desired LR ≈ $0.0001$\n","- You decay 2 times\n","\n","Then solve:\n","\n","$$\n","0.01 \\cdot \\gamma^2 = 0.0001\n","$$\n","\n","$$\n","\\gamma^2 = 0.01\n","$$\n","\n","$$\n","\\gamma = 0.1\n","$$\n","\n","So instead of guessing gamma,\n","**work backward from desired final LR.**\n","\n","This is the cleanest method.\n","\n","### Warning: Too Small Gamma\n","\n","If:\n","\n","$$\n","\\gamma = 0.01\n","$$\n","\n","Then LR collapses too fast.\n","\n","You lose exploration early.\n","Training may stall.\n","\n","\n","### Warning: Too Large Gamma\n","\n","If:\n","\n","$$\n","\\gamma = 0.9\n","$$\n","\n","Decay barely matters.\n","You may oscillate too long.\n","\n","\n","### Practical Recommendation Framework\n","\n","Choose gamma based on:\n","\n","1. Desired final LR\n","2. Number of decay steps\n","3. Dataset size\n","4. Stability of gradients\n","\n","Most robust default:\n","\n","$$\n","\\gamma = 0.1\n","$$\n","\n","Unless you have reason not to."]},{"cell_type":"markdown","id":"09a00c99","metadata":{"papermill":{"duration":0.006314,"end_time":"2026-02-22T04:57:25.323659","exception":false,"start_time":"2026-02-22T04:57:25.317345","status":"completed"},"tags":[]},"source":["## PyTorch Implementation\n","\n","```python\n","# Create a StepLR scheduler object\n","scheduler = torch.optim.lr_scheduler.StepLR(\n","    optimizer,        # The optimizer whose learning rate we want to control\n","    step_size=10,     # Number of epochs to wait before reducing the learning rate\n","    gamma=0.1         # Multiplicative decay factor (new_lr = old_lr * gamma)\n",")\n","```\n","### What This Actually Means\n","\n","* The learning rate will remain unchanged for 10 epochs.\n","\n","* At epoch 10 → LR becomes:\n","\n","  $$\n","  \\text{LR} = \\text{LR} \\times 0.1\n","  $$\n","\n","* At epoch 20 → it is multiplied by 0.1 again.\n","\n","* This creates the staircase (piecewise-constant) decay pattern.\n","\n","\n","At each epoch:\n","\n","```python\n","# Call this once after each epoch\n","scheduler.step()\n","```\n","\n","### What `scheduler.step()` Does\n","\n","* It updates the learning rate based on the current epoch count.\n","\n","* Internally, it checks:\n","\n","  $$\n","  \\left\\lfloor \\frac{\\text{current\\_epoch}}{\\text{step\\_size}} \\right\\rfloor\n","  $$\n","\n","* If the epoch crosses a decay boundary (10, 20, 30...),\n","  it applies the multiplicative drop.\n","\n","\n","### Important Detail (Often Missed)\n","\n","Typical training loop:\n","\n","```python\n","for epoch in range(num_epochs):\n","    train(...)\n","    validate(...)\n","    scheduler.step()   # Step AFTER each epoch\n","```\n","\n","If you call `scheduler.step()` in the wrong place (e.g., before training),\n","your decay timing shifts by one epoch.\n","\n","**Notes:**\n","\n","* `step_size` controls when decay happens.\n","* `gamma` controls how much decay happens.\n","* `scheduler.step()` triggers the update.\n"]},{"cell_type":"markdown","id":"0595e206","metadata":{"papermill":{"duration":0.006045,"end_time":"2026-02-22T04:57:25.335777","exception":false,"start_time":"2026-02-22T04:57:25.329732","status":"completed"},"tags":[]},"source":["## Why It Works\n","\n","Recall optimization phases:\n","\n","1️. Early → large LR helps exploration<br>\n","2️. Later → smaller LR helps convergence\n","\n","StepLR forces a manual phase transition."]},{"cell_type":"markdown","id":"0a8b0b4a","metadata":{"papermill":{"duration":0.006089,"end_time":"2026-02-22T04:57:25.348044","exception":false,"start_time":"2026-02-22T04:57:25.341955","status":"completed"},"tags":[]},"source":["## Weakness of StepLR\n","\n","Abrupt decay can:\n","\n","* Shock optimizer momentum buffers\n","* Cause temporary instability\n","* Reduce LR too early\n","* Waste exploration capacity\n","\n","Momentum-based optimizers especially feel this shock because:\n","\n","$$\n","\\text{velocity}_{t+1} = \\beta v_t + \\alpha \\nabla L\n","$$\n","\n","Sudden drop in $\\alpha$ changes effective velocity."]},{"cell_type":"markdown","id":"5a9670f3","metadata":{"papermill":{"duration":0.006074,"end_time":"2026-02-22T04:57:25.360265","exception":false,"start_time":"2026-02-22T04:57:25.354191","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\"> Why Abrupt LR Drops Can Be a Problem (Optional)<p>\n","\n","Let’s break this from first principles.\n","\n","1️⃣ **What Momentum Is Really Doing**\n","\n","For momentum SGD:\n","\n","$$\n","v_{t+1} = \\beta v_t + \\nabla L_t\n","$$\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha v_{t+1}\n","$$\n","\n","Key idea:\n","\n","- $v_t$ = **accumulated direction**\n","- $\\alpha$ = **step size multiplier**\n","\n","Momentum builds up velocity over time.  \n","It smooths gradients and accelerates movement in consistent directions.\n","\n","Think of it as a rolling ball gaining speed downhill.\n","\n","\n","2️⃣ **What Happens Before Decay**\n","\n","Suppose:\n","\n","- Learning rate: $\\alpha = 0.01$\n","- Momentum velocity is large (because training has been progressing steadily)\n","\n","Updates look like:\n","\n","$$\n","\\Delta \\theta \\approx -0.01 \\cdot v_t\n","$$\n","\n","The system is tuned to this step size.\n","\n","\n","3️⃣ **Sudden StepLR Drop**\n","\n","At epoch 10:\n","\n","$$\n","\\alpha \\rightarrow 0.001\n","$$\n","\n","But notice something:\n","\n","- $v_t$ is still large (built using old scale)\n","- Only $\\alpha$ changed\n","\n","Now updates become:\n","\n","$$\n","\\Delta \\theta \\approx -0.001 \\cdot v_t\n","$$\n","\n","That is a 10× smaller step instantly.\n","\n","\n","4️⃣ **Why This Feels Like a “Shock”**\n","\n","The optimizer dynamics were stable at:\n","\n","$$\n","\\text{velocity size }  ×  \\text{ learning rate}  =  \\text{effective step}\n","$$\n","\n","\n","Suddenly:\n","\n","- Velocity is large\n","- Learning rate shrinks sharply\n","\n","So:\n","\n","- Effective step collapses\n","- Momentum needs time to re-adjust\n","- Training temporarily slows or becomes inconsistent\n","\n","It’s like:\n","\n","> Driving at 100 km/h and instantly switching to first gear.\n","\n","System needs time to re-balance.\n","\n","5️⃣ **Why Smooth Decay Is More Stable**\n","\n","With smooth decay (e.g., exponential or cosine):\n","\n","$$\n","\\alpha_t = \\alpha_0 e^{-kt}\n","$$\n","\n","Learning rate shrinks gradually.\n","\n","Momentum adapts gradually too.\n","\n","No sudden system shock.\n","\n","\n","6️⃣ **\"Reduce LR Too Early\" Problem**\n","\n","StepLR forces decay at fixed epochs.\n","\n","But what if:\n","\n","- Model is still far from minimum?\n","- Gradients are still large?\n","- Loss is decreasing fast?\n","\n","Then reducing LR early:\n","\n","- Slows exploration\n","- Makes convergence slower than necessary\n","- Traps you in suboptimal regions\n","\n","This is why adaptive schedulers (e.g., ReduceLROnPlateau) often work better.\n","\n","\n","**Simple Mental Model**\n","\n","Without momentum:\n","- StepLR = less problematic\n","\n","With momentum:\n","- You are scaling a moving object suddenly\n","- System needs time to stabilize\n","\n","> StepLR causes sudden learning rate drops that disrupt the balance between accumulated momentum and step size, leading to temporary instability or slowed progress.\n","\n","---"]},{"cell_type":"markdown","id":"07d75d21","metadata":{"papermill":{"duration":0.00614,"end_time":"2026-02-22T04:57:25.37252","exception":false,"start_time":"2026-02-22T04:57:25.36638","status":"completed"},"tags":[]},"source":["### <p style=\"text-align:center; color:orange; font-size:18px;\"> “Shock Optimizer Momentum Buffers” What It Actually Means (Optional)</p>\n","\n","\n","1️⃣ **What Is a Momentum Buffer?**\n","\n","In momentum-based optimizers (SGD with momentum, Adam, etc.),  \n","the optimizer keeps an internal variable:\n","\n","$$\n","v_t\n","$$\n","\n","This is often called the **momentum buffer**.\n","\n","It stores a moving average of past gradients:\n","\n","$$\n","v_{t+1} = \\beta v_t + \\nabla L_t\n","$$\n","\n","Think of it as:\n","\n","> Accumulated direction + speed from past steps.\n","\n","It has memory.\n","\n","\n","2️⃣ **Where Learning Rate Enters**\n","\n","The actual parameter update is:\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha v_{t+1}\n","$$\n","\n","Important:\n","\n","- $v_t$ is built gradually.\n","- $\\alpha$ scales how much of that velocity we apply.\n","\n","3️⃣ **What Happens in StepLR**\n","\n","Before decay:\n","\n","$$\n","\\alpha = 0.01\n","$$\n","\n","Suddenly at epoch 10:\n","\n","$$\n","\\alpha = 0.001\n","$$\n","\n","But the momentum buffer:\n","\n","$$\n","v_t\n","$$\n","\n","is still large because it was built assuming the old learning rate dynamics.\n","\n","\n","4️⃣ **The “Shock”**\n","\n","Suddenly:\n","\n","$$\n","\\Delta \\theta = -0.001 \\cdot v_t\n","$$\n","\n","instead of:\n","\n","$$\n","\\Delta \\theta = -0.01 \\cdot v_t\n","$$\n","\n","So:\n","\n","- The optimizer was moving fast.\n","- Instantly, its effective step becomes 10× smaller.\n","- The stored velocity no longer matches the new step scale.\n","\n","That sudden imbalance = **shock to the momentum buffer**.\n","\n","\n","5️⃣ **What You Observe During Training**\n","\n","Right after a StepLR drop, you may see:\n","\n","- Temporary plateau\n","- Slight loss spike\n","- Slower progress for a few epochs\n","- Momentum re-adjustment period\n","\n","Then training stabilizes again.\n","\n","6️⃣ **Why Smooth Schedules Avoid This**\n","\n","With exponential or cosine decay:\n","\n","- $\\alpha$ changes gradually.\n","- Momentum buffer adapts gradually.\n","- No internal mismatch.\n","\n","No shock.\n","\n","---\n"]},{"cell_type":"markdown","id":"d834dff4","metadata":{"papermill":{"duration":0.006053,"end_time":"2026-02-22T04:57:25.384625","exception":false,"start_time":"2026-02-22T04:57:25.378572","status":"completed"},"tags":[]},"source":["## When StepLR Is Still Effective\n","\n","### 1️. Large Datasets (e.g., ImageNet-Scale Training)\n","\n","On very large datasets:\n","\n","- Gradients are stable (low variance)\n","- Optimization landscape is smoother\n","- Training runs for many epochs\n","\n","Because of this stability:\n","\n","- Abrupt LR drops don’t destabilize training much\n","- The model has enough signal to recover quickly\n","- Predefined decay points often align with learning phases\n","\n","In large-scale vision training, simple schedules are often sufficient.\n","\n","### 2️. Fixed-Length Training Pipelines\n","\n","If you know:\n","\n","- Total epochs = 90\n","- You always train exactly 90\n","- Same dataset, same model, same regime\n","\n","Then you can pre-plan:\n","\n","- Decay at 30\n","- Decay at 60\n","\n","This removes the need for adaptive logic.\n","\n","StepLR works well when:\n","\n","> The optimization trajectory is predictable.\n","\n","\n","### 3️. Classic CNN Training (ResNet-Style)\n","\n","Many landmark CNN papers (e.g., early ResNet training pipelines) used milestone-based decay.\n","\n","Typical pattern:\n","\n","Epoch 0–30   → LR = 0.1<br>\n","Epoch 30–60  → LR = 0.01<br>\n","Epoch 60–90  → LR = 0.001\n","\n","Why it works here:\n","\n","- CNNs on large vision datasets train in phases.\n","- Early phase = rapid representation learning.\n","- Mid phase = feature stabilization.\n","- Late phase = refinement.\n","\n","StepLR aligns naturally with these phases.\n","\n","\n","### 4️. When Milestones Are Known in Advance\n","\n","If historical runs show:\n","\n","- Loss plateaus around epoch 25\n","- Another plateau around epoch 55\n","\n","You can intentionally schedule drops there.\n","\n","This turns StepLR from \"blind schedule\" into \"structured optimization design.\"\n","\n","\n","### 5️. Strategic Insight\n","\n","StepLR is strongest when:\n","\n","- Training dynamics are consistent\n","- Dataset is large\n","- Hyperparameters are well-tuned\n","- You care about reproducibility\n","- Simplicity matters\n","\n","It is weakest when:\n","\n","- Data is small\n","- Training is unstable\n","- Optimal decay timing is unknown\n","- You need adaptive behavior\n","\n","\n","\n","> StepLR works best when the optimization landscape and training duration are predictable enough that fixed milestone-based decay aligns with the natural learning phases.\n","\n"]},{"cell_type":"markdown","id":"bbcaec19","metadata":{"papermill":{"duration":0.006076,"end_time":"2026-02-22T04:57:25.396771","exception":false,"start_time":"2026-02-22T04:57:25.390695","status":"completed"},"tags":[]},"source":["# ReduceLROnPlateau - Reactive Learning Rate Control\n","\n","**ReduceLROnPlateau** is a performance-based learning rate scheduler that automatically reduces the learning rate when a monitored validation metric stops improving for a specified number of epochs.\n","\n","Formally, if the monitored metric does not improve for `patience` epochs, the learning rate is updated as:\n","\n","$$\n","\\boxed{\\alpha_{\\text{new}} = \\alpha_{\\text{old}} \\cdot \\text{factor}}\n","$$\n","\n","It adjusts the learning rate based on convergence behavior rather than fixed training time.\n","\n","Forget formulas for a moment.\n","\n","This scheduler does one thing:\n","\n","> If validation performance stops improving → reduce learning rate.\n","\n","That’s it.\n","\n","Now let’s make it concrete."]},{"cell_type":"markdown","id":"5f9316dd","metadata":{"papermill":{"duration":0.006104,"end_time":"2026-02-22T04:57:25.409051","exception":false,"start_time":"2026-02-22T04:57:25.402947","status":"completed"},"tags":[]},"source":["## Manual Example.\n","\n","**Imagine This Training Scenario**\n","\n","Validation loss over epochs:\n","\n","| Epoch | Val Loss |\n","|--------|----------|\n","| 1      | 0.90     |\n","| 2      | 0.75     |\n","| 3      | 0.62     |\n","| 4      | 0.60     |\n","| 5      | 0.60     |\n","| 6      | 0.61     |\n","| 7      | 0.60     |\n","\n","Improvement stops around epoch 4.\n","\n","Model is no longer getting better.\n","\n","That flat region = **plateau**.\n","\n","**What ReduceLROnPlateau Does**\n","\n","If you set:\n","\n","- `patience = 3`\n","- `factor = 0.1`\n","\n","Then:\n","\n","- It waits 3 epochs without improvement.\n","- If no improvement → multiply LR by 0.1.\n","\n","So in the table above:\n","\n","- Epoch 4 → improvement\n","- Epoch 5 → no improvement (1)\n","- Epoch 6 → no improvement (2)\n","- Epoch 7 → no improvement (3)\n","\n","Now patience is exceeded.\n","\n","Learning rate becomes:\n","\n","$$\n","\\alpha_{\\text{new}} = 0.1 \\times \\alpha_{\\text{old}}\n","$$\n","\n"]},{"cell_type":"markdown","id":"d734f50d","metadata":{"papermill":{"duration":0.005992,"end_time":"2026-02-22T04:57:25.421128","exception":false,"start_time":"2026-02-22T04:57:25.415136","status":"completed"},"tags":[]},"source":["## How to Choose `factor` and `patience` in ReduceLROnPlateau\n","\n","These two parameters control how aggressively and how quickly you react to stalled learning.\n","\n","If you tune them blindly, you either:\n","\n","- Reduce LR too early → undertrain  \n","- Reduce LR too late → waste compute  \n","\n","Let’s design this properly.\n","\n","\n","### 1️. Understanding `factor`\n","\n","When triggered:\n","\n","$$\n","\\alpha_{\\text{new}} = \\alpha_{\\text{old}} \\cdot \\text{factor}\n","$$\n","\n","So `factor` controls **drop magnitude**.\n","\n","- Small factor (0.1) → sharp phase transition  \n","- Medium factor (0.3–0.5) → moderate correction  \n","- Large factor (0.7–0.9) → gentle adjustment  \n","\n","### How to Choose `factor`\n","\n","#### Case A: You Want Clear Optimization Phases\n","\n","Use:\n","\n","factor = 0.1\n","\n","Best for:\n","- Large models\n","- Vision training\n","- Clear plateau phases\n","\n","You are saying:\n","> \"We’re done exploring. Now refine aggressively.\"\n","\n","\n","#### Case B: Noisy Validation / Small Dataset\n","\n","Use:\n","\n","factor = 0.5\n","\n","Why?\n","\n","You don't want overreaction.\n","A sharp 10× drop might kill learning prematurely.\n","\n","#### Case C: Fine-Tuning\n","\n","Use:\n","\n","factor = 0.2 or 0.1\n","\n","Fine-tuning benefits from decisive stabilization.\n","\n","\n","### 2️. Understanding `patience`\n","\n","`patience` = number of bad epochs allowed before reducing LR.\n","\n","Core tradeoff:\n","\n","- Small patience → fast reaction  \n","- Large patience → conservative reaction  \n","\n","## How to Choose `patience`\n","\n","Think in terms of **validation noise cycle length**.\n","\n","If validation fluctuates every ~2–3 epochs,\n","patience must be larger than that.\n","\n","### Rule of Thumb\n","\n","| Scenario | Patience |\n","|----------|----------|\n","| Small dataset / noisy | 5–10 |\n","| Medium dataset | 3–5 |\n","| Large stable dataset | 2–3 |\n","| Fine-tuning | 2–4 |\n","\n","\n","### 3. A Quantitative Way (Better Approach)\n","\n","Look at historical training curves.\n","\n","Ask:\n","\n","- How long does plateau last before natural improvement?\n","- How many epochs does validation typically fluctuate?\n","\n","Set:\n","\n","$$\n","\\text{patience} > \\text{noise fluctuation window}\n","$$\n","\n","Otherwise you reduce LR due to randomness.\n","\n","\n","### 4️. Coordinating `factor` and `patience`\n","\n","These two must align.\n","\n","Aggressive drop → longer patience  \n","Gentle drop → shorter patience  \n","\n","Examples:\n","\n","**Conservative Setup**\n","\n","factor = 0.5<br>\n","patience = 3\n","\n","**Aggressive Phase Shift**\n","\n","factor = 0.1<br>\n","patience = 5\n","\n","### 5. Strong Default Configuration\n","\n","If unsure:\n","\n","factor = 0.1<br>\n","patience = 3–5\n","\n","This works well in most research setups."]},{"cell_type":"markdown","id":"1e542ac1","metadata":{"papermill":{"duration":0.0061,"end_time":"2026-02-22T04:57:25.433263","exception":false,"start_time":"2026-02-22T04:57:25.427163","status":"completed"},"tags":[]},"source":["## PyTorch Implementation.\n","\n","```python\n","# Create the ReduceLROnPlateau scheduler\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer,          # The optimizer whose learning rate will be adjusted\n","    mode='min',         # 'min' → reduce LR when metric stops decreasing (e.g., validation loss)\n","                        # 'max' → reduce LR when metric stops increasing (e.g., validation accuracy)\n","    factor=0.1,         # Multiply current LR by this factor when triggered\n","                        # new_lr = old_lr * factor\n","    patience=3,         # Number of consecutive epochs with no improvement\n","                        # before reducing the learning rate\n","    verbose=True        # Print a message when LR is reduced\n",")\n","\n","```\n","\n","Important Usage (Critical)\n","\n","```python\n","# After each validation phase\n","scheduler.step(val_loss)\n","```\n","\n","Explanation:\n","\n","* Must be called **after validation**, not after training step.\n","* `val_loss` (or chosen metric) is passed so the scheduler can:\n","\n","  * Check whether performance improved\n","  * Track plateau duration\n","  * Decide whether to reduce LR\n","\n","If you call:\n","\n","```python\n","scheduler.step()\n","```\n","\n","without passing the metric:\n","\n","* The scheduler cannot monitor improvement\n","* Learning rate scheduling will not behave correctly"]},{"cell_type":"markdown","id":"073ce6c0","metadata":{"papermill":{"duration":0.006054,"end_time":"2026-02-22T04:57:25.445487","exception":false,"start_time":"2026-02-22T04:57:25.439433","status":"completed"},"tags":[]},"source":["## Why This Help\n","\n","When learning slows down:\n","\n","- Large LR may cause oscillation.\n","- Smaller LR helps fine-tune.\n","\n","Instead of guessing when to reduce LR, this scheduler waits for the model to signal it.\n","\n","**How It Differs from StepLR**\n","\n","StepLR:\n","> “It has been 10 epochs. Reduce LR.”\n","\n","ReduceLROnPlateau:\n","> “Validation stopped improving. Reduce LR.”\n","\n","One is time-based.  \n","One is performance-based."]},{"cell_type":"markdown","id":"9700e3d3","metadata":{"papermill":{"duration":0.007437,"end_time":"2026-02-22T04:57:25.459077","exception":false,"start_time":"2026-02-22T04:57:25.45164","status":"completed"},"tags":[]},"source":["## Weaknesses of ReduceLROnPlateau\n","\n","ReduceLROnPlateau is adaptive and elegant but it is not robust in every regime. Its weaknesses come from one core fact:\n","\n","> It reacts to validation signals, and validation signals are noisy and delayed.\n","\n","Let’s dissect the failure modes.\n","\n","### 1️. Sensitive to Noisy Validation Metrics\n","\n","If validation loss fluctuates:\n","\n","0.52 → 0.50 → 0.51 → 0.49 → 0.50\n","\n","Small oscillations may look like a plateau.\n","\n","The scheduler may interpret noise as stagnation and reduce LR too early.\n","\n","Result:\n","- Premature LR decay  \n","- Slower convergence  \n","- Undertraining  \n","\n","This is especially problematic for:\n","- Small datasets  \n","- Small batch sizes  \n","- High-variance validation  \n","\n","\n","### 2️. Delayed Reaction\n","\n","It waits for `patience` epochs.\n","\n","This means:\n","\n","- You are already stuck for several epochs\n","- Only then does LR drop\n","\n","So it is reactive, not proactive.\n","\n","If optimal decay timing is known, fixed schedules can be more efficient.\n","\n","### 3️. Over-Reduction Risk\n","\n","If validation improves slightly but not enough to reset patience (depending on `threshold` settings), LR may drop repeatedly.\n","\n","This can cause:\n","\n","$$\n","\\alpha \\rightarrow 0\n","$$\n","\n","Training slows dramatically.\n","\n","You may end up with tiny learning rates too early.\n","\n","\n","### 4️. Not Ideal for Very Large Datasets\n","\n","On massive datasets (e.g., ImageNet-scale):\n","\n","- Validation is stable\n","- Training dynamics are predictable\n","- Predefined milestone schedules are sufficient\n","\n","In such cases, ReduceLROnPlateau adds complexity without significant gain.\n","\n","\n","### 5️. Harder to Reproduce Exactly\n","\n","Because it depends on validation fluctuations:\n","\n","- Small random differences can trigger LR decay at different epochs\n","- Results may vary across runs\n","\n","Milestone schedules are more deterministic.\n","\n","\n","### 6️. Encourages Passive Optimization Design\n","\n","Strategically, this is important.\n","\n","Using ReduceLROnPlateau can prevent you from understanding:\n","\n","- When convergence naturally slows\n","- How many phases your training has\n","- Whether your base LR is well-tuned\n","\n","It hides structural optimization weaknesses.\n"]},{"cell_type":"markdown","id":"b0b63757","metadata":{"papermill":{"duration":0.006305,"end_time":"2026-02-22T04:57:25.472146","exception":false,"start_time":"2026-02-22T04:57:25.465841","status":"completed"},"tags":[]},"source":["## Strategic Insight\n","\n","Use ReduceLROnPlateau when:\n","\n","- You do not know when convergence will slow\n","- Dataset behavior is unpredictable\n","- You want automatic learning rate tuning\n","\n","Avoid relying on it when:\n","\n","- Validation is very noisy\n","- You need highly reproducible milestone-based training\n","- Training is extremely large-scale and stable"]},{"cell_type":"markdown","id":"8669ee6a","metadata":{"papermill":{"duration":0.006106,"end_time":"2026-02-22T04:57:25.484356","exception":false,"start_time":"2026-02-22T04:57:25.47825","status":"completed"},"tags":[]},"source":["# Cosine Annealing - Smooth Modern Standard\n","\n","**Cosine Annealing** is a learning rate schedule that smoothly decreases the learning rate from a maximum value to a minimum value following a cosine curve over a fixed training duration.\n","\n","Instead of abrupt drops, it creates a continuous decay trajectory.\n","\n","\n","### Formula\n","\n","$$\n","\\boxed{\\alpha_t =\n","\\alpha_{min} +\n","\\frac{1}{2}(\\alpha_{max}-\\alpha_{min})\n","\\left(1 + \\cos\\left(\\frac{t\\pi}{T}\\right)\\right)}\n","$$\n","\n","Where:\n","\n","- $\\alpha_{max}$ = initial learning rate  \n","- $\\alpha_{min}$ = minimum learning rate  \n","- $T$ = total decay steps (often total epochs)  \n","- $t$ = current step  \n","\n","At:\n","\n","- $t = 0$ → $\\alpha_t = \\alpha_{max}$\n","- $t = T$ → $\\alpha_t = \\alpha_{min}$\n","\n","The decay is smooth and continuous.\n","\n","Forget the formula.\n","\n","Cosine Annealing just means:\n","\n","> - Start with a high learning rate  \n","> - Decrease it smoothly over time  \n","> - End with a very small learning rate  \n","\n","No sudden drops. No jumps. Just a smooth curve."]},{"cell_type":"markdown","id":"ba6a1f51","metadata":{"papermill":{"duration":0.006254,"end_time":"2026-02-22T04:57:25.497034","exception":false,"start_time":"2026-02-22T04:57:25.49078","status":"completed"},"tags":[]},"source":["## What Actually Happens During Training\n","\n","At the beginning:\n","\n","- Learning rate is high  \n","- Model explores aggressively  \n","\n","In the middle:\n","\n","- Learning rate slowly decreases  \n","- Model stabilizes  \n","\n","At the end:\n","\n","- Learning rate becomes very small  \n","- Model fine-tunes carefully  \n","\n","So it automatically transitions:\n","\n","Exploration → Stabilization → Refinement"]},{"cell_type":"markdown","id":"b68ef915","metadata":{"papermill":{"duration":0.006115,"end_time":"2026-02-22T04:57:25.509263","exception":false,"start_time":"2026-02-22T04:57:25.503148","status":"completed"},"tags":[]},"source":["## Manual Example of Cosine Annealing\n","\n","Let’s compute it manually so you see exactly what happens.\n","\n","We use the formula:\n","\n","$$\n","\\alpha_t =\n","\\alpha_{min} +\n","\\frac{1}{2}(\\alpha_{max}-\\alpha_{min})\n","\\left(1 + \\cos\\left(\\frac{t\\pi}{T}\\right)\\right)\n","$$\n","\n","\n","Assume:\n","\n","- $\\alpha_{max} = 0.1$\n","- $\\alpha_{min} = 0$\n","- $T = 10$ epochs\n","\n","So it will decay from 0.1 → 0 smoothly over 10 epochs.\n","\n","Since $\\alpha_{min} = 0$, formula simplifies to:\n","\n","$$\n","\\alpha_t =\n","\\frac{1}{2}(0.1)\n","\\left(1 + \\cos\\left(\\frac{t\\pi}{10}\\right)\\right)\n","$$\n","\n","$$\n","\\alpha_t =\n","0.05 \\left(1 + \\cos\\left(\\frac{t\\pi}{10}\\right)\\right)\n","$$\n","\n","### Epoch 0\n","\n","$$\n","\\cos(0) = 1\n","$$\n","\n","$$\n","\\alpha_0 = 0.05(1 + 1) = 0.1\n","$$\n","\n","Starts at maximum.\n","\n","### Epoch 5 (Middle)\n","\n","$$\n","\\cos\\left(\\frac{5\\pi}{10}\\right) = \\cos\\left(\\frac{\\pi}{2}\\right) = 0\n","$$\n","\n","$$\n","\\alpha_5 = 0.05(1 + 0) = 0.05\n","$$\n","\n","Exactly half of the initial LR.\n","\n","### Epoch 10 (End)\n","\n","$$\n","\\cos(\\pi) = -1\n","$$\n","\n","$$\n","\\alpha_{10} = 0.05(1 - 1) = 0\n","$$\n","\n","Learning rate reaches minimum.\n","\n","\n","### Full Table\n","\n","| Epoch | cos value | LR |\n","|--------|-----------|------|\n","| 0 | 1 | 0.10 |\n","| 2 | 0.809 | 0.090 |\n","| 5 | 0 | 0.050 |\n","| 8 | -0.809 | 0.0095 |\n","| 10 | -1 | 0 |\n","\n","Notice:\n","\n","- Big changes early\n","- Smaller and smaller changes near the end\n","\n","That smooth flattening is the key benefit.\n","\n","\n","**What This Means Practically**\n","\n","Early training:\n","- Large steps\n","- Fast exploration\n","\n","Late training:\n","- Tiny steps\n","- Careful fine-tuning\n","\n","No sudden drops.\n","No optimizer shock.\n","\n","Cosine Annealing is just:\n","\n","> A smooth slide from high learning rate to low learning rate over a fixed training period."]},{"cell_type":"markdown","id":"fd1d7f94","metadata":{"papermill":{"duration":0.006131,"end_time":"2026-02-22T04:57:25.521709","exception":false,"start_time":"2026-02-22T04:57:25.515578","status":"completed"},"tags":[]},"source":["## PyTorch implementation\n","\n","```python\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n","    optimizer,\n","    T_max=50,      # Number of epochs for full cosine decay\n","    eta_min=1e-6   # Minimum learning rate\n",")\n","````\n","\n","Call each epoch:\n","\n","```python\n","scheduler.step()\n","```"]},{"cell_type":"markdown","id":"89865dc2","metadata":{"papermill":{"duration":0.00615,"end_time":"2026-02-22T04:57:25.534358","exception":false,"start_time":"2026-02-22T04:57:25.528208","status":"completed"},"tags":[]},"source":["## Why Cosine Often Wins\n","\n","### 1️. Smooth Decay\n","\n","Unlike StepLR, there are no sudden jumps.\n","\n","This avoids:\n","\n","* Momentum disruption\n","* Optimization shocks\n","* Abrupt phase transitions\n","\n","The optimizer state evolves naturally.\n","\n","### 2️. Better Final Convergence\n","\n","Near the end of training:\n","\n","$$\n","\\alpha_t \\rightarrow \\alpha_{min}\n","$$\n","\n","The learning rate becomes very small smoothly,\n","which allows fine-grained parameter refinement.\n","\n","This often improves final accuracy.\n","\n","### 3️. Preserves Exploration Early\n","\n","Early in training:\n","\n","* LR stays relatively high\n","* Exploration remains strong\n","\n","Then it gradually transitions into exploitation.\n","\n","### 4️. Works Well With Modern Optimizers\n","\n","Common strong combinations:\n","\n","* AdamW + Cosine\n","* SGD + Momentum + Cosine\n","\n","It balances stability and performance."]},{"cell_type":"markdown","id":"c17deae6","metadata":{"papermill":{"duration":0.006165,"end_time":"2026-02-22T04:57:25.546815","exception":false,"start_time":"2026-02-22T04:57:25.54065","status":"completed"},"tags":[]},"source":["## When Cosine Fails\n","\n","### 1️. Initial LR Too Low\n","\n","If $\\alpha_{max}$ is small:\n","\n","* Cosine just decays an already weak signal\n","* Exploration never happens\n","\n","Cosine cannot fix a bad base LR.\n","\n","### 2️. No Warmup in Deep Networks\n","\n","Large transformers and deep networks often need:\n","\n","* Warmup phase (increase LR gradually first)\n","\n","Without warmup:\n","\n","* Early gradients may explode\n","* Training may destabilize\n","\n","Cosine alone assumes stable early optimization.\n","\n","### 3️. Very Short Training\n","\n","If total training duration is small:\n","\n","* The cosine curve compresses\n","* Decay happens too quickly\n","* LR shrinks before meaningful learning occurs\n","\n","Cosine works best when training is long enough to benefit from smooth annealing."]},{"cell_type":"markdown","id":"aa74d0e1","metadata":{"execution":{"iopub.execute_input":"2026-02-22T04:37:33.249566Z","iopub.status.busy":"2026-02-22T04:37:33.249198Z","iopub.status.idle":"2026-02-22T04:37:33.262021Z","shell.execute_reply":"2026-02-22T04:37:33.260853Z","shell.execute_reply.started":"2026-02-22T04:37:33.249538Z"},"papermill":{"duration":0.006426,"end_time":"2026-02-22T04:57:25.559689","exception":false,"start_time":"2026-02-22T04:57:25.553263","status":"completed"},"tags":[]},"source":["## Strategic Insight.\n","\n","Cosine Annealing is strong when:\n","\n","* Training duration is known\n","* Dataset is moderate to large\n","* You want smooth, stable convergence\n","* You combine it with proper warmup\n","\n","It is not magic.\n","\n","It is effective because it aligns with how optimization naturally transitions from exploration to refinement.\n"]},{"cell_type":"markdown","id":"9df3359d","metadata":{"papermill":{"duration":0.006438,"end_time":"2026-02-22T04:57:25.572518","exception":false,"start_time":"2026-02-22T04:57:25.56608","status":"completed"},"tags":[]},"source":["# Advanced: Warmup (Production Insight)\n","\n","Large CNNs often use:\n","\n","1. Small LR for first few epochs\n","2. Gradually increase to target LR\n","3. Then apply decay\n","\n","Why?\n","\n","Early layers are fragile.\n","Large LR at start can destabilize training."]},{"cell_type":"markdown","id":"6b7da584","metadata":{"papermill":{"duration":0.006151,"end_time":"2026-02-22T04:57:25.584965","exception":false,"start_time":"2026-02-22T04:57:25.578814","status":"completed"},"tags":[]},"source":["# Scheduler Comparison\n","\n","| Scheduler | Decay Type | Stability | Best For |\n","|------------|------------|-----------|----------|\n","| StepLR | Abrupt | Medium | Classic ImageNet-style |\n","| ReduceLROnPlateau | Reactive | High | Small/unknown datasets |\n","| CosineAnnealing | Smooth | Very High | Modern CNN training |"]},{"cell_type":"markdown","id":"3b9acbd3","metadata":{"papermill":{"duration":0.006405,"end_time":"2026-02-22T04:57:25.597615","exception":false,"start_time":"2026-02-22T04:57:25.59121","status":"completed"},"tags":[]},"source":["# Optimizer + Scheduler Interaction\n","\n","Critical combinations:\n","\n","- SGD + StepLR → traditional CV pipelines\n","- SGD + Cosine → strong modern baseline\n","- AdamW + Cosine → transformer/CNN hybrid pipelines\n","- Adam + ReduceLROnPlateau → research prototyping\n","\n","Scheduler must match optimizer dynamics."]},{"cell_type":"markdown","id":"b8c7dc9d","metadata":{"papermill":{"duration":0.006096,"end_time":"2026-02-22T04:57:25.609847","exception":false,"start_time":"2026-02-22T04:57:25.603751","status":"completed"},"tags":[]},"source":["# Key Takeaways from Day 37\n","\n","- LR scheduling controls convergence phases\n","- Fixed LR caps performance\n","- CosineAnnealing is modern default\n","- ReduceLROnPlateau is adaptive fallback\n","- Optimizer + Scheduler pairing matters\n","- Scheduling can improve final accuracy without changing architecture"]},{"cell_type":"markdown","id":"7902a0ea","metadata":{"papermill":{"duration":0.006002,"end_time":"2026-02-22T04:57:25.621967","exception":false,"start_time":"2026-02-22T04:57:25.615965","status":"completed"},"tags":[]},"source":["---\n","<p style=\"text-align:center; color:skyblue; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31286,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":4.23718,"end_time":"2026-02-22T04:57:26.04825","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-22T04:57:21.81107","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}