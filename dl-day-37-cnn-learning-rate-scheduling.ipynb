{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b5850a",
   "metadata": {
    "papermill": {
     "duration": 0.004462,
     "end_time": "2026-02-19T18:07:07.048190",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.043728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Day 37: CNN Learning Rate Scheduling\n",
    "StepLR · ReduceLROnPlateau · CosineAnnealing · LR Phase Intuition\n",
    "\n",
    "Welcome to Day 37!\n",
    "\n",
    "What You’ll Learn Today:\n",
    "\n",
    "1. Why fixed learning rates are suboptimal\n",
    "2. Learning rate decay intuition\n",
    "3. StepLR\n",
    "4. ReduceLROnPlateau\n",
    "5. CosineAnnealing\n",
    "6. When to use which\n",
    "\n",
    "If you found this notebook helpful, your **<b style=\"color:skyblue;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e632b7",
   "metadata": {
    "papermill": {
     "duration": 0.003292,
     "end_time": "2026-02-19T18:07:07.054932",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.051640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Why Fixed Learning Rate Fails\n",
    "\n",
    "Recall the basic gradient descent update:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\nabla L(\\theta_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta_t$ → current parameters  \n",
    "- $\\alpha$ → learning rate  \n",
    "- $\\nabla L$ → gradient (direction of steepest increase)\n",
    "\n",
    "We subtract the gradient to move **downhill**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f5244",
   "metadata": {
    "papermill": {
     "duration": 0.003371,
     "end_time": "2026-02-19T18:07:07.061925",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.058554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## What the Learning Rate Really Controls\n",
    "\n",
    "The learning rate $\\alpha$ determines **step size**.\n",
    "\n",
    "It answers:\n",
    "\n",
    "> “How far should we move in the gradient direction?”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87e1aef",
   "metadata": {
    "papermill": {
     "duration": 0.004607,
     "end_time": "2026-02-19T18:07:07.069797",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.065190",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## If $\\alpha$ Is Large\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\textbf{large step}\n",
    "$$\n",
    "\n",
    "### Pros\n",
    "- Fast movement across loss surface  \n",
    "- Escapes sharp local minima  \n",
    "- Explores wider regions  \n",
    "\n",
    "### Cons\n",
    "- Can overshoot minima  \n",
    "- Oscillates around optimum  \n",
    "- May diverge  \n",
    "\n",
    "Large LR is good for:\n",
    "- Early training\n",
    "- Rough exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e46d6",
   "metadata": {
    "papermill": {
     "duration": 0.003145,
     "end_time": "2026-02-19T18:07:07.076166",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.073021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## If $\\alpha$ Is Small\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\textbf{small step}\n",
    "$$\n",
    "\n",
    "### Pros\n",
    "- Stable convergence  \n",
    "- Precise fine-tuning  \n",
    "- Less oscillation  \n",
    "\n",
    "### Cons\n",
    "- Very slow training  \n",
    "- Can get stuck in sharp minima  \n",
    "- Poor exploration  \n",
    "\n",
    "Small LR is good for:\n",
    "- Late training\n",
    "- Fine-grained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92577d",
   "metadata": {
    "papermill": {
     "duration": 0.003144,
     "end_time": "2026-02-19T18:07:07.082450",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.079306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Core Problem\n",
    "\n",
    "Training has two different phases:\n",
    "\n",
    "1️. Exploration phase  \n",
    "2️. Refinement phase  \n",
    "\n",
    "A single fixed $\\alpha$ cannot optimize both:\n",
    "\n",
    "- If large → unstable at end\n",
    "- If small → painfully slow at start "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d2fdf2",
   "metadata": {
    "papermill": {
     "duration": 0.003097,
     "end_time": "2026-02-19T18:07:07.088660",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.085563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Geometric Intuition\n",
    "\n",
    "Think of descending a mountain:\n",
    "\n",
    "- At the top → you want big jumps  \n",
    "- Near the bottom → you want tiny careful steps  \n",
    "\n",
    "Using one fixed step size is inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce886c6",
   "metadata": {
    "papermill": {
     "duration": 0.003175,
     "end_time": "2026-02-19T18:07:07.094966",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.091791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Strategic Conclusion\n",
    "\n",
    "> Learning rate must change over time.\n",
    "\n",
    "That’s why we use:\n",
    "- Step decay  \n",
    "- Exponential decay  \n",
    "- Cosine annealing  \n",
    "- OneCycle  \n",
    "- Warmup schedules  \n",
    "\n",
    "Fixed LR is simple but strategically weak for deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbcf37e",
   "metadata": {
    "papermill": {
     "duration": 0.003076,
     "end_time": "2026-02-19T18:07:07.101183",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.098107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Optimization Has Distinct Phases\n",
    "\n",
    "Training a neural network is **not uniform**.  \n",
    "The geometry of the loss surface and gradient magnitudes change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10698b5c",
   "metadata": {
    "papermill": {
     "duration": 0.003162,
     "end_time": "2026-02-19T18:07:07.107431",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.104269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1️. Exploration Phase (Early Training)\n",
    "\n",
    "- Loss decreases rapidly  \n",
    "- Gradients are large  \n",
    "- Parameters are far from optimum  \n",
    "\n",
    "Update rule:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\nabla L(\\theta_t)\n",
    "$$\n",
    "\n",
    "Since $\\|\\nabla L\\|$ is large:\n",
    "\n",
    "- Large $\\alpha$ → fast movement across surface  \n",
    "- Helps escape sharp or poor local regions  \n",
    "- Encourages broader exploration  \n",
    "\n",
    "Goal here:\n",
    "> Move quickly toward a promising basin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b20ed8a",
   "metadata": {
    "papermill": {
     "duration": 0.003071,
     "end_time": "2026-02-19T18:07:07.113583",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.110512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2️. Transition Phase (Approaching Minimum)\n",
    "\n",
    "- Parameters enter a valley (basin)  \n",
    "- Gradients shrink but fluctuate  \n",
    "- Curvature becomes important  \n",
    "\n",
    "If $\\alpha$ remains large:\n",
    "\n",
    "- Updates overshoot the minimum  \n",
    "- Oscillations occur across valley walls  \n",
    "\n",
    "Mathematically:\n",
    "\n",
    "Large $\\alpha$ × small but varying $\\nabla L$ → unstable zig-zag behavior  \n",
    "\n",
    "Goal here:\n",
    "> Reduce step size to stabilize descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d6e5c",
   "metadata": {
    "papermill": {
     "duration": 0.003125,
     "end_time": "2026-02-19T18:07:07.119791",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.116666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3️. Fine Convergence Phase (Late Training)\n",
    "\n",
    "- Gradients are small  \n",
    "- Surface curvature dominates  \n",
    "- Small adjustments refine solution  \n",
    "\n",
    "Now we need:\n",
    "\n",
    "$$\n",
    "\\alpha \\text{ very small}\n",
    "$$\n",
    "\n",
    "Why?\n",
    "\n",
    "Because near minimum:\n",
    "\n",
    "$$\n",
    "\\nabla L \\approx 0\n",
    "$$\n",
    "\n",
    "Large $\\alpha$ would:\n",
    "- Bounce around minimum  \n",
    "- Prevent precise convergence  \n",
    "\n",
    "Goal here:\n",
    "> Minimize noise and fine-tune parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b6b8e",
   "metadata": {
    "papermill": {
     "duration": 0.00307,
     "end_time": "2026-02-19T18:07:07.126008",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.122938",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fixed Learning Rate Fails\n",
    "\n",
    "A single $\\alpha$ cannot satisfy all phases:\n",
    "\n",
    "| Phase | Ideal LR |\n",
    "|-------|----------|\n",
    "| Exploration | Large |\n",
    "| Transition | Medium |\n",
    "| Fine Convergence | Small |\n",
    "\n",
    "If LR is fixed:\n",
    "\n",
    "- Large LR → never truly settles  \n",
    "- Small LR → wastes early training potential  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2b1a0",
   "metadata": {
    "papermill": {
     "duration": 0.003103,
     "end_time": "2026-02-19T18:07:07.132185",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.129082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Core Insight\n",
    "\n",
    "Optimization is **dynamic**.\n",
    "\n",
    "The learning rate should:\n",
    "- Start large  \n",
    "- Gradually decrease  \n",
    "- Become very small near convergence  \n",
    "\n",
    "That’s why we use:\n",
    "\n",
    "- Step decay  \n",
    "- Cosine annealing  \n",
    "- OneCycle policy  \n",
    "- Warmup + decay  \n",
    "\n",
    "> No scheduling = stuck oscillating near the minimum instead of converging smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ba866",
   "metadata": {
    "papermill": {
     "duration": 0.003086,
     "end_time": "2026-02-19T18:07:07.138488",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.135402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is Learning Rate Scheduling?\n",
    "\n",
    "**Learning rate scheduling** is the strategy of changing the learning rate during training instead of keeping it constant.\n",
    "\n",
    "Instead of using a fixed learning rate:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\nabla L\n",
    "$$\n",
    "\n",
    "we use a time-dependent learning rate:\n",
    "\n",
    "$$\n",
    "\\boxed{\\theta_{t+1} = \\theta_t - \\alpha_t \\nabla L}\n",
    "$$\n",
    "\n",
    "where $ \\alpha_t $ changes over time.\n",
    "\n",
    "\n",
    "## Why Do We Need It?\n",
    "\n",
    "Training usually happens in different phases:\n",
    "\n",
    "### 1️. Early Phase > Exploration\n",
    "- Loss decreases rapidly  \n",
    "- Gradients are large  \n",
    "- Larger learning rate helps move quickly  \n",
    "\n",
    "### 2️. Middle Phase > Transition\n",
    "- Approaching a good region  \n",
    "- Oscillations may start  \n",
    "- Learning rate should reduce  \n",
    "\n",
    "### 3️. Final Phase > Fine Convergence\n",
    "- Very small improvements  \n",
    "- Requires precise adjustments  \n",
    "- Small learning rate is necessary  \n",
    "\n",
    "A single fixed learning rate cannot work optimally across all these phases.\n",
    "\n",
    "\n",
    "## What Scheduling Does\n",
    "\n",
    "Learning rate scheduling:\n",
    "\n",
    "- Starts large → enables fast progress  \n",
    "- Gradually reduces → improves stability  \n",
    "- Prevents oscillation near minimum  \n",
    "- Often improves final accuracy  \n",
    "\n",
    "\n",
    "## Simple Example\n",
    "\n",
    "### Without Scheduling\n",
    "\n",
    "LR = 0.01 for all 30 epochs\n",
    "\n",
    "Result:\n",
    "- Fast initial progress  \n",
    "- Later oscillation around the minimum  \n",
    "\n",
    "### With Scheduling\n",
    "\n",
    "Epoch 1–10   → 0.01<br>\n",
    "Epoch 11–20  → 0.001<br>\n",
    "Epoch 21–30  → 0.0001\n",
    "\n",
    "Result:\n",
    "- Early: Large steps  \n",
    "- Later: Small, precise steps  \n",
    "- Smoother convergence  \n",
    "\n",
    "## Common Types of LR Scheduling\n",
    "\n",
    "- Step Decay  \n",
    "- Exponential Decay  \n",
    "- Cosine Annealing  \n",
    "- Reduce on Plateau  \n",
    "- Warmup + Decay  \n",
    "- Cyclical Learning Rate  \n",
    "\n",
    "> Learning rate scheduling is the technique of dynamically adjusting the learning rate during training to improve convergence speed and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2881e",
   "metadata": {
    "papermill": {
     "duration": 0.003023,
     "end_time": "2026-02-19T18:07:07.144575",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.141552",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# StepLR - Discrete Learning Rate Decay\n",
    "\n",
    "StepLR is a learning rate scheduler that reduces the learning rate by a fixed factor (`gamma`) after a fixed number of epochs (`step_size`).\n",
    "\n",
    "It implements **piecewise-constant decay***.\n",
    "\n",
    "Instead of smoothly decreasing the learning rate, it drops it in sudden steps.\n",
    "\n",
    "### StepLR Formula\n",
    "\n",
    "$$\n",
    "\\boxed{\\alpha_t = \\alpha_0 \\cdot \\gamma^{\\left\\lfloor \\frac{t}{\\text{step\\_size}} \\right\\rfloor}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\alpha_t$ → learning rate at epoch $t$\n",
    "- $\\alpha_0$ → initial learning rate\n",
    "- $\\gamma$ → decay factor (e.g., 0.1)\n",
    "- $\\text{step\\_size}$ → number of epochs before decay\n",
    "- $\\lfloor \\cdot \\rfloor$ → floor function (round down)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920b6e8",
   "metadata": {
    "papermill": {
     "duration": 0.003146,
     "end_time": "2026-02-19T18:07:07.150808",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.147662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### <p style=\"color:orange;text-align:center;\">*What is Piecewise-Constant Decay?</p>\n",
    "\n",
    "**Piecewise-constant decay** is a function that:\n",
    "\n",
    "- Stays constant for a period of time  \n",
    "- Then suddenly drops to a new constant value  \n",
    "- Repeats this pattern  \n",
    "\n",
    "It does not change smoothly.  \n",
    "It changes in discrete jumps.\n",
    "\n",
    "\n",
    "Imagine a staircase:\n",
    "\n",
    "\n",
    "Level 1  ──────────<br>\n",
    "↓<br>\n",
    "Level 2  ──────────<br>\n",
    "↓<br>\n",
    "Level 3  ──────────\n",
    "\n",
    "\n",
    "Each flat region is constant.  \n",
    "Each drop happens at specific intervals.\n",
    "\n",
    "That staircase shape = piecewise-constant behavior.\n",
    "\n",
    "### Mathematical View (StepLR Example)\n",
    "\n",
    "For StepLR:\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\alpha_0 \\cdot \\gamma^{\\left\\lfloor \\frac{t}{\\text{step\\_size}} \\right\\rfloor}\n",
    "$$\n",
    "\n",
    "The learning rate:\n",
    "\n",
    "- Remains constant within each interval  \n",
    "- Changes only when $ t $ crosses multiples of `step_size`\n",
    "\n",
    "\n",
    "### Simple Example\n",
    "\n",
    "Let:\n",
    "\n",
    "- $ \\alpha_0 = 0.01 $\n",
    "- $ \\gamma = 0.1 $\n",
    "- $ \\text{step\\_size} = 5 $\n",
    "\n",
    "Then:\n",
    "\n",
    "| Epoch | Learning Rate |\n",
    "|-------|---------------|\n",
    "| 0–4   | 0.01          |\n",
    "| 5–9   | 0.001         |\n",
    "| 10–14 | 0.0001        |\n",
    "\n",
    "Notice:\n",
    "\n",
    "- Within each range → LR is constant  \n",
    "- At epoch 5 and 10 → LR drops suddenly  \n",
    "\n",
    "That is **piecewise-constant decay**.\n",
    "\n",
    "\n",
    "### Why It's Called \"Piecewise\"\n",
    "\n",
    "- \"Piecewise\" → defined in separate intervals (pieces)\n",
    "- \"Constant\" → value does not change within each interval\n",
    "- \"Decay\" → value decreases over time\n",
    "\n",
    "\n",
    "## One-Line Definition\n",
    "\n",
    "> Piecewise-constant decay is a step-like schedule where a value remains constant for fixed intervals and then decreases abruptly at predefined points.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45b3a6",
   "metadata": {
    "papermill": {
     "duration": 0.003072,
     "end_time": "2026-02-19T18:07:07.156988",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.153916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## What the Floor Function Does\n",
    "\n",
    "$$\n",
    "\\left\\lfloor \\frac{t}{10} \\right\\rfloor\n",
    "$$\n",
    "\n",
    "If `step_size = 10`:\n",
    "\n",
    "| Epoch | t/10 | Floor | Power of γ |\n",
    "|-------|------|-------|------------|\n",
    "| 1–10  | <1   | 0     | γ⁰ = 1 |\n",
    "| 11–20 | 1–2  | 1     | γ¹ |\n",
    "| 21–30 | 2–3  | 2     | γ² |\n",
    "\n",
    "This creates the **staircase pattern**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c64f2",
   "metadata": {
    "papermill": {
     "duration": 0.003091,
     "end_time": "2026-02-19T18:07:07.163165",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.160074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Manual Example\n",
    "\n",
    "Given:\n",
    "\n",
    "- $\\alpha_0 = 0.01$\n",
    "- $\\gamma = 0.1$\n",
    "- step_size = 10\n",
    "\n",
    "\n",
    "### Epoch 1:\n",
    "\n",
    "$$\n",
    "\\alpha_1 = 0.01 \\cdot 0.1^{\\lfloor 1/10 \\rfloor}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.01 \\cdot 0.1^0\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.01 \\cdot 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.01\n",
    "$$\n",
    "\n",
    "\n",
    "### Epoch 10:\n",
    "\n",
    "$$\n",
    "\\alpha_{10} = 0.01 \\cdot 0.1^{\\lfloor 10/10 \\rfloor}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.01 \\cdot 0.1^1\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.001\n",
    "$$\n",
    "\n",
    "So decay happens right after epoch 10.\n",
    "\n",
    "\n",
    "### Epoch 15:\n",
    "\n",
    "$$\n",
    "\\alpha_{15} = 0.01 \\cdot 0.1^{\\lfloor 15/10 \\rfloor}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.01 \\cdot 0.1^1\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.001\n",
    "$$\n",
    "\n",
    "Learning rate remains constant until next boundary.\n",
    "\n",
    "\n",
    "### Epoch 25:\n",
    "\n",
    "$$\n",
    "\\alpha_{25} = 0.01 \\cdot 0.1^{\\lfloor 25/10 \\rfloor}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.01 \\cdot 0.1^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.0001\n",
    "$$\n",
    "\n",
    "\n",
    "### Final Schedule\n",
    "\n",
    "| Epoch | LR |\n",
    "|--------|------|\n",
    "| 1–9 | 0.01 |\n",
    "| 10–19 | 0.001 |\n",
    "| 20–29 | 0.0001 |\n",
    "\n",
    "Notice the **sudden drops**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e435543",
   "metadata": {
    "papermill": {
     "duration": 0.003078,
     "end_time": "2026-02-19T18:07:07.169413",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.166335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PyTorch Implementation\n",
    "\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=10,  # decay every 10 epochs\n",
    "    gamma=0.1      # multiply LR by 0.1\n",
    ")\n",
    "````\n",
    "\n",
    "At each epoch:\n",
    "\n",
    "```python\n",
    "scheduler.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6602416",
   "metadata": {
    "papermill": {
     "duration": 0.003152,
     "end_time": "2026-02-19T18:07:07.175643",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.172491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Why It Works\n",
    "\n",
    "Recall optimization phases:\n",
    "\n",
    "1️. Early → large LR helps exploration<br>\n",
    "2️. Later → smaller LR helps convergence\n",
    "\n",
    "StepLR forces a **manual phase transition**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f4cfa",
   "metadata": {
    "papermill": {
     "duration": 0.003187,
     "end_time": "2026-02-19T18:07:07.181950",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.178763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Weakness of StepLR\n",
    "\n",
    "Abrupt decay can:\n",
    "\n",
    "* Shock optimizer momentum buffers\n",
    "* Cause temporary instability\n",
    "* Reduce LR too early\n",
    "* Waste exploration capacity\n",
    "\n",
    "Momentum-based optimizers especially feel this shock because:\n",
    "\n",
    "$$\n",
    "\\text{velocity}_{t+1} = \\beta v_t + \\alpha \\nabla L\n",
    "$$\n",
    "\n",
    "Sudden drop in $\\alpha$ changes effective velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca0742",
   "metadata": {
    "papermill": {
     "duration": 0.003154,
     "end_time": "2026-02-19T18:07:07.188250",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.185096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## When StepLR Is Still Effective\n",
    "\n",
    "* Large datasets (ImageNet-style training)\n",
    "* Fixed training length pipelines\n",
    "* Classic CNN training (ResNet schedules)\n",
    "* When decay milestones are known in advance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca38b41",
   "metadata": {
    "papermill": {
     "duration": 0.003484,
     "end_time": "2026-02-19T18:07:07.194951",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.191467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Intuition\n",
    "\n",
    "Think of StepLR as:\n",
    "\n",
    "> \"Train normally for some time → then suddenly reduce step size → repeat.\"\n",
    "\n",
    "It allows:\n",
    "\n",
    "- Fast learning early  \n",
    "- More careful learning later  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10364a9",
   "metadata": {
    "papermill": {
     "duration": 0.003334,
     "end_time": "2026-02-19T18:07:07.201639",
     "exception": false,
     "start_time": "2026-02-19T18:07:07.198305",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31286,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.789895,
   "end_time": "2026-02-19T18:07:07.524198",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-19T18:07:03.734303",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
