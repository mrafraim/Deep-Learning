{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-34-cnn-regularization-deep-dive?scriptVersionId=295702737\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"a1366c9c","metadata":{"papermill":{"duration":0.00454,"end_time":"2026-02-03T17:29:52.468042","exception":false,"start_time":"2026-02-03T17:29:52.463502","status":"completed"},"tags":[]},"source":["# Day 34: CNN Regularization (Deep Dive)\n","Dropout Placement · BatchNorm Behavior · Overfitting Control\n","\n","Welcome to Day 34! This is not theory revision.\n","\n","Today you learn how CNN regularization actually behaves in real projects, including:\n","- Silent failure modes\n","- Wrong-but-common practices\n","- Rules professionals follow instinctively\n","\n","By the end, you should be able to:\n","\n","✔ Diagnose CNN overfitting quickly  \n","✔ Place Dropout correctly without trial-and-error  \n","✔ Never misuse BatchNorm again  \n","✔ Trust your validation metrics\n","\n","If you found this notebook helpful, your **<b style=\"color:skyblue;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"51e77de6","metadata":{"papermill":{"duration":0.003749,"end_time":"2026-02-03T17:29:52.475323","exception":false,"start_time":"2026-02-03T17:29:52.471574","status":"completed"},"tags":[]},"source":["# Why CNNs Overfit\n","\n","CNNs tend to overfit when their capacity to make decisions exceeds the amount of reliable labeled data. The main drivers are:\n","\n","1. **Excessive model capacity**\n","   - Deep stacks of convolutional layers with many filters increase representational power\n","   - Deeper layers learn highly specialized features tied to the training set\n","\n","2. **Over-parameterized classification heads**\n","   - Fully connected layers introduce a large number of parameters\n","   - These layers can easily memorize training examples instead of learning stable decision rules\n","\n","3. **Insufficient or narrow-domain data**\n","   - Limited labeled images reduce exposure to real-world variability\n","   - Domain-specific datasets amplify memorization risk\n","\n","### Key Insight\n","> CNNs rarely overfit while learning **low-level visual features**.  \n","> Overfitting occurs primarily at the **decision boundary**, where extracted features are mapped to class labels.\n","\n","Early convolution layers learn general, transferable patterns (edges, textures, shapes),  \n","while overfitting emerges in later layers that define complex, fragile class boundaries.\n"]},{"cell_type":"markdown","id":"d8d2f6eb","metadata":{"papermill":{"duration":0.003166,"end_time":"2026-02-03T17:29:52.481953","exception":false,"start_time":"2026-02-03T17:29:52.478787","status":"completed"},"tags":[]},"source":["# Real-World Overfitting Pattern in CNNs\n","\n","Typical signs during training:\n","\n","- **Training loss:** decreases smoothly and monotonically\n","- **Validation loss:** decreases at first, then starts rising\n","- **Validation accuracy:** plateaus or declines\n","\n","### Interpretation\n","> The model is memorizing **dataset-specific patterns** instead of learning general visual concepts.\n","\n","**Implication:** Simply adding regularization (Dropout, weight decay) is not enough, overfitting must be addressed where it actually happens: the classifier/decision layers.\n"]},{"cell_type":"markdown","id":"43f75334","metadata":{"papermill":{"duration":0.003213,"end_time":"2026-02-03T17:29:52.488311","exception":false,"start_time":"2026-02-03T17:29:52.485098","status":"completed"},"tags":[]},"source":["# Dropout in CNNs "]},{"cell_type":"markdown","id":"103a5920","metadata":{"papermill":{"duration":0.00301,"end_time":"2026-02-03T17:29:52.494432","exception":false,"start_time":"2026-02-03T17:29:52.491422","status":"completed"},"tags":[]},"source":["## What Dropout Actually Does\n","\n","During training, Dropout randomly “switches off” neurons according to:\n","\n","$$\n","\\boxed{\\tilde{h} = h \\cdot r} \\quad r \\sim \\text{Bernoulli}(p)\n","$$\n","\n","- $h$ → the original activation of a neuron  \n","- $r$ → a random variable sampled from a Bernoulli distribution with probability $p$  \n","  - $r = 1$ with probability $p$ (neuron stays active)  \n","  - $r = 0$ with probability $1-p$ (neuron is dropped)  \n","- $\\tilde{h}$ → the effective activation after applying Dropout  \n","\n","**Interpretation:** Each neuron is independently “kept” with probability $p$; otherwise, it’s ignored for that forward pass.\n","\n","### Effects on the model\n","- **Prevents co-adaptation:** prevents neurons from relying on each other too heavil\n","- **Encourages redundancy:** forces multiple neurons to learn similar features \n","- **Implicit ensemble:** training sees many subnetwork variations, averaging their predictions improves robustness\n","\n","> **Important:** Dropout is only applied during training. During inference, all neurons are active, and activations are usually scaled to match the expected values seen during training.\n"]},{"cell_type":"markdown","id":"1ec537ee","metadata":{"papermill":{"duration":0.003048,"end_time":"2026-02-03T17:29:52.500568","exception":false,"start_time":"2026-02-03T17:29:52.49752","status":"completed"},"tags":[]},"source":["## Dropout Placement: The #1 CNN Mistake\n","\n","### Incorrect Placement\n","- Applying Dropout right after early convolution layers  \n","- Dropping neurons immediately after feature extraction begins\n","\n","### Why this fails\n","- Early conv layers capture **low-level features** like edges, corners, and textures  \n","- Randomly dropping them **destroys spatial structure** crucial for later layers  \n","- Leads to **unstable training**, slower convergence, and sometimes degraded accuracy\n","\n","> Rule of thumb: **Dropout belongs in the classifier or late-stage feature maps, not at the network’s “vision foundation.”**\n"]},{"cell_type":"markdown","id":"7a06b4df","metadata":{"papermill":{"duration":0.003453,"end_time":"2026-02-03T17:29:52.507107","exception":false,"start_time":"2026-02-03T17:29:52.503654","status":"completed"},"tags":[]},"source":["## Correct Dropout Placement (CNN-Specific)\n","\n","Dropout should be applied **only where overfitting is likely**:\n","\n","1. **After fully connected (dense) layers**: this is where the network has enough capacity to memorize training examples \n","2. **Optionally after late convolution blocks**: only if overfitting is observed in deeper features  \n","3. **Never in early layers**: low-level feature maps need to remain intact\n","\n","### Canonical CNN pattern:\n","\n","- Conv → BN → ReLU → Pool\n","- Conv → BN → ReLU → Pool\n","- Flatten → FC → **Dropout** → FC\n","\n","### Layer-by-layer intuition\n","\n","1. **Conv → BN → ReLU → Pool** (early blocks)\n","\n","   * **Conv:** learns features (edges, textures)\n","   * **BN (BatchNorm):** stabilizes training, keeps activations well-scaled\n","   * **ReLU:** introduces non-linearity\n","   * **Pool:** reduces spatial dimensions, keeps strongest signals\n","\n","2. **Conv → BN → ReLU → Pool** (deeper blocks)\n","\n","   * Learns **more abstract features** (object parts)\n","   * Still **avoid Dropout here** unless overfitting is severe\n","\n","3. **Flatten → FC**\n","\n","   * Transforms spatial feature maps into 1D vector for classification\n","\n","4. **Dropout → FC**\n","\n","   * Applied **only at dense layers**\n","   * Regularizes the **decision boundary**, preventing memorization\n","\n","### Why this works\n","- **Preserves early feature extraction:** edges, textures, and shapes remain stable  \n","- **Regularizes the classifier:** prevents memorization at the decision boundary  \n","- **Improves generalization** without corrupting spatial hierarchies\n"]},{"cell_type":"markdown","id":"210b267b","metadata":{"papermill":{"duration":0.003212,"end_time":"2026-02-03T17:29:52.513517","exception":false,"start_time":"2026-02-03T17:29:52.510305","status":"completed"},"tags":[]},"source":["## PyTorch Example: Correct Dropout Usage"]},{"cell_type":"code","execution_count":1,"id":"d198b6b2","metadata":{"execution":{"iopub.execute_input":"2026-02-03T17:29:52.521744Z","iopub.status.busy":"2026-02-03T17:29:52.521312Z","iopub.status.idle":"2026-02-03T17:29:58.426797Z","shell.execute_reply":"2026-02-03T17:29:58.42566Z"},"papermill":{"duration":5.912241,"end_time":"2026-02-03T17:29:58.429003","exception":false,"start_time":"2026-02-03T17:29:52.516762","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class CNNWithDropout(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        # -------------------------\n","        # Feature extraction layers\n","        # -------------------------\n","        self.features = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),  # Conv layer learns edges/textures\n","            nn.BatchNorm2d(32),                                                   # Stabilizes training\n","            nn.ReLU(),                                                            # Non-linearity\n","            nn.MaxPool2d(kernel_size=2)                                           # Downsamples spatial dimensions\n","        )\n","        \n","        # -------------------------\n","        # Classifier / decision layers\n","        # -------------------------\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),                                                         # Flatten feature maps for FC layers\n","            nn.Linear(32 * 14 * 14, 128),                                         # Fully connected layer\n","            nn.ReLU(),                                                            # Non-linearity\n","            nn.Dropout(p=0.5),                                                    # Correct placement: regularizes dense layer\n","            nn.Linear(128, 10)                                                    # Output layer for 10 classes\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)       # Extract features\n","        x = self.classifier(x)     # Apply classification\n","        return x\n"]},{"cell_type":"markdown","id":"aa44258f","metadata":{"papermill":{"duration":0.003513,"end_time":"2026-02-03T17:29:58.43598","exception":false,"start_time":"2026-02-03T17:29:58.432467","status":"completed"},"tags":[]},"source":["### Why this is correct\n","\n","- Dropout is applied after the FC layer, where the network has enough capacity to overfit.\n","- Early conv layers remain intact, preserving edges, textures, and spatial hierarchy.\n","- BatchNorm + ReLU ensures stable activations before downsampling or FC layers."]},{"cell_type":"markdown","id":"113b33a5","metadata":{"papermill":{"duration":0.003348,"end_time":"2026-02-03T17:29:58.442659","exception":false,"start_time":"2026-02-03T17:29:58.439311","status":"completed"},"tags":[]},"source":["## Dropout Rate Guidelines (Used in Practice)\n","\n","| Layer Type | Typical Dropout | Rationale |\n","|-----------|-----------------|-----------|\n","| Early conv layers | 0.0 – 0.1 | Preserve low-level spatial features (edges, textures) |\n","| Late conv layers  | 0.1 – 0.3 | Mild regularization for high-level feature maps |\n","| Fully connected layers | 0.3 – 0.5 | Strong regularization where memorization occurs |\n","\n","### Rule of thumb\n","> If **training accuracy collapses early**, Dropout is too aggressive.  \n","> If **training accuracy is near-perfect but validation degrades**, Dropout is too weak.\n","\n","### Practical note\n","- Start without Dropout, confirm overfitting exists  \n","- Add Dropout only to the classifier first  \n","- Increase rates gradually, never jump straight to high values\n"]},{"cell_type":"markdown","id":"f8f0e79a","metadata":{"papermill":{"duration":0.00316,"end_time":"2026-02-03T17:29:58.448991","exception":false,"start_time":"2026-02-03T17:29:58.445831","status":"completed"},"tags":[]},"source":["# Batch Normalization in CNNs"]},{"cell_type":"markdown","id":"3003758f","metadata":{"papermill":{"duration":0.003229,"end_time":"2026-02-03T17:29:58.455395","exception":false,"start_time":"2026-02-03T17:29:58.452166","status":"completed"},"tags":[]},"source":["## What BatchNorm Actually Normalizes\n","\n","BatchNorm does not normalize weights. It normalizes **activations** produced by a layer.\n","\n","### The normalization step\n","$$\n","\\boxed{\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}}\n","$$\n","\n","- $x$ → activation value from a neuron (per channel)\n","- $\\mu$ → mean activation over the mini-batch\n","- $\\sigma^2$ → variance over the mini-batch\n","- $\\epsilon$ → small constant for numerical stability\n","- $\\hat{x}$ → normalized activation with zero mean and unit variance\n","\n","This forces activations to have:\n","- mean ≈ 0  \n","- variance ≈ 1  \n","\n","But this is done **per channel**, not per neuron.\n","\n","### Learnable rescaling\n","After normalization, BatchNorm restores representational flexibility:\n","\n","$$\n","y = \\gamma \\hat{x} + \\beta\n","$$\n","\n","Where:\n","- $\\gamma$ → learnable scale parameter\n","- $\\beta$ → learnable shift parameter\n","\n","This allows the network to **decide the optimal activation scale**, instead of being forced to stay normalized.\n","\n","### CNN-Specific Meaning\n","\n","In a CNN, an activation tensor looks like:\n","\n","$$\n","(N, C, H, W)\n","$$\n","\n","Where:\n","- $N$ → batch size  \n","- $C$ → number of channels (feature maps)  \n","- $H, W$ → spatial dimensions  \n","\n","**BatchNorm computes $\\mu$ and $\\sigma^2$ separately for each channel**, using:\n","- all samples in the batch\n","- all spatial locations\n","\n","\n","### Concrete Numerical Example\n","\n","Assume:\n","- Batch size = 2\n","- One channel\n","- Feature map size = $2 \\times 2$\n","\n","**Raw activations (one channel)**\n","\n","Image 1:\n","\n","$$\n","\\begin{bmatrix}\n","2 & 4 \\\\\n","6 & 8\n","\\end{bmatrix}\n","$$\n","\n","Image 2:\n","\n","$$\n","\\begin{bmatrix}\n","1 & 3 \\\\\n","5 & 7\n","\\end{bmatrix}\n","$$\n","\n","\n","**Step 1: Collect all values (same channel)**\n","\n","$$\n","{2, 4, 6, 8, 1, 3, 5, 7}\n","$$\n","\n","**Step 2: Compute statistics**\n","- Mean:\n","$$\n","\\mu = \\frac{1+2+3+4+5+6+7+8}{8}\n","     = \\frac{36}{8}\n","     = 4.5\n","$$\n","\n","- Variance:\n","\n","| $x$ | $x-\\mu$ | $(x-\\mu)^2$ |\n","|----|---------|-------------|\n","| 1 | $-3.5$ | $12.25$ |\n","| 2 | $-2.5$ | $6.25$ |\n","| 3 | $-1.5$ | $2.25$ |\n","| 4 | $-0.5$ | $0.25$ |\n","| 5 | $0.5$ | $0.25$ |\n","| 6 | $1.5$ | $2.25$ |\n","| 7 | $2.5$ | $6.25$ |\n","| 8 | $3.5$ | $12.25$ |\n","\n","Sum:\n","$$\n","12.25+6.25+2.25+0.25+0.25+2.25+6.25+12.25 = 42\n","$$\n","\n","Variance:\n","$$\n","\\sigma^2 = \\frac{42}{8} = 5.25\n","$$\n","\n","\n","**Step 3: Normalize one activation**\n","\n","Take $x = 8$:\n","\n","$$\n","\\hat{x} = \\frac{8 - 4.5}{\\sqrt{5.25 + \\epsilon}} \\approx 1.53\n","$$\n","\n","\n","**Step 4: Why Rescaling Is Needed**\n","\n","If we stopped here, **all activations would be forced to zero-mean/unit-variance** too restrictive.\n","\n","So BatchNorm adds:\n","\n","$$\n","y = \\gamma \\hat{x} + \\beta\n","$$\n","\n","Example:\n","- $\\gamma = 2$\n","- $\\beta = 1$\n","\n","Then:\n","$$\n","y = 2(1.53) + 1 = 4.06\n","$$\n","\n","This is the **final output** passed to the next layer.\n","\n","### What the Network Learns\n","\n","- $\\mu, \\sigma^2$ → **not learned**, computed per batch\n","- $\\gamma, \\beta$ → **learned via backprop**\n","- Normalization stabilizes gradients\n","- Rescaling preserves expressiveness\n","\n","### Intuition You Should Remember\n","\n","> BatchNorm stabilizes learning, then gives control back to the network.\n","\n","- Normalization → training stability  \n","- $\\gamma, \\beta$ → expressive power  \n"]},{"cell_type":"markdown","id":"25e07598","metadata":{"papermill":{"duration":0.00318,"end_time":"2026-02-03T17:29:58.461798","exception":false,"start_time":"2026-02-03T17:29:58.458618","status":"completed"},"tags":[]},"source":["## BatchNorm Behavior: Training vs Evaluation\n","\n","### 1️. During Training (`model.train()`)\n","- Computes mean ($\\mu$) and variance ($\\sigma^2$) from the current batch \n","- Normalizes activations using these batch statistics:\n","  $$\n","  \\boxed{\\hat{x} = \\frac{x - \\mu_\\text{batch}}{\\sqrt{\\sigma^2_\\text{batch} + \\epsilon}}}\n","  $$\n","- Updates running averages of mean and variance for later use:\n","  $$\n","  \\text{running\\_mean} \\gets (1 - \\alpha) \\cdot \\text{running\\_mean} + \\alpha \\cdot \\mu_\\text{batch}\n","  $$\n","  $$\n","  \\text{running\\_var} \\gets (1 - \\alpha) \\cdot \\text{running\\_var} + \\alpha \\cdot \\sigma^2_\\text{batch}\n","  $$\n","---\n","\n","####  <b style=\"color:orange;\">Running Mean and Running Variance in BatchNorm</b>\n","\n","BatchNorm keeps moving averages of each channel’s statistics during training:\n","\n","- **Running mean (`running_mean`)** → smoothed average of all batch means seen so far  \n","- **Running variance (`running_var`)** → smoothed average of all batch variances seen so far  \n","\n","During evaluation or deployment, you often feed one image at a time, so batch statistics are unstable or unavailable.  \n","Running mean and variance provide a stable reference for what “normal” activations should look like.\n","\n","**How they are updated**\n","Let:\n","- $\\mu_\\text{batch}$ = mean of the current batch  \n","- $\\sigma^2_\\text{batch}$ = variance of the current batch  \n","- $\\alpha$ = momentum (default ≈ 0.1), controls how fast the running mean and variance update during training. \n","\n","Update rules (exponential moving average):\n","\n","$$\n","\\text{running\\_mean} \\gets (1 - \\alpha) \\cdot \\text{running\\_mean} + \\alpha \\cdot \\mu_\\text{batch}\n","$$\n","\n","$$\n","\\text{running\\_var} \\gets (1 - \\alpha) \\cdot \\text{running\\_var} + \\alpha \\cdot \\sigma^2_\\text{batch}\n","$$\n","\n","Step-by-step:\n","\n","- Multiply the old running mean by $(1 - \\alpha)$ → gives weight to past batches\n","- Multiply the current batch mean by $\\alpha$ → gives weight to current batch\n","- Add them → smooths the value into running_mean\n","\n","Same logic applies for variance.\n","\n","---\n","\n","### 2️. During Evaluation (`model.eval()`)\n","- Uses stored `running_mean` and `running_var` instead of batch statistics  \n","- Ensures consistent activations, independent of batch size or batch composition:\n","  $$\n","  \\hat{x} = \\frac{x - \\text{running\\_mean}}{\\sqrt{\\text{running\\_var} + \\epsilon}}\n","  $$\n","\n","### 3️. Why this matters\n","- ❌ Forgetting `model.eval()` makes validation metrics erratic, because batch statistics fluctuate  \n","- ❌ Deploying a model in `train()` mode causes unstable predictions on single inputs  \n","- ✅ Always switch to `eval()` during validation and deployment\n","\n","### CNN Intuition\n","Think of BatchNorm as a per-channel thermostat:\n","- `train()`: constantly adjusts to the current “temperature”  (batch statistics)   \n","- `eval()`: it keeps the last known average, keeping predictions stable and consistent\n"]},{"cell_type":"markdown","id":"6b8bbd11","metadata":{"papermill":{"duration":0.003129,"end_time":"2026-02-03T17:29:58.468054","exception":false,"start_time":"2026-02-03T17:29:58.464925","status":"completed"},"tags":[]},"source":["## Conceptual Demonstration: BatchNorm Training vs Eval\n","\n","```python\n","# Training mode\n","model.train()\n","out_train = model(x)\n","\n","# Evaluation mode\n","model.eval()\n","out_eval = model(x)\n","````\n","\n","* **Same input, different outputs**\n","\n","  * `out_train` uses **batch statistics** (mean & variance from current batch)\n","  * `out_eval` uses **running statistics** (stored moving averages)\n","* This is expected, BatchNorm behaves differently in train vs eval mode\n","\n","### Key Takeaways\n","\n","* Always call `model.eval()` during validation and deployment\n","* Forgetting this leads to:\n","\n","  * Erratic validation loss/accuracy\n","  * Unstable predictions in production\n","  * Misleading model performance"]},{"cell_type":"markdown","id":"d1e28f9b","metadata":{"papermill":{"duration":0.003048,"end_time":"2026-02-03T17:29:58.474193","exception":false,"start_time":"2026-02-03T17:29:58.471145","status":"completed"},"tags":[]},"source":["## Correct CNN Layer Ordering\n","\n","### Correct:\n","\n","$$Conv → BatchNorm → ReLU$$\n","\n","### Incorrect:\n","\n","$$Conv → ReLU → BatchNorm$$\n","\n","### Why this matters\n","- **BatchNorm normalizes activations**, assuming a roughly symmetric distribution  \n","- **ReLU truncates negative values**, making the distribution highly skewed  \n","- If BN comes after ReLU, normalization is less effective, training can be unstable, and gradient flow is compromised\n","\n","> Rule of thumb: **always place BatchNorm before the non-linearity**"]},{"cell_type":"markdown","id":"bd6554a4","metadata":{"papermill":{"duration":0.004121,"end_time":"2026-02-03T17:29:58.481426","exception":false,"start_time":"2026-02-03T17:29:58.477305","status":"completed"},"tags":[]},"source":["## BatchNorm as a Regularizer\n","\n","BatchNorm helps generalization not by dropping neurons, but by stabilizing the learning process:\n","\n","### How it regularizes\n","1. **Injects stochasticity** via batch statistics  \n","   - Each mini-batch produces slightly different mean & variance  \n","   - Acts like mild noise, discouraging memorization  \n","2. **Stabilizes gradients**  \n","   - Prevents exploding/vanishing activations, making optimization smoother  \n","3. **Accelerates convergence**  \n","   - Allows higher learning rates without destabilizing training  \n","\n","### Modern practice\n","> BatchNorm is almost always included by default in CNNs  \n","> Dropout is applied **selectively**, mainly in fully connected layers or late conv blocks if overfitting persists\n"]},{"cell_type":"markdown","id":"3da20c4e","metadata":{"papermill":{"duration":0.00305,"end_time":"2026-02-03T17:29:58.487803","exception":false,"start_time":"2026-02-03T17:29:58.484753","status":"completed"},"tags":[]},"source":["# CNN Overfitting Control: Decision Framework\n"]},{"cell_type":"markdown","id":"8595d961","metadata":{"papermill":{"duration":0.003054,"end_time":"2026-02-03T17:29:58.49401","exception":false,"start_time":"2026-02-03T17:29:58.490956","status":"completed"},"tags":[]},"source":["## Practical Overfitting Control Checklist\n","\n","When a CNN shows overfitting, follow these steps in order of effectiveness:\n","\n","1. **Add BatchNorm**  \n","   - Stabilizes activations and gradients  \n","   - Often reduces need for heavy Dropout\n","\n","2. **Reduce fully connected (FC) layer width**  \n","   - Fewer parameters → less capacity to memorize\n","\n","3. **Add Dropout selectively**  \n","   - Apply only in **classifier / late FC layers**  \n","   - Avoid early conv layers\n","\n","4. **Apply data augmentation**  \n","   - Introduces real variability in input  \n","   - Prevents model from memorizing dataset-specific patterns\n","\n","5. **Use early stopping**  \n","   - Monitor validation loss  \n","   - Stop training when generalization starts to degrade\n"]},{"cell_type":"markdown","id":"f4eed1ef","metadata":{"papermill":{"duration":0.003163,"end_time":"2026-02-03T17:29:58.500225","exception":false,"start_time":"2026-02-03T17:29:58.497062","status":"completed"},"tags":[]},"source":["## Dropout vs BatchNorm\n","\n","| Aspect | Dropout | BatchNorm |\n","|--------|---------|-----------|\n","| **Noise source** | Randomly drops neurons during training | Variation in batch mean & variance (stochastic normalization) |\n","| **Placement sensitivity** | Very high; must be in classifier / late conv layers | Medium; usually before ReLU, works in most conv blocks |\n","| **Effect on training speed** | Slows training slightly (more stochastic updates) | Speeds up training, allows higher learning rates |\n","| **Default usage** | Optional; only if overfitting | Almost always included, stabilizes learning by default |\n","\n","### Key Takeaways\n","- **Dropout:** aggressive, targeted regularization  \n","- **BatchNorm:** gentle, pervasive stabilization & implicit regularization  \n","- In modern CNN practice: **BN is default; Dropout is selective**\n"]},{"cell_type":"markdown","id":"fe528c38","metadata":{"papermill":{"duration":0.003025,"end_time":"2026-02-03T17:29:58.506365","exception":false,"start_time":"2026-02-03T17:29:58.50334","status":"completed"},"tags":[]},"source":["## Key Takeaways from Day 34\n","\n","- CNNs overfit mostly in FC layers\n","- Dropout placement > dropout rate\n","- BatchNorm behaves differently in train vs eval\n","- Forgetting `model.eval()` silently breaks validation\n","- BatchNorm + selective dropout beats blind regularization\n","\n","---"]},{"cell_type":"markdown","id":"01e82a8e","metadata":{"papermill":{"duration":0.003016,"end_time":"2026-02-03T17:29:58.512446","exception":false,"start_time":"2026-02-03T17:29:58.50943","status":"completed"},"tags":[]},"source":["<p style=\"text-align:center; color:skyblue; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":12.804525,"end_time":"2026-02-03T17:30:01.483472","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-03T17:29:48.678947","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}