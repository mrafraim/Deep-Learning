{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-30-cnn-vs-rnn?scriptVersionId=291830925\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"419b91b4","metadata":{"papermill":{"duration":0.003133,"end_time":"2026-01-14T12:14:09.261674","exception":false,"start_time":"2026-01-14T12:14:09.258541","status":"completed"},"tags":[]},"source":["# Day 30: CNN vs RNN\n","\n","Welcome to Day 30!\n","\n","Today you'll learn:\n","\n","1. Identify the core difference between CNN and RNN.\n","2. Visualize how each model processes data.\n","3. Recognize strengths and weaknesses of both architectures.\n","4. Determine when CNN outperforms RNN.\n","5. Identify scenarios where RNN is necessary.\n","6. Understand CNN + RNN hybrid approaches in production.\n","\n","If you found this notebook helpful, your **<b style=\"color:red;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"8ea48822","metadata":{"papermill":{"duration":0.001886,"end_time":"2026-01-14T12:14:09.265684","exception":false,"start_time":"2026-01-14T12:14:09.263798","status":"completed"},"tags":[]},"source":["# Fundamental Difference \n","\n","| Aspect | CNN | RNN |\n","|-------|-----|-----|\n","| Purpose | Detect local patterns | Capture sequential dependencies |\n","| Input Processing | Parallel | Sequential |\n","| Memory | None (stateless) | Hidden state (remembers past) |\n","| Order Sensitivity | Weak | Strong |\n","| Typical Data | Images, short text, local patterns | Sequences: text, audio, time series |\n","\n","> Think:  \n","> - CNN asks: *“What local patterns exist here?”*  \n","> - RNN asks: *“What happened before this point?”*\n"]},{"cell_type":"markdown","id":"1477905b","metadata":{"papermill":{"duration":0.00188,"end_time":"2026-01-14T12:14:09.269481","exception":false,"start_time":"2026-01-14T12:14:09.267601","status":"completed"},"tags":[]},"source":["# How CNN Thinks\n","\n","CNN treats data as spatial or local structures.\n","\n","- Applies filters over local regions\n","- Detects edges, shapes, n-grams, motifs\n","- Same filter reused everywhere (weight sharing)\n","- Order matters locally, not globally\n","\n","**Example**\n","\n","Sentence: `\"I love this movie\"`\n","\n","- Step 1: Tokenization: `['I', 'love', 'this', 'movie']`\n","- Step 2: Convert to indices (vocab mapping): `['I', 'love', 'this', 'movie'] → [1, 2, 3, 4]`\n","- Step 3: CNN Filters (n-gram):\n","     - 2-word filter (bigram) slides over sequence:\n","       - [1,2] → captures \"I love\"\n","       - [2,3] → captures \"love this\"\n","       - [3,4] → captures \"this movie\" \n","     \n","     - Each filter produces a feature map:\n"," \n","       - `[0.8, 1.2, 0.5]` # filter activation\n","    - Max pooling selects most important feature.\n","    - CNN ignores sequence order beyond local window.  \n","    \n","**Takeaway:** CNN detects sentiment phrases, not long-term context."]},{"cell_type":"markdown","id":"0678af34","metadata":{"papermill":{"duration":0.00205,"end_time":"2026-01-14T12:14:09.273436","exception":false,"start_time":"2026-01-14T12:14:09.271386","status":"completed"},"tags":[]},"source":["# How RNN Thinks\n","\n","RNN treats data as ordered sequences.\n","\n","- Processes one step at a time\n","- Maintains hidden state\n","- Current output depends on previous inputs\n","- Naturally models time and order\n","\n","**Example**\n","\n","Sentence: `\"I love this movie\"`\n","\n","- Step 1: Token embedding: `[1, 2, 3, 4] → embeddings → [[0.1,0.3,...],[0.7,-0.2,...],...]`\n","- Step 2: Hidden state propagation\n","    - h0 = 0\n","    - h1 = f(embedding1, h0) → remembers \"I\"\n","    - h2 = f(embedding2, h1) → remembers \"I love\"\n","    - h3 = f(embedding3, h2) → remembers \"I love this\"\n","    - h4 = f(embedding4, h3) → final sentence context\n","- `f` = RNN cell operation\n","- Output at final step = contextualized vector for entire sentence\n","\n","**Takeaway:** RNN captures word order and dependencies over sequence.   \n"]},{"cell_type":"markdown","id":"f1a88e74","metadata":{"papermill":{"duration":0.001844,"end_time":"2026-01-14T12:14:09.277133","exception":false,"start_time":"2026-01-14T12:14:09.275289","status":"completed"},"tags":[]},"source":["# Training & Performance Considerations\n","\n","| Factor | CNN | RNN |\n","|-------|-----|-----|\n","| Training Speed | Fast (parallelizable) | Slow (sequential) |\n","| Gradient Issues | Rare | Vanishing/exploding possible |\n","| Long-Term Dependencies | Weak | Strong (with LSTM/GRU) |\n","| Memory Requirement | Low | Higher (hidden states) |\n","| Production Scalability | Excellent | Moderate |\n","\n","> - CNN scales better for large datasets.\n","> - RNN required when sequence order is critical.\n"]},{"cell_type":"markdown","id":"0dc776a8","metadata":{"papermill":{"duration":0.001906,"end_time":"2026-01-14T12:14:09.280954","exception":false,"start_time":"2026-01-14T12:14:09.279048","status":"completed"},"tags":[]},"source":["# When CNN is the Better Choice\n","\n","Use CNN when:\n","\n","- Local patterns matter more than order\n","- You need fast training\n","- Dataset is large\n","- Long-range dependencies are not critical\n","\n","**Common CNN Use Cases**\n","\n","- Image classification\n","- Object detection\n","- Short text sentiment classification\n","- Keyword spotting\n","- Log/event pattern detectio\n","\n","In production NLP, CNN often beats RNN for speed\n","\n","**Mini Example:**  \n","Sentence: `\"I love this movie\"`  \n","- CNN filter detects `\"love this\"` → predicts positive sentiment  \n","- Order of \"I\" and \"movie\" is less critical\n"]},{"cell_type":"markdown","id":"89651f1d","metadata":{"papermill":{"duration":0.001887,"end_time":"2026-01-14T12:14:09.284775","exception":false,"start_time":"2026-01-14T12:14:09.282888","status":"completed"},"tags":[]},"source":["# When RNN is the Better Choice\n","\n","Use RNN when:\n","\n","- Order is crucial\n","- Meaning depends on long context\n","- Sequential prediction is required\n","\n","**Common RNN Use Cases**\n","\n","- Language modeling: predict next word\n","- Speech recognition\n","- Time-series forecasting\n","- Text generation\n","- Machine translation\n","\n","If order matters deeply, CNN alone is insufficient.\n","\n","**Mini Example:**\n","\n","Sentence: `\"I love this movie but hate the ending\"`  \n","- RNN tracks full context → captures contrasting sentiment  \n","- CNN may misclassify based on first phrase alone\n"]},{"cell_type":"markdown","id":"c2f3fb81","metadata":{"papermill":{"duration":0.001987,"end_time":"2026-01-14T12:14:09.288622","exception":false,"start_time":"2026-01-14T12:14:09.286635","status":"completed"},"tags":[]},"source":["# Why CNN Can Still Work for Text\n","\n","CNN:\n","- Captures n-gram features\n","- Ignores long dependency issues\n","- Is faster and simpler\n","\n","This works because:\n","- Many NLP tasks rely on local phrases\n","- Long memory is often unnecessary\n","\n","That’s why CNNs were dominant in NLP before transformers.\n"]},{"cell_type":"markdown","id":"df37e98b","metadata":{"papermill":{"duration":0.001913,"end_time":"2026-01-14T12:14:09.292399","exception":false,"start_time":"2026-01-14T12:14:09.290486","status":"completed"},"tags":[]},"source":["# CNN + RNN Hybrid Models\n","\n","- **CNN** extracts local features (phrases, motifs)\n","- **RNN** captures temporal or sequential dependencies\n","\n","Example Pipeline:\n","1. CNN: extract phrase-level features from text\n","2. RNN: model sequence of phrases\n","3. Output: sentiment or next-word prediction\n","\n","**Use Case:**  \n","- Video analysis: CNN → frames, RNN → temporal sequence\n","- Speech recognition: CNN → phonemes, RNN → spoken sentence\n"]},{"cell_type":"markdown","id":"64af242b","metadata":{"papermill":{"duration":0.00195,"end_time":"2026-01-14T12:14:09.296208","exception":false,"start_time":"2026-01-14T12:14:09.294258","status":"completed"},"tags":[]},"source":["# Summary Table\n","\n","| Criteria | CNN | RNN |\n","|-------|-----|-----|\n","| Speed | ⭐⭐⭐⭐ | ⭐⭐ |\n","| Order Awareness | ⭐⭐ | ⭐⭐⭐⭐ |\n","| Long-term Context | ⭐ | ⭐⭐⭐⭐ |\n","| Training Stability | ⭐⭐⭐⭐ | ⭐⭐ |\n","| Production Scalability | ⭐⭐⭐⭐ | ⭐⭐ |\n"]},{"cell_type":"markdown","id":"16001e3b","metadata":{"papermill":{"duration":0.001848,"end_time":"2026-01-14T12:14:09.299942","exception":false,"start_time":"2026-01-14T12:14:09.298094","status":"completed"},"tags":[]},"source":["# Key Takeaways from day 30\n","\n","- CNN ≠ worse RNN, they solve different problems\n","- CNN = pattern detector\n","- RNN = memory-based sequence model\n","- CNN is often preferred in production for speed\n","- RNN is chosen when temporal dependency is unavoidable\n","- Modern models evolved to Transformers to fix both limits\n","\n","---"]},{"cell_type":"markdown","id":"43f96aab","metadata":{"papermill":{"duration":0.001885,"end_time":"2026-01-14T12:14:09.303722","exception":false,"start_time":"2026-01-14T12:14:09.301837","status":"completed"},"tags":[]},"source":["<p style=\"text-align:center; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31239,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":3.540517,"end_time":"2026-01-14T12:14:09.624888","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-01-14T12:14:06.084371","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}