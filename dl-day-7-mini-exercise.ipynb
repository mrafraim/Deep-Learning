{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-7-mini-exercise?scriptVersionId=285889427\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"8976320c","metadata":{"papermill":{"duration":0.004508,"end_time":"2025-12-13T08:36:45.219985","exception":false,"start_time":"2025-12-13T08:36:45.215477","status":"completed"},"tags":[]},"source":["# Day 7: Mini Exercise\n","Welcome to Day 7! \n","\n","Today you will:\n","- Manually compute the output of a neuron  \n","- Practice forward propagation calculations  \n","- Build small network experiments using NumPy  \n","- Strengthen your intuition of weights, bias, activation, and loss\n","\n","Let’s begin!\n","\n","---"]},{"cell_type":"markdown","id":"f7951c53","metadata":{"papermill":{"duration":0.003348,"end_time":"2025-12-13T08:36:45.226997","exception":false,"start_time":"2025-12-13T08:36:45.223649","status":"completed"},"tags":[]},"source":["# Exercise 1: Manual Neuron Output\n","\n","Given:\n","- Input: $x = 3$\n","- Weight: $w = 0.7$\n","- Bias: $b = -1$\n","- Activation: ReLU\n","\n","Step-by-step:\n","\n","### 1. Weighted sum  \n","$$\n","z = w \\cdot x + b\n","$$\n","$$\n","z = 0.7 \\cdot 3 - 1 = 1.1\n","$$\n","\n","### 2. Activation  \n","ReLU:\n","$$\n","ReLU(z) = \\max(0, z)\n","$$\n","\n","Final output:\n","$$\n","a = 1.1\n","$$\n"]},{"cell_type":"markdown","id":"474fb5cf","metadata":{"papermill":{"duration":0.003612,"end_time":"2025-12-13T08:36:45.234031","exception":false,"start_time":"2025-12-13T08:36:45.230419","status":"completed"},"tags":[]},"source":["# Code for Exercise 1"]},{"cell_type":"code","execution_count":1,"id":"fca98deb","metadata":{"execution":{"iopub.execute_input":"2025-12-13T08:36:45.242625Z","iopub.status.busy":"2025-12-13T08:36:45.242239Z","iopub.status.idle":"2025-12-13T08:36:45.252592Z","shell.execute_reply":"2025-12-13T08:36:45.25166Z"},"papermill":{"duration":0.016566,"end_time":"2025-12-13T08:36:45.254075","exception":false,"start_time":"2025-12-13T08:36:45.237509","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["z = 1.0999999999999996\n","Activated output = 1.0999999999999996\n"]}],"source":["x = 3\n","w = 0.7\n","b = -1\n","\n","z = w * x + b\n","a = max(0, z)  # ReLU\n","\n","print(\"z =\", z)\n","print(\"Activated output =\", a)"]},{"cell_type":"markdown","id":"3360b96f","metadata":{"papermill":{"duration":0.003486,"end_time":"2025-12-13T08:36:45.26143","exception":false,"start_time":"2025-12-13T08:36:45.257944","status":"completed"},"tags":[]},"source":["# Exercise 2: Compute Prediction + Loss\n","\n","Given:\n","- Input: $x = 2$\n","- Weight: $w = -0.4$\n","- Bias: $b = 0.2$\n","- Target: $y = 1$\n","- Activation: Sigmoid\n","\n","### Step-1: Weighted sum\n","$$\n","z = w x + b\n","$$\n","\n","### Step-2: Sigmoid activation\n","$$\n","\\sigma(z) = \\frac{1}{1 + e^{-z}}\n","$$\n","\n","### Step-3: MSE loss\n","$$\n","L = (a - y)^2\n","$$\n"]},{"cell_type":"markdown","id":"5e9cda6f","metadata":{"papermill":{"duration":0.003338,"end_time":"2025-12-13T08:36:45.268233","exception":false,"start_time":"2025-12-13T08:36:45.264895","status":"completed"},"tags":[]},"source":["# Code for Exercise 2"]},{"cell_type":"code","execution_count":2,"id":"76556381","metadata":{"execution":{"iopub.execute_input":"2025-12-13T08:36:45.276798Z","iopub.status.busy":"2025-12-13T08:36:45.276104Z","iopub.status.idle":"2025-12-13T08:36:45.281857Z","shell.execute_reply":"2025-12-13T08:36:45.280979Z"},"papermill":{"duration":0.011829,"end_time":"2025-12-13T08:36:45.283464","exception":false,"start_time":"2025-12-13T08:36:45.271635","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["z = -0.6000000000000001\n","Sigmoid activation = 0.35434369377420455\n","Loss = 0.4168720657691381\n"]}],"source":["import numpy as np\n","\n","x = 2\n","w = -0.4\n","b = 0.2\n","y_true = 1\n","\n","# Forward pass\n","z = w * x + b\n","a = 1 / (1 + np.exp(-z))  # Sigmoid\n","loss = (a - y_true)**2    # MSE\n","\n","print(\"z =\", z)\n","print(\"Sigmoid activation =\", a)\n","print(\"Loss =\", loss)\n"]},{"cell_type":"markdown","id":"e5ee4541","metadata":{"papermill":{"duration":0.003455,"end_time":"2025-12-13T08:36:45.291011","exception":false,"start_time":"2025-12-13T08:36:45.287556","status":"completed"},"tags":[]},"source":["# Exercise 3: Two Neuron Layer (Forward Prop)\n","\n","We have a layer with **2 neurons**.\n","\n","Inputs:\n","$$\n","x = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n","$$\n","\n","Weights:\n","$$\n","W = \\begin{bmatrix}\n","0.5 & -0.3 \\\\\n","0.8 & 0.1\n","\\end{bmatrix}\n","$$\n","\n","Bias:\n","$$\n","b = \\begin{bmatrix}\n","0.0 \\\\\n","-0.2\n","\\end{bmatrix}\n","$$\n","\n","Activation: ReLU\n","\n","### Step-1: Linear output\n","$$\n","z = W x + b\n","$$\n","\n","### Step-2: ReLU activation\n","$$\n","a_i = \\max(0, z_i)\n","$$\n"]},{"cell_type":"markdown","id":"1fb4e593","metadata":{"papermill":{"duration":0.003405,"end_time":"2025-12-13T08:36:45.29805","exception":false,"start_time":"2025-12-13T08:36:45.294645","status":"completed"},"tags":[]},"source":["# Code for Exercise 3"]},{"cell_type":"code","execution_count":3,"id":"73d8e5e3","metadata":{"execution":{"iopub.execute_input":"2025-12-13T08:36:45.306388Z","iopub.status.busy":"2025-12-13T08:36:45.306078Z","iopub.status.idle":"2025-12-13T08:36:45.320201Z","shell.execute_reply":"2025-12-13T08:36:45.319059Z"},"papermill":{"duration":0.020329,"end_time":"2025-12-13T08:36:45.321768","exception":false,"start_time":"2025-12-13T08:36:45.301439","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["z:\n"," [[-0.1]\n"," [ 0.8]]\n","\n","Activated output:\n"," [[0. ]\n"," [0.8]]\n"]}],"source":["import numpy as np\n","\n","x = np.array([[1],\n","              [2]])\n","\n","W = np.array([[0.5, -0.3],\n","              [0.8,  0.1]])\n","\n","b = np.array([[ 0.0],\n","              [-0.2]])\n","\n","z = np.dot(W, x) + b\n","a = np.maximum(0, z)\n","\n","print(\"z:\\n\", z)\n","print(\"\\nActivated output:\\n\", a)\n"]},{"cell_type":"markdown","id":"4c689179","metadata":{"execution":{"iopub.execute_input":"2025-12-11T10:54:04.536294Z","iopub.status.busy":"2025-12-11T10:54:04.535949Z","iopub.status.idle":"2025-12-11T10:54:04.543186Z","shell.execute_reply":"2025-12-11T10:54:04.541906Z","shell.execute_reply.started":"2025-12-11T10:54:04.536251Z"},"papermill":{"duration":0.003556,"end_time":"2025-12-13T08:36:45.329227","exception":false,"start_time":"2025-12-13T08:36:45.325671","status":"completed"},"tags":[]},"source":["# Exercise 4: Modify Weights, See Changes\n","\n","Let’s observe how changing weights affects the output.\n","\n","Try changing:\n","- Weight values  \n","- Bias  \n","- Activation (ReLU → Sigmoid)  \n","\n","Questions to think about:\n","1. Does increasing weights make outputs larger?  \n","2. How does bias shift the output?  \n","3. How do different activations behave?\n"]},{"cell_type":"markdown","id":"1bccb220","metadata":{"papermill":{"duration":0.003392,"end_time":"2025-12-13T08:36:45.33618","exception":false,"start_time":"2025-12-13T08:36:45.332788","status":"completed"},"tags":[]},"source":["# Interactive Experiment Code"]},{"cell_type":"code","execution_count":4,"id":"3ccc8f83","metadata":{"execution":{"iopub.execute_input":"2025-12-13T08:36:45.34456Z","iopub.status.busy":"2025-12-13T08:36:45.344258Z","iopub.status.idle":"2025-12-13T08:36:45.351874Z","shell.execute_reply":"2025-12-13T08:36:45.350913Z"},"papermill":{"duration":0.013613,"end_time":"2025-12-13T08:36:45.353349","exception":false,"start_time":"2025-12-13T08:36:45.339736","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["z:\n"," [[-0.1]\n"," [ 0.8]]\n","\n","ReLU output:\n"," [[0. ]\n"," [0.8]]\n","\n","Sigmoid output:\n"," [[0.47502081]\n"," [0.68997448]]\n"]}],"source":["# Baseline\n","x = np.array([[1],\n","              [2]])\n","\n","W = np.array([[0.5, -0.3],\n","              [0.8,  0.1]])\n","\n","b = np.array([[ 0.0],\n","              [-0.2]])\n","\n","# Forward pass\n","z = np.dot(W, x) + b\n","a_relu = np.maximum(0, z)\n","a_sigmoid = 1 / (1 + np.exp(-z))\n","\n","print(\"z:\\n\", z)\n","print(\"\\nReLU output:\\n\", a_relu)\n","print(\"\\nSigmoid output:\\n\", a_sigmoid)"]},{"cell_type":"code","execution_count":5,"id":"322301a6","metadata":{"execution":{"iopub.execute_input":"2025-12-13T08:36:45.363344Z","iopub.status.busy":"2025-12-13T08:36:45.362442Z","iopub.status.idle":"2025-12-13T08:36:45.369942Z","shell.execute_reply":"2025-12-13T08:36:45.368945Z"},"papermill":{"duration":0.014177,"end_time":"2025-12-13T08:36:45.371477","exception":false,"start_time":"2025-12-13T08:36:45.3573","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["See the impact of changing only the weights:\n","z:\n"," [[0. ]\n"," [0.9]]\n","\n","ReLU output:\n"," [[0. ]\n"," [0.9]]\n","\n","Sigmoid output:\n"," [[0.5      ]\n"," [0.7109495]]\n"]}],"source":["W = np.array([[1.0, -0.5],\n","              [0.7,  0.2]])  # weights changed\n","\n","# Forward pass\n","z = np.dot(W, x) + b\n","a_relu = np.maximum(0, z)\n","a_sigmoid = 1 / (1 + np.exp(-z))\n","\n","print(\"See the impact of changing only the weights:\")\n","print(\"z:\\n\", z)\n","print(\"\\nReLU output:\\n\", a_relu)\n","print(\"\\nSigmoid output:\\n\", a_sigmoid)"]},{"cell_type":"markdown","id":"be32b1d6","metadata":{"papermill":{"duration":0.003646,"end_time":"2025-12-13T08:36:45.379135","exception":false,"start_time":"2025-12-13T08:36:45.375489","status":"completed"},"tags":[]},"source":["### When we changed weights only\n","\n","* Input stayed the same.\n","* Bias stayed the same.\n","* Result:\n","  (z) changed from\n","\n","$$\n","\\begin{bmatrix}\n","-0.1 \\\\\n","0.8\n","\\end{bmatrix}\n","$$\n","\n","$$to$$\n","\n","$$\n","\\begin{bmatrix}\n","0. \\\\\n","0.9\n","\\end{bmatrix}\n","$$\n","\n","**What this means:**\n","Weights decide how strongly inputs push the neuron up or down.\n","Change weights → the *direction and strength* of the push changes.\n","\n","That’s why the second neuron’s output increased and the first moved closer to zero."]},{"cell_type":"code","execution_count":6,"id":"e17629a0","metadata":{"execution":{"iopub.execute_input":"2025-12-13T08:36:45.388109Z","iopub.status.busy":"2025-12-13T08:36:45.387464Z","iopub.status.idle":"2025-12-13T08:36:45.39436Z","shell.execute_reply":"2025-12-13T08:36:45.393405Z"},"papermill":{"duration":0.01294,"end_time":"2025-12-13T08:36:45.395825","exception":false,"start_time":"2025-12-13T08:36:45.382885","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["See the impact of changing only the bias:\n","z:\n"," [[0.1]\n"," [0.9]]\n","\n","ReLU output:\n"," [[0.1]\n"," [0.9]]\n","\n","Sigmoid output:\n"," [[0.52497919]\n"," [0.7109495 ]]\n"]}],"source":["W = np.array([[0.5, -0.3],\n","              [0.8,  0.1]]) # Weights of baseline\n","\n","b = np.array([[0.2],\n","              [-0.1]]) # Bias changed\n","\n","# Forward pass\n","z = np.dot(W, x) + b\n","a_relu = np.maximum(0, z)\n","a_sigmoid = 1 / (1 + np.exp(-z))\n","\n","print(\"See the impact of changing only the bias:\")\n","print(\"z:\\n\", z)\n","print(\"\\nReLU output:\\n\", a_relu)\n","print(\"\\nSigmoid output:\\n\", a_sigmoid)"]},{"cell_type":"markdown","id":"7cdb0964","metadata":{"papermill":{"duration":0.003681,"end_time":"2025-12-13T08:36:45.40344","exception":false,"start_time":"2025-12-13T08:36:45.399759","status":"completed"},"tags":[]},"source":["### When you changed **bias only**\n","\n","* Input stayed the same.\n","* Weights stayed the same as baseline.\n","* Result:\n","  (z) changed from \n","\n","$$\n","\\begin{bmatrix}\n","-0.1 \\\\\n","0.8\n","\\end{bmatrix}\n","$$\n","\n","$$to$$\n","\n","$$\n","\\begin{bmatrix}\n","0.1 \\\\\n","0.9\n","\\end{bmatrix}\n","$$\n","\n","**What this means:**\n","Bias adds a constant shift to the neuron output.\n","Same push from inputs, but the whole output moves up.\n","\n","That’s why both neurons increased by about the same amount."]},{"cell_type":"markdown","id":"e76bb78f","metadata":{"papermill":{"duration":0.003562,"end_time":"2025-12-13T08:36:45.410684","exception":false,"start_time":"2025-12-13T08:36:45.407122","status":"completed"},"tags":[]},"source":["### Why ReLU and Sigmoid behave differently\n","\n","* **ReLU** cuts off negatives → negative stays 0, positive passes through unchanged.\n","* **Sigmoid** squashes values → small changes in (z) become soft probability shifts, not jumps.\n","\n","\n","> Weights decide how inputs matter. Bias decides when the neuron activates. Activation decides how the signal is expressed."]},{"cell_type":"markdown","id":"aec940f9","metadata":{"papermill":{"duration":0.003585,"end_time":"2025-12-13T08:36:45.418075","exception":false,"start_time":"2025-12-13T08:36:45.41449","status":"completed"},"tags":[]},"source":["# Summary of Day 7\n","\n","Today you practiced:\n","\n","- Manual neuron calculations  \n","- Weighted sums and activations  \n","- Loss computation  \n","- A tiny 2-neuron forward pass  \n","- Playing with weights & biases to build intuition  \n","\n","This manual work strengthens your understanding before moving to bigger networks and full backpropagation.\n","\n","---"]},{"cell_type":"markdown","id":"6ce150c9","metadata":{"papermill":{"duration":0.003618,"end_time":"2025-12-13T08:36:45.425278","exception":false,"start_time":"2025-12-13T08:36:45.42166","status":"completed"},"tags":[]},"source":["<p style=\"text-align:center; font-size:18px;\">\n","© 2025 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":6.689154,"end_time":"2025-12-13T08:36:45.747573","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-13T08:36:39.058419","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}