{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-28-nlp-preprocessing?scriptVersionId=290982353\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"7277e55a","metadata":{"papermill":{"duration":0.004594,"end_time":"2026-01-09T17:21:35.97902","exception":false,"start_time":"2026-01-09T17:21:35.974426","status":"completed"},"tags":[]},"source":["# Day 28: NLP Preprocessing\n","\n","Welcome to Day 28!\n","\n","Today you’ll learn:\n","1. What Natural Language Processing (NLP) is and why it matters in AI\n","2. Why raw text is challenging for neural networks\n","3. Types of NLP problems with real-world use cases\n","4. Tokenization strategies:\n","   - Word-level\n","   - Character-level\n","   - Subword-level (BPE/WordPiece)\n","5. Vocabulary creation and integer encoding\n","6. Why neural networks require embeddings rather than raw indices\n","7. How embeddings capture semantic relationships\n","8. The necessity of padding for batch processing\n","9. How all these steps integrate into a robust training pipeline\n","\n","By the end of this notebook, you'll be able to transform raw text into numerically usable, semantically rich, batchable tensors ready for deep learning.\n","\n","If you found this notebook helpful, your **<b style=\"color:orange;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---\n"]},{"cell_type":"markdown","id":"9437d18d","metadata":{"papermill":{"duration":0.003041,"end_time":"2026-01-09T17:21:35.985331","exception":false,"start_time":"2026-01-09T17:21:35.98229","status":"completed"},"tags":[]},"source":["# What is NLP?\n","\n","Natural Language Processing (NLP) is the branch of AI focused on enabling machines to understand, interpret, and generate human language. \n","\n","Key points:\n","- Language is inherently ambiguous, contextual, and structured at multiple levels (words, syntax, semantics, discourse)\n","- NLP allows AI to extract meaning, sentiment, or patterns from text\n","\n","**Examples of NLP Applications:**\n","- **Text classification:** Spam detection, topic tagging\n","- **Sentiment analysis:** Product reviews, social media analysis\n","- **Named Entity Recognition (NER):** Extracting people, locations, organizations\n","- **Machine translation:** Google Translate, DeepL\n","- **Conversational AI:** Chatbots, virtual assistants\n","- **Text generation:** GPT, story generation, code generation\n"]},{"cell_type":"markdown","id":"f28c7b73","metadata":{"papermill":{"duration":0.00284,"end_time":"2026-01-09T17:21:35.991244","exception":false,"start_time":"2026-01-09T17:21:35.988404","status":"completed"},"tags":[]},"source":["## Challenges in NLP\n","\n","**Text characteristics:**\n","- **Unstructured:** Unlike images (pixels), text is variable and symbolic\n","- **Variable length:** Sentences and paragraphs differ in size\n","- **Ambiguous:** Words can have multiple meanings (polysemy)\n","- **Context-dependent:** Meaning depends on surrounding words\n","\n","**Neural network requirements:**\n","- Expect **numerical input**\n","- Prefer **fixed-size tensors**\n","- Often work in **batches** for efficiency\n","\n","Preprocessing bridges **human-readable text → machine-readable numerical tensors**\n"]},{"cell_type":"markdown","id":"aea56ed1","metadata":{"papermill":{"duration":0.002876,"end_time":"2026-01-09T17:21:35.996979","exception":false,"start_time":"2026-01-09T17:21:35.994103","status":"completed"},"tags":[]},"source":["##  Types of NLP Tasks\n","\n","| Task | Example | Description |\n","|------|---------|-------------|\n","| Text Classification | Spam vs Ham | Assign a label to entire text |\n","| Sequence Prediction | Next word/character | Predict next token given context |\n","| Named Entity Recognition | \"John\" → Person | Identify entities in text |\n","| Machine Translation | English → French | Translate text from one language to another |\n","| Text Generation | GPT-like models | Generate coherent text given a prompt |\n","| Question Answering | SQuAD | Answer questions from a passage |\n","| Summarization | News article → Summary | Condense information while preserving meaning |\n"]},{"cell_type":"markdown","id":"3ebce505","metadata":{"papermill":{"duration":0.002661,"end_time":"2026-01-09T17:21:36.002478","exception":false,"start_time":"2026-01-09T17:21:35.999817","status":"completed"},"tags":[]},"source":["## Raw Text Example\n"]},{"cell_type":"code","execution_count":1,"id":"0430e631","metadata":{"execution":{"iopub.execute_input":"2026-01-09T17:21:36.009746Z","iopub.status.busy":"2026-01-09T17:21:36.009456Z","iopub.status.idle":"2026-01-09T17:21:36.019463Z","shell.execute_reply":"2026-01-09T17:21:36.018336Z"},"papermill":{"duration":0.015811,"end_time":"2026-01-09T17:21:36.021038","exception":false,"start_time":"2026-01-09T17:21:36.005227","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["['Deep learning is powerful',\n"," 'NLP is fascinating',\n"," 'Transformers revolutionized NLP']"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["sentences = [\n","    \"Deep learning is powerful\",\n","    \"NLP is fascinating\",\n","    \"Transformers revolutionized NLP\"\n","]\n","\n","sentences"]},{"cell_type":"markdown","id":"6f25df3c","metadata":{"papermill":{"duration":0.002838,"end_time":"2026-01-09T17:21:36.027128","exception":false,"start_time":"2026-01-09T17:21:36.02429","status":"completed"},"tags":[]},"source":["Observation:\n","\n","- Variable-length sequences\n","- Words are strings, not usable directly by neural networks"]},{"cell_type":"markdown","id":"da0f3991","metadata":{"papermill":{"duration":0.00321,"end_time":"2026-01-09T17:21:36.033306","exception":false,"start_time":"2026-01-09T17:21:36.030096","status":"completed"},"tags":[]},"source":["#  Tokenization\n","\n","Tokenization = breaking text into atomic units (tokens) that models can process\n","\n","**Why it’s needed:**\n","- Neural networks cannot process raw strings\n","- Tokens act as the vocabulary units\n","\n","**Tokenization strategies:**\n","1. **Word-level:** Each word → one token\n","2. **Character-level:** Each character → one token\n","3. **Subword-level:** Merge common sequences (e.g., BPE, WordPiece, SentencePiece)\n","\n","**Trade-offs:**\n","- Word-level: simple, interpretable, large vocabulary\n","- Character-level: small vocab, handles unknown words, longer sequences\n","- Subword-level: balance between vocab size & generalization (used in Transformers)\n"]},{"cell_type":"markdown","id":"b294fcaa","metadata":{"papermill":{"duration":0.002826,"end_time":"2026-01-09T17:21:36.039044","exception":false,"start_time":"2026-01-09T17:21:36.036218","status":"completed"},"tags":[]},"source":["## Word-Level Tokenization"]},{"cell_type":"code","execution_count":2,"id":"feca1706","metadata":{"execution":{"iopub.execute_input":"2026-01-09T17:21:36.04621Z","iopub.status.busy":"2026-01-09T17:21:36.045933Z","iopub.status.idle":"2026-01-09T17:21:36.053277Z","shell.execute_reply":"2026-01-09T17:21:36.052116Z"},"papermill":{"duration":0.013563,"end_time":"2026-01-09T17:21:36.055373","exception":false,"start_time":"2026-01-09T17:21:36.04181","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenzied Sentences:\n"]},{"data":{"text/plain":["[['deep', 'learning', 'is', 'powerful'],\n"," ['nlp', 'is', 'fascinating'],\n"," ['transformers', 'revolutionized', 'nlp']]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_sentences = [s.lower().split() for s in sentences]\n","\n","print(\"Tokenzied Sentences:\")\n","tokenized_sentences"]},{"cell_type":"markdown","id":"cc714150","metadata":{"papermill":{"duration":0.002865,"end_time":"2026-01-09T17:21:36.061686","exception":false,"start_time":"2026-01-09T17:21:36.058821","status":"completed"},"tags":[]},"source":["Notes:\n","\n","- Converted text to lowercase for consistency\n","- Split on spaces → basic tokenization\n","- Still strings, need mapping to integers"]},{"cell_type":"markdown","id":"4d8ee690","metadata":{"papermill":{"duration":0.002818,"end_time":"2026-01-09T17:21:36.067351","exception":false,"start_time":"2026-01-09T17:21:36.064533","status":"completed"},"tags":[]},"source":["## Vocabulary Creation"]},{"cell_type":"code","execution_count":3,"id":"823c3c07","metadata":{"execution":{"iopub.execute_input":"2026-01-09T17:21:36.076549Z","iopub.status.busy":"2026-01-09T17:21:36.076225Z","iopub.status.idle":"2026-01-09T17:21:36.082161Z","shell.execute_reply":"2026-01-09T17:21:36.080593Z"},"papermill":{"duration":0.011936,"end_time":"2026-01-09T17:21:36.08394","exception":false,"start_time":"2026-01-09T17:21:36.072004","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["{'powerful', 'transformers', 'revolutionized', 'fascinating', 'nlp', 'learning', 'is', 'deep'}\n","Vocabulary size:  8\n"]}],"source":["# Flatten all token lists to build a set of unique words (vocabulary)\n","\n","vocab = set()\n","\n","for sentence in tokenized_sentences:\n","    for word in sentence:\n","        vocab.add(word)\n","\n","print(vocab)\n","\n","vocab_size = len(vocab)\n","print(\"Vocabulary size: \", vocab_size)"]},{"cell_type":"code","execution_count":4,"id":"d54db3b1","metadata":{"execution":{"iopub.execute_input":"2026-01-09T17:21:36.093556Z","iopub.status.busy":"2026-01-09T17:21:36.092313Z","iopub.status.idle":"2026-01-09T17:21:36.098908Z","shell.execute_reply":"2026-01-09T17:21:36.098294Z"},"papermill":{"duration":0.013315,"end_time":"2026-01-09T17:21:36.100801","exception":false,"start_time":"2026-01-09T17:21:36.087486","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Word to index mapping:\n"]},{"data":{"text/plain":["{'powerful': 0,\n"," 'transformers': 1,\n"," 'revolutionized': 2,\n"," 'fascinating': 3,\n"," 'nlp': 4,\n"," 'learning': 5,\n"," 'is': 6,\n"," 'deep': 7}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Map words to unique integer IDs\n","word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n","idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n","\n","print(\"Word to index mapping:\")\n","word_to_idx"]},{"cell_type":"code","execution_count":5,"id":"c866eda9","metadata":{"execution":{"iopub.execute_input":"2026-01-09T17:21:36.1094Z","iopub.status.busy":"2026-01-09T17:21:36.109091Z","iopub.status.idle":"2026-01-09T17:21:36.117142Z","shell.execute_reply":"2026-01-09T17:21:36.116082Z"},"papermill":{"duration":0.014792,"end_time":"2026-01-09T17:21:36.119317","exception":false,"start_time":"2026-01-09T17:21:36.104525","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoded sentences:\n"]},{"data":{"text/plain":["[[7, 5, 6, 0], [4, 6, 3], [1, 2, 4]]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Convert each token in the sentences to its corresponding integer index\n","encoded_sentences = [\n","    [word_to_idx[word] for word in sentence]\n","    for sentence in tokenized_sentences\n","]\n","\n","print(\"Encoded sentences:\")\n","encoded_sentences\n"]},{"cell_type":"markdown","id":"0db782b5","metadata":{"papermill":{"duration":0.003252,"end_time":"2026-01-09T17:21:36.126382","exception":false,"start_time":"2026-01-09T17:21:36.12313","status":"completed"},"tags":[]},"source":["## Why Not Use Word Indices Directly?\n","\n","**Problem with raw indices:**\n","- Imply ordinal relationship (e.g., 2 < 7 → king < queen? meaningless)\n","- No semantic similarity captured\n","- Poor generalization\n","\n","**Solution:** **Word Embeddings**\n","- Map words → dense vectors in ℝ^d\n","- Vectors capture semantic relationships (king ≈ queen - man + woman)\n","- Learned during training or pre-trained (Word2Vec, GloVe, FastText, BERT)\n"]},{"cell_type":"markdown","id":"de8c7e9b","metadata":{"papermill":{"duration":0.003036,"end_time":"2026-01-09T17:21:36.132662","exception":false,"start_time":"2026-01-09T17:21:36.129626","status":"completed"},"tags":[]},"source":["# Embedding Layer\n","\n","- Converts word indices → dense vectors\n","- Captures semantic similarity\n","- Learned end-to-end with task\n","\n","Mathematically:\n","$$\n","\\text{word index} \\rightarrow \\mathbf{v} \\in \\mathbb{R}^d\n","$$\n","\n","- `v` = embedding vector\n","- Words with similar context → similar vectors\n","\n","*Real-world note: Pre-trained embeddings reduce training time and improve generalization.*"]},{"cell_type":"markdown","id":"0f4428a8","metadata":{"papermill":{"duration":0.003127,"end_time":"2026-01-09T17:21:36.13896","exception":false,"start_time":"2026-01-09T17:21:36.135833","status":"completed"},"tags":[]},"source":["## Embedding Layer in PyTorch"]},{"cell_type":"code","execution_count":6,"id":"aacf3535","metadata":{"execution":{"iopub.execute_input":"2026-01-09T17:21:36.147283Z","iopub.status.busy":"2026-01-09T17:21:36.146862Z","iopub.status.idle":"2026-01-09T17:21:41.473356Z","shell.execute_reply":"2026-01-09T17:21:41.472678Z"},"papermill":{"duration":5.332768,"end_time":"2026-01-09T17:21:41.474934","exception":false,"start_time":"2026-01-09T17:21:36.142166","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Embedded output shape: torch.Size([4, 5])\n","Embedded output tensor:\n"]},{"data":{"text/plain":["tensor([[ 0.2719, -0.9351, -0.7394,  2.2545, -1.1973],\n","        [ 0.0709, -0.7132, -0.1445, -0.1652,  2.6867],\n","        [ 0.3410, -0.1827, -1.2986,  0.9409,  2.7127],\n","        [-1.0017, -0.6368, -1.1354,  0.6748,  0.5120]],\n","       grad_fn=<EmbeddingBackward0>)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","\n","# Define embedding dimension\n","embedding_dim = 5\n","\n","# Create a PyTorch embedding layer\n","embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n","\n","# Convert first encoded sentence to tensor\n","sample_input = torch.tensor(encoded_sentences[0])\n","\n","# Pass through embedding layer\n","embedded_output = embedding(sample_input)\n","\n","print(\"Embedded output shape:\", embedded_output.shape)\n","print(\"Embedded output tensor:\")\n","embedded_output\n"]},{"cell_type":"markdown","id":"eb18403a","metadata":{"papermill":{"duration":0.004337,"end_time":"2026-01-09T17:21:41.483039","exception":false,"start_time":"2026-01-09T17:21:41.478702","status":"completed"},"tags":[]},"source":["- Each word in our vocabulary gets mapped to a vector of size `embedding_dim`\n","- `num_embeddings = vocab_size`: total number of unique tokens (words) in our vocab.\n","- PyTorch initializes this table randomly. During training, these vectors are updated.\n","- Output Shape = `(sequence_length, embedding_dim)`\n","- Ready to feed into RNN, LSTM, GRU, or Transformer"]},{"cell_type":"markdown","id":"464ea71c","metadata":{"papermill":{"duration":0.003172,"end_time":"2026-01-09T17:21:41.489745","exception":false,"start_time":"2026-01-09T17:21:41.486573","status":"completed"},"tags":[]},"source":["# Padding \n","\n","Padding is the process of adding special placeholder tokens to sequences so that all sequences in a batch have the same length.  \n","\n","In NLP, sequences are usually sentences represented as lists of word IDs, and sentences naturally vary in length:\n","\n","- Sentence 1: $[12, 5, 9]$ # length 3\n","- Sentence 2: $[7, 2]$ # length 2\n","- Sentence 3: $[3, 8, 1, 4, 9]$ # length 5\n","\n","\n","Neural networks require uniform input shapes, so we add `<PAD>` tokens to the shorter sequences:\n","\n","Padded Sentences (max length = 5):\n","\n","- $[12, 5, 9, 0, 0]$ # padded with 0\n","- $[7, 2, 0, 0, 0]$ # padded with 0\n","- $[3, 8, 1, 4, 9]$ # no padding needed\n","\n","Here, `0` is typically used as the **PAD token**.\n","\n","\n","**Key Notes**\n","\n","- **PAD token value**: Usually `0`, but can be any integer not representing a real word.  \n","- **Masking**: Some models use attention masks or sequence masks to ignore PAD tokens during training.  \n","- **Dynamic padding**: Many libraries support padding each batch to the longest sentence in that batch to reduce computation waste.\n"]},{"cell_type":"markdown","id":"6ab2b706","metadata":{"papermill":{"duration":0.003039,"end_time":"2026-01-09T17:21:41.495971","exception":false,"start_time":"2026-01-09T17:21:41.492932","status":"completed"},"tags":[]},"source":["## Why Padding is Needed\n","\n","1. **Uniform sequence length for batches**\n","\n","   - Deep learning frameworks process data in batches for efficiency.  \n","   - All sequences in a batch must have the same length to form a 2D tensor: `[batch_size, seq_length]`.  \n","\n","2. **Vectorized computation**\n","\n","   - Neural networks operate on tensors, not Python lists of varying lengths.  \n","   - Without padding, sequences of different lengths would create “jagged arrays” that cannot be efficiently processed.  \n","\n","3. **Compatibility with RNNs, LSTMs, Transformers**\n","\n","   - These models expect fixed-length inputs per batch (or require masking to ignore PAD tokens.) \n","   - Padding allows these models to compute forward and backward passes without shape errors.\n"]},{"cell_type":"markdown","id":"de853b8c","metadata":{"papermill":{"duration":0.003059,"end_time":"2026-01-09T17:21:41.502147","exception":false,"start_time":"2026-01-09T17:21:41.499088","status":"completed"},"tags":[]},"source":["## Padding Sequences"]},{"cell_type":"code","execution_count":7,"id":"1f13e3d8","metadata":{"execution":{"iopub.execute_input":"2026-01-09T17:21:41.510235Z","iopub.status.busy":"2026-01-09T17:21:41.509756Z","iopub.status.idle":"2026-01-09T17:21:41.534691Z","shell.execute_reply":"2026-01-09T17:21:41.533548Z"},"papermill":{"duration":0.031422,"end_time":"2026-01-09T17:21:41.536604","exception":false,"start_time":"2026-01-09T17:21:41.505182","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([[7, 5, 6, 0],\n","        [4, 6, 3, 0],\n","        [1, 2, 4, 0]])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","# Convert encoded sentences → tensors\n","tensor_sentences = [torch.tensor(seq) for seq in encoded_sentences]\n","\n","# Pad sequences\n","padded_sequences = pad_sequence(\n","    tensor_sentences,\n","    batch_first=True,  # output shape = (batch_size, seq_len)\n","    padding_value=0\n",")\n","\n","padded_sequences\n"]},{"cell_type":"markdown","id":"cb5ec4d3","metadata":{"papermill":{"duration":0.003388,"end_time":"2026-01-09T17:21:41.543962","exception":false,"start_time":"2026-01-09T17:21:41.540574","status":"completed"},"tags":[]},"source":["#  NLP Preprocessing Pipeline\n","\n","<div style=\"text-align:center\">\n","    \n","```mermaid\n","flowchart TD\n","    A[Raw Text] --> B[Lowercasing & Cleaning]\n","    B --> C[Tokenization]\n","    C --> D[Vocabulary Mapping]\n","    D --> E[Integer Encoding]\n","    E --> F[Embedding]\n","    F --> G[Padding]\n","    G --> H[Batching]\n","    H --> I[Neural Network Input]\n","\n","    %% Node styles\n","    classDef startEnd fill:#ffcc00,stroke:#333,stroke-width:2px,color:#000\n","    classDef process fill:#00ccff,stroke:#333,stroke-width:2px,color:#000\n","\n","    class A,I startEnd\n","    class B,C,D,E,F,G,H process\n","```\n","\n","</div>"]},{"cell_type":"markdown","id":"d2294984","metadata":{"papermill":{"duration":0.003421,"end_time":"2026-01-09T17:21:41.551079","exception":false,"start_time":"2026-01-09T17:21:41.547658","status":"completed"},"tags":[]},"source":["\n","1. **Raw text**\n","2. **Lowercasing & Cleaning:** Remove punctuation, special characters, stopwords if needed\n","3. **Tokenization:** Word, character, or subword\n","4. **Vocabulary mapping:** Build word → index dictionary\n","5. **Integer encoding:** Convert tokens → indices\n","6. **Embedding:** Convert indices → dense vectors\n","7. **Padding:** Equalize sequence lengths in batch\n","8. **Batching:** Form batch tensors\n","9. **Neural Network Input:** Ready for RNN/LSTM/Transformer\n","\n","*Professional tip: Some pipelines also include lemmatization, stemming, or subword tokenization for better generalization.*"]},{"cell_type":"markdown","id":"3c814f6a","metadata":{"papermill":{"duration":0.003462,"end_time":"2026-01-09T17:21:41.558274","exception":false,"start_time":"2026-01-09T17:21:41.554812","status":"completed"},"tags":[]},"source":["# Key Takeaways from Day 28\n","\n","- NLP transforms unstructured text → structured, numeric tensors\n","- Preprocessing is mandatory, not optional\n","- Tokenization defines the granularity of language representation\n","- Embeddings encode semantic meaning beyond raw indices\n","- Padding allows batch training and efficient computation\n","- Preprocessing decisions significantly affect model performance\n","\n","---\n"]},{"cell_type":"markdown","id":"7c917033","metadata":{"papermill":{"duration":0.003275,"end_time":"2026-01-09T17:21:41.564924","exception":false,"start_time":"2026-01-09T17:21:41.561649","status":"completed"},"tags":[]},"source":["<p style=\"text-align:center; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":11.629104,"end_time":"2026-01-09T17:21:44.389345","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-01-09T17:21:32.760241","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}