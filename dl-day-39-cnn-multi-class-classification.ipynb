{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e281ddbe",
   "metadata": {
    "papermill": {
     "duration": 0.006205,
     "end_time": "2026-02-26T08:48:44.865698",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.859493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Day 39: CNN Multi-Class Classification\n",
    "\n",
    "Welcome to Day 39!\n",
    "\n",
    "Today You’ll Learn\n",
    "\n",
    "1. How CNNs produce multi-class predictions  \n",
    "2. What logits really represent  \n",
    "3. Softmax and probability interpretation  \n",
    "4. Why CrossEntropyLoss includes Softmax internally  \n",
    "5. Handling class imbalance properly  \n",
    "6. Top-k accuracy and when to use it  \n",
    "7. Practical evaluation strategy for real-world CNNs\n",
    "\n",
    "If you found this notebook helpful, your **<b style=\"color:skyblue;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7be115",
   "metadata": {
    "papermill": {
     "duration": 0.004653,
     "end_time": "2026-02-26T08:48:44.875838",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.871185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995185ea",
   "metadata": {
    "papermill": {
     "duration": 0.005888,
     "end_time": "2026-02-26T08:48:44.886646",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.880758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Problem Setup\n",
    "\n",
    "You are given:\n",
    "\n",
    "- An image $x$  \n",
    "- $C$ possible classes  \n",
    "\n",
    "Goal:\n",
    "\n",
    "$$\n",
    "\\text{Predict exactly ONE class out of } C\n",
    "$$\n",
    "\n",
    "This is called:\n",
    "\n",
    "**Single-label multi-class classification**\n",
    "\n",
    "Classes are mutually exclusive.\n",
    "\n",
    "Example:\n",
    "\n",
    "- CIFAR-10 → $C = 10$  \n",
    "- ImageNet → $C = 1000$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09435c28",
   "metadata": {
    "papermill": {
     "duration": 0.005378,
     "end_time": "2026-02-26T08:48:44.896979",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.891601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## What the CNN Produces\n",
    "\n",
    "After feature extraction, the CNN outputs a vector:\n",
    "\n",
    "```python\n",
    "Linear(features, C)\n",
    "```\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "If final feature vector is:\n",
    "\n",
    "$$\n",
    "h \\in \\mathbb{R}^{d}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "z = Wh + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $W \\in \\mathbb{R}^{C \\times d}$\n",
    "* $b \\in \\mathbb{R}^{C}$\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "z = [z_1, z_2, ..., z_C]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0219bd",
   "metadata": {
    "papermill": {
     "duration": 0.005571,
     "end_time": "2026-02-26T08:48:44.907283",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.901712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## What Are Logits?\n",
    "\n",
    "Logits are the raw output scores of a neural network before converting them into probabilities.\n",
    "\n",
    "The values $z_i$ are called:\n",
    "\n",
    "* Logits\n",
    "* Raw scores\n",
    "* Unnormalized predictions\n",
    "\n",
    "Important:\n",
    "\n",
    "$$\n",
    "z_i \\in (-\\infty, +\\infty)\n",
    "$$\n",
    "\n",
    "They are **NOT probabilities**.\n",
    "\n",
    "They are simply the output of the final linear layer:\n",
    "\n",
    "$$\n",
    "z = Wh + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $h$ = feature vector  \n",
    "- $W$ = weight matrix  \n",
    "- $b$ = bias  \n",
    "\n",
    "### Key Properties\n",
    "\n",
    "- Logits are real numbers  \n",
    "- They are NOT probabilities  \n",
    "- They do NOT sum to 1  \n",
    "- They can be negative or positive\n",
    "\n",
    "Example:\n",
    "\n",
    "If $C = 3$:\n",
    "\n",
    "$$\n",
    "z = [2.3,\\ -1.1,\\ 0.7]\n",
    "$$\n",
    "\n",
    "These are just scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d768edb8",
   "metadata": {
    "papermill": {
     "duration": 0.004512,
     "end_time": "2026-02-26T08:48:44.916302",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.911790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## How Prediction Works\n",
    "\n",
    "We choose:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_i z_i\n",
    "$$\n",
    "\n",
    "Meaning:\n",
    "\n",
    "Pick the class with the largest score.\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "[2.3,\\ -1.1,\\ 0.7]\n",
    "$$\n",
    "\n",
    "Largest = $2.3$\n",
    "So predict class 1.\n",
    "\n",
    "Only ranking matters.\n",
    "\n",
    "Scale does NOT matter.\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "[100,\\ 50,\\ -20]\n",
    "$$\n",
    "\n",
    "Still class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3633673e",
   "metadata": {
    "papermill": {
     "duration": 0.004778,
     "end_time": "2026-02-26T08:48:44.925556",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.920778",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Why They Are Not Probabilities\n",
    "\n",
    "Probabilities must:\n",
    "\n",
    "* Be between $0$ and $1$\n",
    "* Sum to $1$\n",
    "\n",
    "Logits:\n",
    "\n",
    "* Can be negative\n",
    "* Can be large\n",
    "* Do NOT sum to $1$\n",
    "\n",
    "They are raw evidence scores.\n",
    "\n",
    "Probabilities are obtained later using Softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ea21a",
   "metadata": {
    "papermill": {
     "duration": 0.00474,
     "end_time": "2026-02-26T08:48:44.934948",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.930208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### <p style=\"text-align:center; color:orange; font-size:18px;\">Optional: Explain $$ \\hat{y} = \\arg\\max_i z_i $$ </p>\n",
    "\n",
    "Let’s decode it slowly.\n",
    "\n",
    "\n",
    "**Step 1. What Is $z_i$?**\n",
    "\n",
    "The model outputs:\n",
    "\n",
    "$$\n",
    "z = [z_1, z_2, ..., z_C]\n",
    "$$\n",
    "\n",
    "Each $z_i$ is a score for class $i$.\n",
    "\n",
    "Example (3 classes):\n",
    "\n",
    "$$\n",
    "z = [2.3,\\ -1.1,\\ 0.7]\n",
    "$$\n",
    "\n",
    "**Step 2. What Does “max” Mean?**\n",
    "\n",
    "The maximum value here is:\n",
    "\n",
    "$$\n",
    "2.3\n",
    "$$\n",
    "\n",
    "That’s the largest score.\n",
    "\n",
    "**Step 3. What Does “argmax” Mean?**\n",
    "\n",
    "Important:\n",
    "\n",
    "- **max** → gives the value  \n",
    "- **argmax** → gives the index (position)\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "z = [2.3,\\ -1.1,\\ 0.7]\n",
    "$$\n",
    "\n",
    "- max = 2.3  \n",
    "- argmax = 1  \n",
    "\n",
    "(assuming indexing starts from 1)\n",
    "\n",
    "Because 2.3 is at position 1.\n",
    "\n",
    "**Step 4. What Is $\\hat{y}$?**\n",
    "\n",
    "$\\hat{y}$ means:\n",
    "\n",
    "> Predicted label\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_i z_i\n",
    "$$\n",
    "\n",
    "means:\n",
    "\n",
    "> The predicted class is the index of the largest score.\n",
    "\n",
    "\n",
    "**Super Simple Version**\n",
    "\n",
    "The model outputs scores:\n",
    "\n",
    "Class 1 → 2.3<br>\n",
    "Class 2 → -1.1<br>\n",
    "Class 3 → 0.7\n",
    "\n",
    "Biggest score = 2.3  \n",
    "So prediction = Class 1.\n",
    "\n",
    "**One-Line Meaning**\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_i z_i\n",
    "$$\n",
    "\n",
    "means:\n",
    "\n",
    "> Pick the class with the highest score.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18bf8cb",
   "metadata": {
    "papermill": {
     "duration": 0.004462,
     "end_time": "2026-02-26T08:48:44.943841",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.939379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Softmax\n",
    "\n",
    "Softmax is a normalization function that converts logits into a probability distribution across classes.\n",
    "\n",
    "Given logits:\n",
    "\n",
    "$$\n",
    "z = [z_1, z_2, ..., z_C]\n",
    "$$\n",
    "\n",
    "Softmax computes:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z_i) =\n",
    "\\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\n",
    "$$\n",
    "\n",
    "for each class $i$.\n",
    "\n",
    "Softmax transforms:\n",
    "\n",
    "$$\n",
    "(-\\infty, +\\infty)\n",
    "$$\n",
    "\n",
    "into:\n",
    "\n",
    "$$\n",
    "(0,1)\n",
    "$$\n",
    "\n",
    "while ensuring:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{C} P(y=i) = 1\n",
    "$$\n",
    "\n",
    "So the outputs become valid probabilities.\n",
    "\n",
    "**Key Properties**\n",
    "\n",
    "1. Outputs are between 0 and 1  \n",
    "2. Outputs sum to 1  \n",
    "3. Preserves ranking (largest logit → largest probability)  \n",
    "4. Sensitive to relative differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045b0136",
   "metadata": {
    "papermill": {
     "duration": 0.004416,
     "end_time": "2026-02-26T08:48:44.952724",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.948308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Manual Softmax Example\n",
    "\n",
    "Suppose the model outputs:\n",
    "\n",
    "$$\n",
    "z = [2.0,\\ 1.0,\\ 0.1]\n",
    "$$\n",
    "\n",
    "These are raw scores.\n",
    "\n",
    "Compute:\n",
    "\n",
    "$$\n",
    "e^{z_i}\n",
    "$$\n",
    "\n",
    "Using approximate values:\n",
    "\n",
    "$$\n",
    "e^{2.0} \\approx 7.39\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^{1.0} \\approx 2.72\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^{0.1} \\approx 1.11\n",
    "$$\n",
    "\n",
    "Now we have:\n",
    "\n",
    "$$\n",
    "[7.39,\\ 2.72,\\ 1.11]\n",
    "$$\n",
    "\n",
    "Compute the Sum:\n",
    "\n",
    "$$\n",
    "7.39 + 2.72 + 1.11 = 11.22\n",
    "$$\n",
    "\n",
    "\n",
    "Divide Each by the Sum\n",
    "\n",
    "Softmax formula:\n",
    "\n",
    "$$\n",
    "P(y=i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "Now compute probabilities:\n",
    "\n",
    "Class 1:\n",
    "\n",
    "$$\n",
    "\\frac{7.39}{11.22} \\approx 0.66\n",
    "$$\n",
    "\n",
    "Class 2:\n",
    "\n",
    "$$\n",
    "\\frac{2.72}{11.22} \\approx 0.24\n",
    "$$\n",
    "\n",
    "Class 3:\n",
    "\n",
    "$$\n",
    "\\frac{1.11}{11.22} \\approx 0.10\n",
    "$$\n",
    "\n",
    "\n",
    "Final Softmax Output:\n",
    "\n",
    "$$\n",
    "[0.66,\\ 0.24,\\ 0.10]\n",
    "$$\n",
    "\n",
    "\n",
    "**Check**\n",
    "\n",
    "Do they sum to 1?\n",
    "\n",
    "$$\n",
    "0.66 + 0.24 + 0.10 = 1.00\n",
    "$$\n",
    "\n",
    "Yes.\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "The model assigns:\n",
    "\n",
    "- 66% probability to class 1  \n",
    "- 24% to class 2  \n",
    "- 10% to class 3  \n",
    "\n",
    "Prediction = class 1 (highest probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ea393a",
   "metadata": {
    "papermill": {
     "duration": 0.004354,
     "end_time": "2026-02-26T08:48:44.961694",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.957340",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Why Exponential?\n",
    "\n",
    "The exponential function:\n",
    "\n",
    "- Makes all outputs positive  \n",
    "- Magnifies larger logits more strongly  \n",
    "\n",
    "If one logit is slightly larger, its probability increases significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa944f",
   "metadata": {
    "papermill": {
     "duration": 0.004445,
     "end_time": "2026-02-26T08:48:44.970607",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.966162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Critical Rule (PyTorch)\n",
    "\n",
    "### Do NOT apply Softmax before CrossEntropyLoss\n",
    "\n",
    "```python\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "Why?\n",
    "\n",
    "Because `CrossEntropyLoss` already computes:\n",
    "\n",
    "$$\n",
    "\\text{LogSoftmax} + \\text{Negative Log Likelihood}\n",
    "$$\n",
    "\n",
    "Internally it does:\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{e^{z_i}}{\\sum_j e^{z_j}}\\right)\n",
    "$$\n",
    "\n",
    "If you apply Softmax manually first:\n",
    "\n",
    "* You exponentiate twice\n",
    "* You reduce numerical stability\n",
    "* You can cause gradient issues\n",
    "\n",
    "### Correct Usage\n",
    "\n",
    "Model should output **logits**, not probabilities.\n",
    "\n",
    "Pass logits directly to:\n",
    "\n",
    "```python\n",
    "loss = criterion(logits, labels)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb2b8c",
   "metadata": {
    "papermill": {
     "duration": 0.004807,
     "end_time": "2026-02-26T08:48:44.979945",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.975138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CrossEntropy Loss\n",
    "\n",
    "CrossEntropy Loss is the negative log of the probability assigned to the true class, small when confident and correct, huge when confident and wrong.\n",
    "\n",
    "For the true class $y$:\n",
    "\n",
    "$$\n",
    "L = -\\log(P(y))\n",
    "$$\n",
    "\n",
    "It only cares about the probability assigned to the correct class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23005243",
   "metadata": {
    "papermill": {
     "duration": 0.005239,
     "end_time": "2026-02-26T08:48:44.989772",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.984533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## What This Means\n",
    "\n",
    "If the model assigns:\n",
    "\n",
    "- High probability → small loss  \n",
    "- Low probability → large loss  \n",
    "\n",
    "Because:\n",
    "\n",
    "- $\\log(1) = 0$\n",
    "- $\\log(\\text{small number})$ is very negative  \n",
    "- Negative sign makes it large positive loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66394eca",
   "metadata": {
    "papermill": {
     "duration": 0.004566,
     "end_time": "2026-02-26T08:48:44.999082",
     "exception": false,
     "start_time": "2026-02-26T08:48:44.994516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Manual Example 1. Moderate Confidence\n",
    "\n",
    "Suppose true class = Class 1.\n",
    "\n",
    "Model prediction:\n",
    "\n",
    "$$\n",
    "P = [0.51,\\ 0.30,\\ 0.19]\n",
    "$$\n",
    "\n",
    "Correct class probability:\n",
    "\n",
    "$$\n",
    "P(y) = 0.51\n",
    "$$\n",
    "\n",
    "Compute loss:\n",
    "\n",
    "$$\n",
    "L = -\\log(0.51)\n",
    "$$\n",
    "\n",
    "Using natural log:\n",
    "\n",
    "$$\n",
    "\\log(0.51) \\approx -0.673\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "L \\approx 0.673\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db41e18",
   "metadata": {
    "papermill": {
     "duration": 0.004582,
     "end_time": "2026-02-26T08:48:45.008155",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.003573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Manual Example 2. High Confidence\n",
    "\n",
    "Now model predicts:\n",
    "\n",
    "$$\n",
    "P = [0.95,\\ 0.03,\\ 0.02]\n",
    "$$\n",
    "\n",
    "Correct class probability:\n",
    "\n",
    "$$\n",
    "P(y) = 0.95\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "\n",
    "$$\n",
    "L = -\\log(0.95)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log(0.95) \\approx -0.051\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "L \\approx 0.051\n",
    "$$\n",
    "\n",
    "Much smaller loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b22fed",
   "metadata": {
    "papermill": {
     "duration": 0.004624,
     "end_time": "2026-02-26T08:48:45.017422",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.012798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Manual Example 3. Confident but Wrong\n",
    "\n",
    "True class = Class 1.\n",
    "\n",
    "Model predicts:\n",
    "\n",
    "$$\n",
    "P = [0.01,\\ 0.97,\\ 0.02]\n",
    "$$\n",
    "\n",
    "Correct class probability:\n",
    "\n",
    "$$\n",
    "P(y) = 0.01\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "\n",
    "$$\n",
    "L = -\\log(0.01)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log(0.01) \\approx -4.605\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "L \\approx 4.605\n",
    "$$\n",
    "\n",
    "Huge loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170338e0",
   "metadata": {
    "papermill": {
     "duration": 0.004603,
     "end_time": "2026-02-26T08:48:45.027060",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.022457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### <p style=\"text-align:center; color:orange; font-size:18px;\"> Clear Your Confusion (Optional) </p>\n",
    "\n",
    "#### Prediction Step (Argmax)\n",
    "\n",
    "When predicting, we pick the largest probability:\n",
    "\n",
    "$$\n",
    "P = [0.01,\\ 0.97,\\ 0.02]\n",
    "$$\n",
    "\n",
    "Largest value = 0.97\n",
    "So the model predicts:\n",
    "\n",
    "Class 2\n",
    "\n",
    "Correct.\n",
    "\n",
    "That’s **prediction**.\n",
    "\n",
    "\n",
    "#### Loss Step (Training)\n",
    "\n",
    "Loss does NOT care what the model predicted.\n",
    "\n",
    "Loss cares about:\n",
    "\n",
    "> What probability did the model assign to the TRUE class?\n",
    "\n",
    "You told me:\n",
    "\n",
    "True class = Class 1.\n",
    "\n",
    "So the correct class index is 1.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "P(y) = P(\\text{true class}) = 0.01\n",
    "$$\n",
    "\n",
    "Not 0.97.\n",
    "\n",
    "Because 0.97 belongs to Class 2.\n",
    "\n",
    "\n",
    "#### Why We Don’t Use 0.97\n",
    "\n",
    "Because 0.97 is the probability of the **wrong class**.\n",
    "\n",
    "The loss must measure:\n",
    "\n",
    "> How wrong was the model about the correct answer?\n",
    "\n",
    "The model was extremely confident in Class 2 (0.97).\n",
    "\n",
    "But Class 2 is wrong.\n",
    "\n",
    "So the model gave only 0.01 probability to the true class.\n",
    "\n",
    "That is terrible.\n",
    "\n",
    "So loss becomes large:\n",
    "\n",
    "$$\n",
    "L = -\\log(0.01) \\approx 4.605\n",
    "$$\n",
    "\n",
    "Huge penalty.\n",
    "\n",
    "\n",
    "#### The Core Rule\n",
    "\n",
    "Prediction uses:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_i P_i\n",
    "$$\n",
    "\n",
    "Loss uses:\n",
    "\n",
    "$$\n",
    "L = -\\log(P_{\\text{true class}})\n",
    "$$\n",
    "\n",
    "Two completely different things.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab500a",
   "metadata": {
    "papermill": {
     "duration": 0.004591,
     "end_time": "2026-02-26T08:48:45.036228",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.031637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Why CrossEntropy Works So Well\n",
    "\n",
    "It:\n",
    "\n",
    "- Penalizes confident wrong predictions heavily  \n",
    "- Rewards confident correct predictions  \n",
    "- Pushes probability mass toward true class  \n",
    "\n",
    "It doesn’t just optimize accuracy.\n",
    "\n",
    "It optimizes **confidence quality**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05090bd6",
   "metadata": {
    "papermill": {
     "duration": 0.004381,
     "end_time": "2026-02-26T08:48:45.045154",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.040773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Key Insight\n",
    "\n",
    "Accuracy treats:\n",
    "\n",
    "- 0.51 and 0.95 the same  \n",
    "\n",
    "CrossEntropy does not.\n",
    "\n",
    "It forces the model to:\n",
    "\n",
    "> Be correct and be confident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37e690c",
   "metadata": {
    "papermill": {
     "duration": 0.004647,
     "end_time": "2026-02-26T08:48:45.055009",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.050362",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Where Does Softmax Fit?\n",
    "\n",
    "We’ll go through training flow mathematically, cleanly and precisely.\n",
    "\n",
    "**Step 0; Model Output (Logits)**\n",
    "\n",
    "The model outputs raw scores:\n",
    "\n",
    "$$\n",
    "z = [2.0,\\ 1.0,\\ 0.1]\n",
    "$$\n",
    "\n",
    "These are logits.\n",
    "\n",
    "They are just real numbers:\n",
    "\n",
    "$$\n",
    "z_i \\in (-\\infty, +\\infty)\n",
    "$$\n",
    "\n",
    "No probabilities yet.\n",
    "\n",
    "\n",
    "**Step 1. Apply Softmax (Convert to Probabilities)**\n",
    "\n",
    "Softmax formula:\n",
    "\n",
    "$$\n",
    "P(y=i) = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Compute exponentials\n",
    "\n",
    "$$\n",
    "e^{2.0} \\approx 7.39\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^{1.0} \\approx 2.72\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^{0.1} \\approx 1.11\n",
    "$$\n",
    "\n",
    "Compute denominator\n",
    "\n",
    "$$\n",
    "7.39 + 2.72 + 1.11 = 11.22\n",
    "$$\n",
    "\n",
    "Compute probabilities\n",
    "\n",
    "$$\n",
    "P_1 = \\frac{7.39}{11.22} \\approx 0.66\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_2 = \\frac{2.72}{11.22} \\approx 0.24\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_3 = \\frac{1.11}{11.22} \\approx 0.10\n",
    "$$\n",
    "\n",
    "Now we have:\n",
    "\n",
    "$$\n",
    "P = [0.66,\\ 0.24,\\ 0.10]\n",
    "$$\n",
    "\n",
    "This is where **Softmax happens**.\n",
    "\n",
    "\n",
    "**Step 2. Compute CrossEntropy Loss**\n",
    "\n",
    "Definition:\n",
    "\n",
    "$$\n",
    "L = -\\log P(y)\n",
    "$$\n",
    "\n",
    "Assume true class is **Class 1**.\n",
    "\n",
    "So we use:\n",
    "\n",
    "$$\n",
    "P(y) = 0.66\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "\n",
    "$$\n",
    "L = -\\log(0.66)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log(0.66) \\approx -0.415\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "L \\approx 0.415\n",
    "$$\n",
    "\n",
    "Small loss → good prediction.\n",
    "\n",
    "\n",
    "**Step 3. Confident but Wrong Case**\n",
    "\n",
    "Suppose logits are:\n",
    "\n",
    "$$\n",
    "z = [0.1,\\ 4.0,\\ 0.2]\n",
    "$$\n",
    "\n",
    "### Apply Softmax\n",
    "\n",
    "Exponentials:\n",
    "\n",
    "$$\n",
    "e^{0.1} \\approx 1.11\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^{4.0} \\approx 54.6\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^{0.2} \\approx 1.22\n",
    "$$\n",
    "\n",
    "Sum:\n",
    "\n",
    "$$\n",
    "1.11 + 54.6 + 1.22 \\approx 56.93\n",
    "$$\n",
    "\n",
    "Probabilities:\n",
    "\n",
    "$$\n",
    "P \\approx [0.02,\\ 0.96,\\ 0.02]\n",
    "$$\n",
    "\n",
    "True class is still Class 1.\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "L = -\\log(0.02)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log(0.02) \\approx -3.91\n",
    "$$\n",
    "\n",
    "$$\n",
    "L \\approx 3.91\n",
    "$$\n",
    "\n",
    "Huge loss.\n",
    "\n",
    "Because the model gave very low probability to the true class.\n",
    "\n",
    "\n",
    "### Where Softmax Happens in PyTorch\n",
    "\n",
    "When you write:\n",
    "\n",
    "```python\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(logits, target)\n",
    "```\n",
    "\n",
    "Internally PyTorch computes:\n",
    "\n",
    "$$\n",
    "-\\left(\n",
    "z_y - \\log \\sum_j e^{z_j}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "This expression is mathematically equivalent to:\n",
    "\n",
    "$$\n",
    "-\\log \\left(\n",
    "\\frac{e^{z_y}}{\\sum_j e^{z_j}}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Which is:\n",
    "\n",
    "$$\n",
    "-\\log(\\text{Softmax}(z_y))\n",
    "$$\n",
    "\n",
    "So during training:\n",
    "\n",
    "$$\n",
    "\\text{Softmax is applied internally.}\n",
    "$$\n",
    "\n",
    "You do NOT call it manually.\n",
    "\n",
    "\n",
    "### Full Pipeline Summary\n",
    "\n",
    "**During Training**\n",
    "\n",
    "$$\n",
    "\\text{Logits} \\rightarrow \\text{CrossEntropyLoss}\n",
    "$$\n",
    "\n",
    "Softmax is inside the loss.\n",
    "\n",
    "**During Inference**\n",
    "\n",
    "If you want probabilities:\n",
    "\n",
    "$$\n",
    "\\text{Logits} \\rightarrow \\text{Softmax} \\rightarrow \\text{Probabilities}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8cc1a4",
   "metadata": {
    "papermill": {
     "duration": 0.004482,
     "end_time": "2026-02-26T08:48:45.064024",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.059542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Class Imbalance\n",
    "\n",
    "Suppose dataset distribution:\n",
    "\n",
    "- Class A → 90%  \n",
    "- Class B → 5%  \n",
    "- Class C → 5%  \n",
    "\n",
    "A lazy model can learn:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{Class A (always)}\n",
    "$$\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "$$\n",
    "90\\%\n",
    "$$\n",
    "\n",
    "But:\n",
    "\n",
    "- Recall for B = 0  \n",
    "- Recall for C = 0  \n",
    "\n",
    "Model is statistically “good” but practically useless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100c4ab",
   "metadata": {
    "papermill": {
     "duration": 0.004473,
     "end_time": "2026-02-26T08:48:45.073195",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.068722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Why This Happens\n",
    "\n",
    "CrossEntropy minimizes **average loss**:\n",
    "\n",
    "$$\n",
    "L = -\\log P(y)\n",
    "$$\n",
    "\n",
    "If 90% of samples are Class A:\n",
    "\n",
    "- Gradients are dominated by Class A  \n",
    "- The optimizer mostly improves Class A  \n",
    "- Minority classes barely influence updates  \n",
    "\n",
    "The optimization objective is skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa38649e",
   "metadata": {
    "papermill": {
     "duration": 0.004404,
     "end_time": "2026-02-26T08:48:45.082226",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.077822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Solution 1: Weighted Loss\n",
    "\n",
    "Modify loss:\n",
    "\n",
    "$$\n",
    "L = -w_y \\log P(y)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $w_y$ = weight of the true class  \n",
    "\n",
    "Rare classes get larger weights.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "class_weights = torch.tensor([0.2, 0.4, 0.4]).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "```\n",
    "\n",
    "Effect:\n",
    "\n",
    "* Errors on minority classes produce larger gradients\n",
    "* Optimizer is forced to care about them\n",
    "\n",
    "\n",
    "**Important Insight**\n",
    "\n",
    "Weights should often be:\n",
    "\n",
    "$$\n",
    "w_i \\propto \\frac{1}{\\text{frequency}_i}\n",
    "$$\n",
    "\n",
    "Or normalized inverse frequency.\n",
    "\n",
    "Otherwise you risk:\n",
    "\n",
    "* Overcompensating\n",
    "* Training instability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebfcc96",
   "metadata": {
    "papermill": {
     "duration": 0.004408,
     "end_time": "2026-02-26T08:48:45.091222",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.086814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### <p style=\"text-align:center; color:orange; font-size:18px;\"> Inverse Frequency (Optional) </p>\n",
    "\n",
    "We want each class to contribute roughly equally to training.\n",
    "\n",
    "Suppose dataset:\n",
    "\n",
    "* Class A → 90%\n",
    "* Class B → 5%\n",
    "* Class C → 5%\n",
    "\n",
    "If we do nothing:\n",
    "\n",
    "Expected gradient contribution:\n",
    "\n",
    "$$\n",
    "0.9 \\nabla L_A + 0.05 \\nabla L_B + 0.05 \\nabla L_C\n",
    "$$\n",
    "\n",
    "Class A dominates updates.\n",
    "\n",
    "So we increase weights for rare classes.\n",
    "\n",
    "\n",
    "**Why Inverse Frequency?**\n",
    "\n",
    "If a class appears less often, we want its total influence over time to match others.\n",
    "\n",
    "Let frequency of class $i$ be:\n",
    "\n",
    "$$\n",
    "f_i\n",
    "$$\n",
    "\n",
    "If we choose:\n",
    "\n",
    "$$\n",
    "w_i = \\frac{1}{f_i}\n",
    "$$\n",
    "\n",
    "Then expected contribution becomes:\n",
    "\n",
    "$$\n",
    "f_i \\cdot \\frac{1}{f_i} \\cdot \\nabla L_i\n",
    "= \\nabla L_i\n",
    "$$\n",
    "\n",
    "Meaning:\n",
    "\n",
    "Each class contributes equally in expectation.\n",
    "\n",
    "That’s the mathematical reason.\n",
    "\n",
    "\n",
    "**Concrete Numbers**\n",
    "\n",
    "Dataset:\n",
    "\n",
    "* A → 0.90\n",
    "* B → 0.05\n",
    "* C → 0.05\n",
    "\n",
    "Inverse frequency weights:\n",
    "\n",
    "$$\n",
    "w_A = \\frac{1}{0.90} \\approx 1.11\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_B = \\frac{1}{0.05} = 20\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_C = 20\n",
    "$$\n",
    "\n",
    "Now:\n",
    "\n",
    "Expected contribution:\n",
    "\n",
    "$$\n",
    "0.90 \\times 1.11 \\approx 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "0.05 \\times 20 = 1\n",
    "$$\n",
    "\n",
    "Balanced.\n",
    "\n",
    "\n",
    "**Why Not Use Huge Weights Directly?**\n",
    "\n",
    "Notice:\n",
    "\n",
    "Rare classes got weight 20.\n",
    "\n",
    "That’s large.\n",
    "\n",
    "If imbalance is extreme:\n",
    "\n",
    "Example:\n",
    "\n",
    "* A → 99%\n",
    "* B → 1%\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "w_B = 100\n",
    "$$\n",
    "\n",
    "Now gradients for B are 100× larger.\n",
    "\n",
    "This can cause:\n",
    "\n",
    "* Exploding gradients\n",
    "* Training instability\n",
    "* Overfitting minority samples\n",
    "\n",
    "That’s what “overcompensating” means.\n",
    "\n",
    "\n",
    "**Why Normalize?**\n",
    "\n",
    "Instead of raw inverse frequency:\n",
    "\n",
    "We often normalize:\n",
    "\n",
    "$$\n",
    "w_i = \\frac{1/f_i}{\\sum_j (1/f_j)}\n",
    "$$\n",
    "\n",
    "or scale so that:\n",
    "\n",
    "$$\n",
    "\\sum w_i = C\n",
    "$$\n",
    "\n",
    "This keeps gradient magnitudes reasonable.\n",
    "\n",
    "You preserve balance\n",
    "without blowing up updates.\n",
    "\n",
    "\n",
    "**Core Insight**\n",
    "\n",
    "You want:\n",
    "\n",
    "$$\n",
    "f_i \\cdot w_i \\approx \\text{constant}\n",
    "$$\n",
    "\n",
    "That’s the real objective.\n",
    "\n",
    "Inverse frequency achieves that.\n",
    "\n",
    "But scaling must be controlled.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481dfda",
   "metadata": {
    "papermill": {
     "duration": 0.004378,
     "end_time": "2026-02-26T08:48:45.100171",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.095793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Solution 2: Weighted Sampling\n",
    "\n",
    "Instead of modifying loss, modify sampling.\n",
    "\n",
    "Use:\n",
    "\n",
    "```python\n",
    "WeightedRandomSampler\n",
    "```\n",
    "\n",
    "Idea:\n",
    "\n",
    "* Oversample minority classes\n",
    "* Create balanced mini-batches\n",
    "\n",
    "Now each batch contains roughly equal class representation.\n",
    "\n",
    "Loss stays unchanged.\n",
    "\n",
    "**Strategic Difference**\n",
    "\n",
    "Weighted Loss:\n",
    "\n",
    "* Adjusts gradient magnitude\n",
    "\n",
    "Weighted Sampling:\n",
    "\n",
    "* Adjusts data distribution seen by optimizer\n",
    "\n",
    "Sampling often produces more stable training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e14b0",
   "metadata": {
    "papermill": {
     "duration": 0.004565,
     "end_time": "2026-02-26T08:48:45.109270",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.104705",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Solution 3. Targeted Augmentation\n",
    "\n",
    "Augment minority classes more aggressively.\n",
    "\n",
    "Example:\n",
    "\n",
    "* More rotations\n",
    "* Stronger color jitter\n",
    "* Random erasing\n",
    "\n",
    "This increases their effective data size.\n",
    "\n",
    "Instead of:\n",
    "\n",
    "5% raw samples,\n",
    "\n",
    "You increase their representation in training space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510eafdc",
   "metadata": {
    "papermill": {
     "duration": 0.004933,
     "end_time": "2026-02-26T08:48:45.118648",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.113715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Which One Is Better?\n",
    "\n",
    "It depends.\n",
    "\n",
    "* Mild imbalance → Weighted loss may be enough\n",
    "* Severe imbalance → Weighted sampling works better\n",
    "* Small minority dataset → Combine sampling + augmentation\n",
    "\n",
    "In high-risk domains (medical, fraud detection):\n",
    "\n",
    "You often combine all three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b671e577",
   "metadata": {
    "papermill": {
     "duration": 0.004491,
     "end_time": "2026-02-26T08:48:45.127818",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.123327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Final Mental Model\n",
    "\n",
    "Imbalanced data → biased gradients → biased model.\n",
    "\n",
    "Fix by:\n",
    "\n",
    "1. Reweighting loss\n",
    "2. Resampling data\n",
    "3. Expanding minority manifold via augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bab5c2",
   "metadata": {
    "papermill": {
     "duration": 0.004384,
     "end_time": "2026-02-26T08:48:45.136597",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.132213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Top-k Accuracy\n",
    "\n",
    "<p style=\"text-align:center; color:red; font-size:18px;\">To be continue...</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c7396",
   "metadata": {
    "papermill": {
     "duration": 0.004518,
     "end_time": "2026-02-26T08:48:45.145625",
     "exception": false,
     "start_time": "2026-02-26T08:48:45.141107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "<p style=\"text-align:center; color:skyblue; font-size:18px;\">\n",
    "© 2026 Mostafizur Rahman\n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31286,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.166372,
   "end_time": "2026-02-26T08:48:45.571463",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-26T08:48:41.405091",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
