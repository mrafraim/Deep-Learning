{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-39-cnn-multi-class-classification?scriptVersionId=300741287\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"412ce82c","metadata":{"papermill":{"duration":0.00541,"end_time":"2026-03-01T03:40:46.326463","exception":false,"start_time":"2026-03-01T03:40:46.321053","status":"completed"},"tags":[]},"source":["# Day 39: CNN Multi-Class Classification\n","\n","Welcome to Day 39!\n","\n","Today You’ll Learn\n","\n","1. How CNNs produce multi-class predictions  \n","2. What logits really represent  \n","3. Softmax and probability interpretation  \n","4. Why CrossEntropyLoss includes Softmax internally  \n","5. Handling class imbalance properly  \n","6. Top-k accuracy and when to use it  \n","7. Practical evaluation strategy for real-world CNNs\n","\n","If you found this notebook helpful, your **<b style=\"color:skyblue;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---"]},{"cell_type":"markdown","id":"a3c438df","metadata":{"papermill":{"duration":0.004122,"end_time":"2026-03-01T03:40:46.336098","exception":false,"start_time":"2026-03-01T03:40:46.331976","status":"completed"},"tags":[]},"source":["# Multi-Class Classification"]},{"cell_type":"markdown","id":"d3d1f0a3","metadata":{"papermill":{"duration":0.004064,"end_time":"2026-03-01T03:40:46.344499","exception":false,"start_time":"2026-03-01T03:40:46.340435","status":"completed"},"tags":[]},"source":["## Problem Setup\n","\n","You are given:\n","\n","- An image $x$  \n","- $C$ possible classes  \n","\n","Goal:\n","\n","$$\n","\\text{Predict exactly ONE class out of } C\n","$$\n","\n","This is called:\n","\n","**Single-label multi-class classification**\n","\n","Classes are mutually exclusive.\n","\n","Example:\n","\n","- CIFAR-10 → $C = 10$  \n","- ImageNet → $C = 1000$"]},{"cell_type":"markdown","id":"cc0a4a9d","metadata":{"papermill":{"duration":0.004081,"end_time":"2026-03-01T03:40:46.352793","exception":false,"start_time":"2026-03-01T03:40:46.348712","status":"completed"},"tags":[]},"source":["## What the CNN Produces\n","\n","After feature extraction, the CNN outputs a vector:\n","\n","```python\n","Linear(features, C)\n","```\n","\n","Mathematically:\n","\n","If final feature vector is:\n","\n","$$\n","h \\in \\mathbb{R}^{d}\n","$$\n","\n","Then:\n","\n","$$\n","z = Wh + b\n","$$\n","\n","Where:\n","\n","* $W \\in \\mathbb{R}^{C \\times d}$\n","* $b \\in \\mathbb{R}^{C}$\n","\n","Output:\n","\n","$$\n","z = [z_1, z_2, ..., z_C]\n","$$\n"]},{"cell_type":"markdown","id":"a9aebb80","metadata":{"papermill":{"duration":0.00407,"end_time":"2026-03-01T03:40:46.36115","exception":false,"start_time":"2026-03-01T03:40:46.35708","status":"completed"},"tags":[]},"source":["## What Are Logits?\n","\n","Logits are the raw output scores of a neural network before converting them into probabilities.\n","\n","The values $z_i$ are called:\n","\n","* Logits\n","* Raw scores\n","* Unnormalized predictions\n","\n","Important:\n","\n","$$\n","z_i \\in (-\\infty, +\\infty)\n","$$\n","\n","They are **NOT probabilities**.\n","\n","They are simply the output of the final linear layer:\n","\n","$$\n","z = Wh + b\n","$$\n","\n","Where:\n","\n","- $h$ = feature vector  \n","- $W$ = weight matrix  \n","- $b$ = bias  \n","\n","### Key Properties\n","\n","- Logits are real numbers  \n","- They are NOT probabilities  \n","- They do NOT sum to 1  \n","- They can be negative or positive\n","\n","Example:\n","\n","If $C = 3$:\n","\n","$$\n","z = [2.3,\\ -1.1,\\ 0.7]\n","$$\n","\n","These are just scores."]},{"cell_type":"markdown","id":"fa916266","metadata":{"papermill":{"duration":0.004053,"end_time":"2026-03-01T03:40:46.369373","exception":false,"start_time":"2026-03-01T03:40:46.36532","status":"completed"},"tags":[]},"source":["## How Prediction Works\n","\n","We choose:\n","\n","$$\n","\\hat{y} = \\arg\\max_i z_i\n","$$\n","\n","Meaning:\n","\n","Pick the class with the largest score.\n","\n","Example:\n","\n","$$\n","[2.3,\\ -1.1,\\ 0.7]\n","$$\n","\n","Largest = $2.3$\n","So predict class 1.\n","\n","Only ranking matters.\n","\n","Scale does NOT matter.\n","\n","Example:\n","\n","$$\n","[100,\\ 50,\\ -20]\n","$$\n","\n","Still class 1."]},{"cell_type":"markdown","id":"09c5ed5e","metadata":{"papermill":{"duration":0.004098,"end_time":"2026-03-01T03:40:46.377749","exception":false,"start_time":"2026-03-01T03:40:46.373651","status":"completed"},"tags":[]},"source":["## Why They Are Not Probabilities\n","\n","Probabilities must:\n","\n","* Be between $0$ and $1$\n","* Sum to $1$\n","\n","Logits:\n","\n","* Can be negative\n","* Can be large\n","* Do NOT sum to $1$\n","\n","They are raw evidence scores.\n","\n","Probabilities are obtained later using Softmax.\n"]},{"cell_type":"markdown","id":"a3daa482","metadata":{"papermill":{"duration":0.004085,"end_time":"2026-03-01T03:40:46.386036","exception":false,"start_time":"2026-03-01T03:40:46.381951","status":"completed"},"tags":[]},"source":["\n","---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\">Optional: Explain $$ \\hat{y} = \\arg\\max_i z_i $$ </p>\n","\n","Let’s decode it slowly.\n","\n","\n","**Step 1. What Is $z_i$?**\n","\n","The model outputs:\n","\n","$$\n","z = [z_1, z_2, ..., z_C]\n","$$\n","\n","Each $z_i$ is a score for class $i$.\n","\n","Example (3 classes):\n","\n","$$\n","z = [2.3,\\ -1.1,\\ 0.7]\n","$$\n","\n","**Step 2. What Does “max” Mean?**\n","\n","The maximum value here is:\n","\n","$$\n","2.3\n","$$\n","\n","That’s the largest score.\n","\n","**Step 3. What Does “argmax” Mean?**\n","\n","Important:\n","\n","- **max** → gives the value  \n","- **argmax** → gives the index (position)\n","\n","Example:\n","\n","$$\n","z = [2.3,\\ -1.1,\\ 0.7]\n","$$\n","\n","- max = 2.3  \n","- argmax = 1  \n","\n","(assuming indexing starts from 1)\n","\n","Because 2.3 is at position 1.\n","\n","**Step 4. What Is $\\hat{y}$?**\n","\n","$\\hat{y}$ means:\n","\n","> Predicted label\n","\n","So:\n","\n","$$\n","\\hat{y} = \\arg\\max_i z_i\n","$$\n","\n","means:\n","\n","> The predicted class is the index of the largest score.\n","\n","\n","**Super Simple Version**\n","\n","The model outputs scores:\n","\n","Class 1 → 2.3<br>\n","Class 2 → -1.1<br>\n","Class 3 → 0.7\n","\n","Biggest score = 2.3  \n","So prediction = Class 1.\n","\n","**One-Line Meaning**\n","\n","$$\n","\\hat{y} = \\arg\\max_i z_i\n","$$\n","\n","means:\n","\n","> Pick the class with the highest score.\n","\n","---"]},{"cell_type":"markdown","id":"b5cdcccd","metadata":{"papermill":{"duration":0.004118,"end_time":"2026-03-01T03:40:46.394363","exception":false,"start_time":"2026-03-01T03:40:46.390245","status":"completed"},"tags":[]},"source":["# Softmax\n","\n","Softmax is a normalization function that converts logits into a probability distribution across classes.\n","\n","Given logits:\n","\n","$$\n","z = [z_1, z_2, ..., z_C]\n","$$\n","\n","Softmax computes:\n","\n","$$\n","\\text{Softmax}(z_i) =\n","\\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\n","$$\n","\n","for each class $i$.\n","\n","Softmax transforms:\n","\n","$$\n","(-\\infty, +\\infty)\n","$$\n","\n","into:\n","\n","$$\n","(0,1)\n","$$\n","\n","while ensuring:\n","\n","$$\n","\\sum_{i=1}^{C} P(y=i) = 1\n","$$\n","\n","So the outputs become valid probabilities.\n","\n","**Key Properties**\n","\n","1. Outputs are between 0 and 1  \n","2. Outputs sum to 1  \n","3. Preserves ranking (largest logit → largest probability)  \n","4. Sensitive to relative differences"]},{"cell_type":"markdown","id":"dd43d8b2","metadata":{"papermill":{"duration":0.004168,"end_time":"2026-03-01T03:40:46.402749","exception":false,"start_time":"2026-03-01T03:40:46.398581","status":"completed"},"tags":[]},"source":["## Manual Softmax Example\n","\n","Suppose the model outputs:\n","\n","$$\n","z = [2.0,\\ 1.0,\\ 0.1]\n","$$\n","\n","These are raw scores.\n","\n","Compute:\n","\n","$$\n","e^{z_i}\n","$$\n","\n","Using approximate values:\n","\n","$$\n","e^{2.0} \\approx 7.39\n","$$\n","\n","$$\n","e^{1.0} \\approx 2.72\n","$$\n","\n","$$\n","e^{0.1} \\approx 1.11\n","$$\n","\n","Now we have:\n","\n","$$\n","[7.39,\\ 2.72,\\ 1.11]\n","$$\n","\n","Compute the Sum:\n","\n","$$\n","7.39 + 2.72 + 1.11 = 11.22\n","$$\n","\n","\n","Divide Each by the Sum\n","\n","Softmax formula:\n","\n","$$\n","P(y=i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n","$$\n","\n","Now compute probabilities:\n","\n","Class 1:\n","\n","$$\n","\\frac{7.39}{11.22} \\approx 0.66\n","$$\n","\n","Class 2:\n","\n","$$\n","\\frac{2.72}{11.22} \\approx 0.24\n","$$\n","\n","Class 3:\n","\n","$$\n","\\frac{1.11}{11.22} \\approx 0.10\n","$$\n","\n","\n","Final Softmax Output:\n","\n","$$\n","[0.66,\\ 0.24,\\ 0.10]\n","$$\n","\n","\n","**Check**\n","\n","Do they sum to 1?\n","\n","$$\n","0.66 + 0.24 + 0.10 = 1.00\n","$$\n","\n","Yes.\n","\n","**Interpretation**\n","\n","The model assigns:\n","\n","- 66% probability to class 1  \n","- 24% to class 2  \n","- 10% to class 3  \n","\n","Prediction = class 1 (highest probability)."]},{"cell_type":"markdown","id":"e924eb56","metadata":{"papermill":{"duration":0.004059,"end_time":"2026-03-01T03:40:46.410956","exception":false,"start_time":"2026-03-01T03:40:46.406897","status":"completed"},"tags":[]},"source":["## Why Exponential?\n","\n","The exponential function:\n","\n","- Makes all outputs positive  \n","- Magnifies larger logits more strongly  \n","\n","If one logit is slightly larger, its probability increases significantly."]},{"cell_type":"markdown","id":"d510b949","metadata":{"papermill":{"duration":0.004158,"end_time":"2026-03-01T03:40:46.419641","exception":false,"start_time":"2026-03-01T03:40:46.415483","status":"completed"},"tags":[]},"source":["## Critical Rule (PyTorch)\n","\n","### Do NOT apply Softmax before CrossEntropyLoss\n","\n","```python\n","criterion = torch.nn.CrossEntropyLoss()\n","```\n","\n","Why?\n","\n","Because `CrossEntropyLoss` already computes:\n","\n","$$\n","\\text{LogSoftmax} + \\text{Negative Log Likelihood}\n","$$\n","\n","Internally it does:\n","\n","$$\n","\\log\\left(\\frac{e^{z_i}}{\\sum_j e^{z_j}}\\right)\n","$$\n","\n","If you apply Softmax manually first:\n","\n","* You exponentiate twice\n","* You reduce numerical stability\n","* You can cause gradient issues\n","\n","### Correct Usage\n","\n","Model should output **logits**, not probabilities.\n","\n","Pass logits directly to:\n","\n","```python\n","loss = criterion(logits, labels)\n","```\n"]},{"cell_type":"markdown","id":"4aadc1fd","metadata":{"papermill":{"duration":0.00413,"end_time":"2026-03-01T03:40:46.42802","exception":false,"start_time":"2026-03-01T03:40:46.42389","status":"completed"},"tags":[]},"source":["# CrossEntropy Loss\n","\n","CrossEntropy Loss is the negative log of the probability assigned to the true class, small when confident and correct, huge when confident and wrong.\n","\n","For the true class $y$:\n","\n","$$\n","L = -\\log(P(y))\n","$$\n","\n","It only cares about the probability assigned to the correct class."]},{"cell_type":"markdown","id":"9700d719","metadata":{"papermill":{"duration":0.00414,"end_time":"2026-03-01T03:40:46.43642","exception":false,"start_time":"2026-03-01T03:40:46.43228","status":"completed"},"tags":[]},"source":["## What This Means\n","\n","If the model assigns:\n","\n","- High probability → small loss  \n","- Low probability → large loss  \n","\n","Because:\n","\n","- $\\log(1) = 0$\n","- $\\log(\\text{small number})$ is very negative  \n","- Negative sign makes it large positive loss  "]},{"cell_type":"markdown","id":"76f8a847","metadata":{"papermill":{"duration":0.00419,"end_time":"2026-03-01T03:40:46.444943","exception":false,"start_time":"2026-03-01T03:40:46.440753","status":"completed"},"tags":[]},"source":["## Manual Example 1. Moderate Confidence\n","\n","Suppose true class = Class 1.\n","\n","Model prediction:\n","\n","$$\n","P = [0.51,\\ 0.30,\\ 0.19]\n","$$\n","\n","Correct class probability:\n","\n","$$\n","P(y) = 0.51\n","$$\n","\n","Compute loss:\n","\n","$$\n","L = -\\log(0.51)\n","$$\n","\n","Using natural log:\n","\n","$$\n","\\log(0.51) \\approx -0.673\n","$$\n","\n","So:\n","\n","$$\n","L \\approx 0.673\n","$$"]},{"cell_type":"markdown","id":"b748c496","metadata":{"papermill":{"duration":0.004138,"end_time":"2026-03-01T03:40:46.453346","exception":false,"start_time":"2026-03-01T03:40:46.449208","status":"completed"},"tags":[]},"source":["## Manual Example 2. High Confidence\n","\n","Now model predicts:\n","\n","$$\n","P = [0.95,\\ 0.03,\\ 0.02]\n","$$\n","\n","Correct class probability:\n","\n","$$\n","P(y) = 0.95\n","$$\n","\n","Compute:\n","\n","$$\n","L = -\\log(0.95)\n","$$\n","\n","$$\n","\\log(0.95) \\approx -0.051\n","$$\n","\n","So:\n","\n","$$\n","L \\approx 0.051\n","$$\n","\n","Much smaller loss.\n"]},{"cell_type":"markdown","id":"dd46c39e","metadata":{"papermill":{"duration":0.004112,"end_time":"2026-03-01T03:40:46.461677","exception":false,"start_time":"2026-03-01T03:40:46.457565","status":"completed"},"tags":[]},"source":["## Manual Example 3. Confident but Wrong\n","\n","True class = Class 1.\n","\n","Model predicts:\n","\n","$$\n","P = [0.01,\\ 0.97,\\ 0.02]\n","$$\n","\n","Correct class probability:\n","\n","$$\n","P(y) = 0.01\n","$$\n","\n","Compute:\n","\n","$$\n","L = -\\log(0.01)\n","$$\n","\n","$$\n","\\log(0.01) \\approx -4.605\n","$$\n","\n","So:\n","\n","$$\n","L \\approx 4.605\n","$$\n","\n","Huge loss."]},{"cell_type":"markdown","id":"27bc96bd","metadata":{"papermill":{"duration":0.004122,"end_time":"2026-03-01T03:40:46.469982","exception":false,"start_time":"2026-03-01T03:40:46.46586","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\"> Clear Your Confusion (Optional) </p>\n","\n","#### Prediction Step (Argmax)\n","\n","When predicting, we pick the largest probability:\n","\n","$$\n","P = [0.01,\\ 0.97,\\ 0.02]\n","$$\n","\n","Largest value = 0.97\n","So the model predicts:\n","\n","Class 2\n","\n","Correct.\n","\n","That’s **prediction**.\n","\n","\n","#### Loss Step (Training)\n","\n","Loss does NOT care what the model predicted.\n","\n","Loss cares about:\n","\n","> What probability did the model assign to the TRUE class?\n","\n","You told me:\n","\n","True class = Class 1.\n","\n","So the correct class index is 1.\n","\n","Therefore:\n","\n","$$\n","P(y) = P(\\text{true class}) = 0.01\n","$$\n","\n","Not 0.97.\n","\n","Because 0.97 belongs to Class 2.\n","\n","\n","#### Why We Don’t Use 0.97\n","\n","Because 0.97 is the probability of the **wrong class**.\n","\n","The loss must measure:\n","\n","> How wrong was the model about the correct answer?\n","\n","The model was extremely confident in Class 2 (0.97).\n","\n","But Class 2 is wrong.\n","\n","So the model gave only 0.01 probability to the true class.\n","\n","That is terrible.\n","\n","So loss becomes large:\n","\n","$$\n","L = -\\log(0.01) \\approx 4.605\n","$$\n","\n","Huge penalty.\n","\n","\n","#### The Core Rule\n","\n","Prediction uses:\n","\n","$$\n","\\hat{y} = \\arg\\max_i P_i\n","$$\n","\n","Loss uses:\n","\n","$$\n","L = -\\log(P_{\\text{true class}})\n","$$\n","\n","Two completely different things.\n","\n","---"]},{"cell_type":"markdown","id":"ebf9ef12","metadata":{"papermill":{"duration":0.004267,"end_time":"2026-03-01T03:40:46.478573","exception":false,"start_time":"2026-03-01T03:40:46.474306","status":"completed"},"tags":[]},"source":["\n","## Why CrossEntropy Works So Well\n","\n","It:\n","\n","- Penalizes confident wrong predictions heavily  \n","- Rewards confident correct predictions  \n","- Pushes probability mass toward true class  \n","\n","It doesn’t just optimize accuracy.\n","\n","It optimizes **confidence quality**."]},{"cell_type":"markdown","id":"7a527abb","metadata":{"papermill":{"duration":0.004125,"end_time":"2026-03-01T03:40:46.486957","exception":false,"start_time":"2026-03-01T03:40:46.482832","status":"completed"},"tags":[]},"source":["## Key Insight\n","\n","Accuracy treats:\n","\n","- 0.51 and 0.95 the same  \n","\n","CrossEntropy does not.\n","\n","It forces the model to:\n","\n","> Be correct and be confident."]},{"cell_type":"markdown","id":"06db6589","metadata":{"papermill":{"duration":0.004144,"end_time":"2026-03-01T03:40:46.495345","exception":false,"start_time":"2026-03-01T03:40:46.491201","status":"completed"},"tags":[]},"source":["# Where Does Softmax Fit?\n","\n","We’ll go through training flow mathematically, cleanly and precisely.\n","\n","**Step 0; Model Output (Logits)**\n","\n","The model outputs raw scores:\n","\n","$$\n","z = [2.0,\\ 1.0,\\ 0.1]\n","$$\n","\n","These are logits.\n","\n","They are just real numbers:\n","\n","$$\n","z_i \\in (-\\infty, +\\infty)\n","$$\n","\n","No probabilities yet.\n","\n","\n","**Step 1. Apply Softmax (Convert to Probabilities)**\n","\n","Softmax formula:\n","\n","$$\n","P(y=i) = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\n","$$\n","\n","Compute exponentials\n","\n","$$\n","e^{2.0} \\approx 7.39\n","$$\n","\n","$$\n","e^{1.0} \\approx 2.72\n","$$\n","\n","$$\n","e^{0.1} \\approx 1.11\n","$$\n","\n","Compute denominator\n","\n","$$\n","7.39 + 2.72 + 1.11 = 11.22\n","$$\n","\n","Compute probabilities\n","\n","$$\n","P_1 = \\frac{7.39}{11.22} \\approx 0.66\n","$$\n","\n","$$\n","P_2 = \\frac{2.72}{11.22} \\approx 0.24\n","$$\n","\n","$$\n","P_3 = \\frac{1.11}{11.22} \\approx 0.10\n","$$\n","\n","Now we have:\n","\n","$$\n","P = [0.66,\\ 0.24,\\ 0.10]\n","$$\n","\n","This is where **Softmax happens**.\n","\n","\n","**Step 2. Compute CrossEntropy Loss**\n","\n","Definition:\n","\n","$$\n","L = -\\log P(y)\n","$$\n","\n","Assume true class is **Class 1**.\n","\n","So we use:\n","\n","$$\n","P(y) = 0.66\n","$$\n","\n","Compute:\n","\n","$$\n","L = -\\log(0.66)\n","$$\n","\n","$$\n","\\log(0.66) \\approx -0.415\n","$$\n","\n","So:\n","\n","$$\n","L \\approx 0.415\n","$$\n","\n","Small loss → good prediction.\n","\n","\n","**Step 3. Confident but Wrong Case**\n","\n","Suppose logits are:\n","\n","$$\n","z = [0.1,\\ 4.0,\\ 0.2]\n","$$\n","\n","### Apply Softmax\n","\n","Exponentials:\n","\n","$$\n","e^{0.1} \\approx 1.11\n","$$\n","\n","$$\n","e^{4.0} \\approx 54.6\n","$$\n","\n","$$\n","e^{0.2} \\approx 1.22\n","$$\n","\n","Sum:\n","\n","$$\n","1.11 + 54.6 + 1.22 \\approx 56.93\n","$$\n","\n","Probabilities:\n","\n","$$\n","P \\approx [0.02,\\ 0.96,\\ 0.02]\n","$$\n","\n","True class is still Class 1.\n","\n","So:\n","\n","$$\n","L = -\\log(0.02)\n","$$\n","\n","$$\n","\\log(0.02) \\approx -3.91\n","$$\n","\n","$$\n","L \\approx 3.91\n","$$\n","\n","Huge loss.\n","\n","Because the model gave very low probability to the true class.\n","\n","\n","### Where Softmax Happens in PyTorch\n","\n","When you write:\n","\n","```python\n","criterion = torch.nn.CrossEntropyLoss()\n","loss = criterion(logits, target)\n","```\n","\n","Internally PyTorch computes:\n","\n","$$\n","-\\left(\n","z_y - \\log \\sum_j e^{z_j}\n","\\right)\n","$$\n","\n","This expression is mathematically equivalent to:\n","\n","$$\n","-\\log \\left(\n","\\frac{e^{z_y}}{\\sum_j e^{z_j}}\n","\\right)\n","$$\n","\n","Which is:\n","\n","$$\n","-\\log(\\text{Softmax}(z_y))\n","$$\n","\n","So during training:\n","\n","$$\n","\\text{Softmax is applied internally.}\n","$$\n","\n","You do NOT call it manually.\n","\n","\n","### Full Pipeline Summary\n","\n","**During Training**\n","\n","$$\n","\\text{Logits} \\rightarrow \\text{CrossEntropyLoss}\n","$$\n","\n","Softmax is inside the loss.\n","\n","**During Inference**\n","\n","If you want probabilities:\n","\n","$$\n","\\text{Logits} \\rightarrow \\text{Softmax} \\rightarrow \\text{Probabilities}\n","$$\n"]},{"cell_type":"markdown","id":"24bb5b2f","metadata":{"papermill":{"duration":0.004423,"end_time":"2026-03-01T03:40:46.504005","exception":false,"start_time":"2026-03-01T03:40:46.499582","status":"completed"},"tags":[]},"source":["# Class Imbalance\n","\n","Suppose dataset distribution:\n","\n","- Class A → 90%  \n","- Class B → 5%  \n","- Class C → 5%  \n","\n","A lazy model can learn:\n","\n","$$\n","\\hat{y} = \\text{Class A (always)}\n","$$\n","\n","Accuracy:\n","\n","$$\n","90\\%\n","$$\n","\n","But:\n","\n","- Recall for B = 0  \n","- Recall for C = 0  \n","\n","Model is statistically “good” but practically useless."]},{"cell_type":"markdown","id":"7cf6c649","metadata":{"papermill":{"duration":0.004013,"end_time":"2026-03-01T03:40:46.512156","exception":false,"start_time":"2026-03-01T03:40:46.508143","status":"completed"},"tags":[]},"source":["## Why This Happens\n","\n","CrossEntropy minimizes **average loss**:\n","\n","$$\n","L = -\\log P(y)\n","$$\n","\n","If 90% of samples are Class A:\n","\n","- Gradients are dominated by Class A  \n","- The optimizer mostly improves Class A  \n","- Minority classes barely influence updates  \n","\n","The optimization objective is skewed."]},{"cell_type":"markdown","id":"d91f8d28","metadata":{"papermill":{"duration":0.003988,"end_time":"2026-03-01T03:40:46.520297","exception":false,"start_time":"2026-03-01T03:40:46.516309","status":"completed"},"tags":[]},"source":["## Solution 1: Weighted Loss\n","\n","Modify loss:\n","\n","$$\n","L = -w_y \\log P(y)\n","$$\n","\n","Where:\n","\n","- $w_y$ = weight of the true class  \n","\n","Rare classes get larger weights.\n","\n","Example:\n","\n","```python\n","class_weights = torch.tensor([0.2, 0.4, 0.4]).to(device)\n","criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","```\n","\n","Effect:\n","\n","* Errors on minority classes produce larger gradients\n","* Optimizer is forced to care about them\n","\n","\n","**Important Insight**\n","\n","Weights should often be:\n","\n","$$\n","w_i \\propto \\frac{1}{\\text{frequency}_i}\n","$$\n","\n","Or normalized inverse frequency.\n","\n","Otherwise you risk:\n","\n","* Overcompensating\n","* Training instability\n"]},{"cell_type":"markdown","id":"c7567b26","metadata":{"papermill":{"duration":0.004092,"end_time":"2026-03-01T03:40:46.528599","exception":false,"start_time":"2026-03-01T03:40:46.524507","status":"completed"},"tags":[]},"source":["---\n","\n","### <p style=\"text-align:center; color:orange; font-size:18px;\"> Inverse Frequency (Optional) </p>\n","\n","We want each class to contribute roughly equally to training.\n","\n","Suppose dataset:\n","\n","* Class A → 90%\n","* Class B → 5%\n","* Class C → 5%\n","\n","If we do nothing:\n","\n","Expected gradient contribution:\n","\n","$$\n","0.9 \\nabla L_A + 0.05 \\nabla L_B + 0.05 \\nabla L_C\n","$$\n","\n","Class A dominates updates.\n","\n","So we increase weights for rare classes.\n","\n","\n","**Why Inverse Frequency?**\n","\n","If a class appears less often, we want its total influence over time to match others.\n","\n","Let frequency of class $i$ be:\n","\n","$$\n","f_i\n","$$\n","\n","If we choose:\n","\n","$$\n","w_i = \\frac{1}{f_i}\n","$$\n","\n","Then expected contribution becomes:\n","\n","$$\n","f_i \\cdot \\frac{1}{f_i} \\cdot \\nabla L_i\n","= \\nabla L_i\n","$$\n","\n","Meaning:\n","\n","Each class contributes equally in expectation.\n","\n","That’s the mathematical reason.\n","\n","\n","**Concrete Numbers**\n","\n","Dataset:\n","\n","* A → 0.90\n","* B → 0.05\n","* C → 0.05\n","\n","Inverse frequency weights:\n","\n","$$\n","w_A = \\frac{1}{0.90} \\approx 1.11\n","$$\n","\n","$$\n","w_B = \\frac{1}{0.05} = 20\n","$$\n","\n","$$\n","w_C = 20\n","$$\n","\n","Now:\n","\n","Expected contribution:\n","\n","$$\n","0.90 \\times 1.11 \\approx 1\n","$$\n","\n","$$\n","0.05 \\times 20 = 1\n","$$\n","\n","Balanced.\n","\n","\n","**Why Not Use Huge Weights Directly?**\n","\n","Notice:\n","\n","Rare classes got weight 20.\n","\n","That’s large.\n","\n","If imbalance is extreme:\n","\n","Example:\n","\n","* A → 99%\n","* B → 1%\n","\n","Then:\n","\n","$$\n","w_B = 100\n","$$\n","\n","Now gradients for B are 100× larger.\n","\n","This can cause:\n","\n","* Exploding gradients\n","* Training instability\n","* Overfitting minority samples\n","\n","That’s what “overcompensating” means.\n","\n","\n","**Why Normalize?**\n","\n","Instead of raw inverse frequency:\n","\n","We often normalize:\n","\n","$$\n","w_i = \\frac{1/f_i}{\\sum_j (1/f_j)}\n","$$\n","\n","or scale so that:\n","\n","$$\n","\\sum w_i = C\n","$$\n","\n","This keeps gradient magnitudes reasonable.\n","\n","You preserve balance\n","without blowing up updates.\n","\n","\n","**Core Insight**\n","\n","You want:\n","\n","$$\n","f_i \\cdot w_i \\approx \\text{constant}\n","$$\n","\n","That’s the real objective.\n","\n","Inverse frequency achieves that.\n","\n","But scaling must be controlled.\n","\n","---\n"]},{"cell_type":"markdown","id":"724451a0","metadata":{"papermill":{"duration":0.004091,"end_time":"2026-03-01T03:40:46.536973","exception":false,"start_time":"2026-03-01T03:40:46.532882","status":"completed"},"tags":[]},"source":["## Solution 2: Weighted Sampling\n","\n","Instead of modifying loss, modify sampling.\n","\n","Use:\n","\n","```python\n","WeightedRandomSampler\n","```\n","\n","Idea:\n","\n","* Oversample minority classes\n","* Create balanced mini-batches\n","\n","Now each batch contains roughly equal class representation.\n","\n","Loss stays unchanged.\n","\n","**Strategic Difference**\n","\n","Weighted Loss:\n","\n","* Adjusts gradient magnitude\n","\n","Weighted Sampling:\n","\n","* Adjusts data distribution seen by optimizer\n","\n","Sampling often produces more stable training."]},{"cell_type":"markdown","id":"94937388","metadata":{"papermill":{"duration":0.004039,"end_time":"2026-03-01T03:40:46.545173","exception":false,"start_time":"2026-03-01T03:40:46.541134","status":"completed"},"tags":[]},"source":["## Solution 3. Targeted Augmentation\n","\n","Augment minority classes more aggressively.\n","\n","Example:\n","\n","* More rotations\n","* Stronger color jitter\n","* Random erasing\n","\n","This increases their effective data size.\n","\n","Instead of:\n","\n","5% raw samples,\n","\n","You increase their representation in training space.\n"]},{"cell_type":"markdown","id":"018295d9","metadata":{"papermill":{"duration":0.004121,"end_time":"2026-03-01T03:40:46.553482","exception":false,"start_time":"2026-03-01T03:40:46.549361","status":"completed"},"tags":[]},"source":["## Which One Is Better?\n","\n","It depends.\n","\n","* Mild imbalance → Weighted loss may be enough\n","* Severe imbalance → Weighted sampling works better\n","* Small minority dataset → Combine sampling + augmentation\n","\n","In high-risk domains (medical, fraud detection):\n","\n","You often combine all three."]},{"cell_type":"markdown","id":"4248534a","metadata":{"papermill":{"duration":0.004108,"end_time":"2026-03-01T03:40:46.561778","exception":false,"start_time":"2026-03-01T03:40:46.55767","status":"completed"},"tags":[]},"source":["## Final Mental Model\n","\n","Imbalanced data → biased gradients → biased model.\n","\n","Fix by:\n","\n","1. Reweighting loss\n","2. Resampling data\n","3. Expanding minority manifold via augmentation\n"]},{"cell_type":"markdown","id":"fc6164ba","metadata":{"papermill":{"duration":0.004057,"end_time":"2026-03-01T03:40:46.57002","exception":false,"start_time":"2026-03-01T03:40:46.565963","status":"completed"},"tags":[]},"source":["# Top-k Accuracy\n","\n","Let the model output logits:\n","\n","$$\n","z = [z_1, z_2, ..., z_C]\n","$$\n","\n","Sort them in descending order.\n","\n","Top-k accuracy checks:\n","\n","> Is the true class index inside the k largest logits?\n","\n","Formally:\n","\n","Let $S_k$ = indices of the top-k largest scores.\n","\n","Top-k is correct if:\n","\n","$$\n","y \\in S_k\n","$$"]},{"cell_type":"markdown","id":"856da0c9","metadata":{"papermill":{"duration":0.004067,"end_time":"2026-03-01T03:40:46.578253","exception":false,"start_time":"2026-03-01T03:40:46.574186","status":"completed"},"tags":[]},"source":["## Difference from Normal Accuracy\n","\n","Top-1 accuracy:\n","\n","$$\n","\\hat{y} = \\arg\\max_i z_i\n","$$\n","\n","Correct if:\n","\n","$$\n","\\hat{y} = y\n","$$\n","\n","Top-k accuracy:\n","\n","Correct if true class appears in top k guesses."]},{"cell_type":"markdown","id":"1a056c74","metadata":{"papermill":{"duration":0.004055,"end_time":"2026-03-01T03:40:46.586484","exception":false,"start_time":"2026-03-01T03:40:46.582429","status":"completed"},"tags":[]},"source":["## Manual Example - 1\n","\n","Suppose:\n","\n","True class = **Class 4**\n","\n","Logits:\n","\n","$$\n","z = [2.5,\\ 1.9,\\ 1.2,\\ 2.1,\\ 0.3]\n","$$\n","\n","Let’s rank them:\n","\n","| Class | Score |\n","|--------|--------|\n","| 1 | 2.5 |\n","| 4 | 2.1 |\n","| 2 | 1.9 |\n","| 3 | 1.2 |\n","| 5 | 0.3 |\n","\n","**Top-1**\n","\n","Highest score = Class 1  \n","\n","Prediction = 1  \n","\n","True class = 4  \n","\n","Top-1 → ❌ Wrong  \n","\n","\n","**Top-2**\n","\n","Top 2 classes:\n","\n","$$\n","[1, 4]\n","$$\n","\n","True class = 4  \n","\n","Top-2 → ✅ Correct  \n","\n","**Top-3**\n","\n","Top 3 classes:\n","\n","$$\n","[1, 4, 2]\n","$$\n","\n","True class inside → ✅ Correct  "]},{"cell_type":"markdown","id":"199517ee","metadata":{"papermill":{"duration":0.004124,"end_time":"2026-03-01T03:40:46.594878","exception":false,"start_time":"2026-03-01T03:40:46.590754","status":"completed"},"tags":[]},"source":["## Manual Example - 2\n","\n","Suppose 1000 classes.\n","\n","Model predicts top 5:\n","\n","$$\n","[dog,\\ wolf,\\ fox,\\ coyote,\\ husky]\n","$$\n","\n","True class = wolf\n","\n","Top-1 → dog → ❌  \n","Top-5 → wolf included → ✅  \n","\n","This is why ImageNet reports **Top-5 accuracy**."]},{"cell_type":"markdown","id":"ff0db10a","metadata":{"papermill":{"duration":0.004453,"end_time":"2026-03-01T03:40:46.603542","exception":false,"start_time":"2026-03-01T03:40:46.599089","status":"completed"},"tags":[]},"source":["## What Top-k Really Measures\n","\n","It measures:\n","\n","> How well the model ranks the correct class.\n","\n","It’s a ranking metric.\n","\n","Not just classification correctness.\n","\n","Top-1 asks:\n","\n","> Did the model’s first guess match?\n","\n","Top-k asks:\n","\n","> Was the correct answer among its first k guesses?"]},{"cell_type":"markdown","id":"a85f77a4","metadata":{"papermill":{"duration":0.004164,"end_time":"2026-03-01T03:40:46.612748","exception":false,"start_time":"2026-03-01T03:40:46.608584","status":"completed"},"tags":[]},"source":["## Why This Matters\n","\n","In large-class problems:\n","\n","- Choosing the exact highest class is hard.\n","\n","But being in top-5 means:\n","\n","- Model roughly understands the image.\n","\n","Top-k measures **ranking quality**, not just strict correctness.\n"]},{"cell_type":"markdown","id":"0169fe90","metadata":{"papermill":{"duration":0.004075,"end_time":"2026-03-01T03:40:46.620979","exception":false,"start_time":"2026-03-01T03:40:46.616904","status":"completed"},"tags":[]},"source":["## When to Use Top-k\n","\n","Use Top-k when:\n","\n","* Number of classes is large\n","* Classes are visually similar\n","* Ranking quality matters\n","\n","Examples:\n","\n","* Image classification (ImageNet)\n","* Recommendation systems\n","* Retrieval systems"]},{"cell_type":"markdown","id":"6ed30d2a","metadata":{"papermill":{"duration":0.004126,"end_time":"2026-03-01T03:40:46.629246","exception":false,"start_time":"2026-03-01T03:40:46.62512","status":"completed"},"tags":[]},"source":["## PyTorch Implementation\n","\n","```python\n","# outputs: (batch_size, num_classes)\n","# labels: (batch_size)\n","\n","_, preds = outputs.topk(5, dim=1)\n","\n","correct = preds.eq(labels.view(-1, 1))\n","\n","top5_acc = correct.any(dim=1).float().mean().item()\n","```\n","\n","### Important\n","\n","Use:\n","\n","```python\n","correct.any(dim=1)\n","```\n","\n","Not `.sum()` directly.\n","\n","Because we only need to know:\n","\n","Did at least one of the top-k match?"]},{"cell_type":"markdown","id":"b349aaad","metadata":{"papermill":{"duration":0.004218,"end_time":"2026-03-01T03:40:46.637746","exception":false,"start_time":"2026-03-01T03:40:46.633528","status":"completed"},"tags":[]},"source":["# Accuracy vs CrossEntropy"]},{"cell_type":"markdown","id":"2b02efc4","metadata":{"papermill":{"duration":0.00405,"end_time":"2026-03-01T03:40:46.645978","exception":false,"start_time":"2026-03-01T03:40:46.641928","status":"completed"},"tags":[]},"source":["## What Accuracy Measures\n","\n","For classification:\n","\n","$$\n","\\hat{y} = \\arg\\max_i z_i\n","$$\n","\n","Accuracy checks:\n","\n","$$\n","\\hat{y} = y\n","$$\n","\n","It is a **binary metric**:\n","\n","- 1 → correct  \n","- 0 → incorrect  \n","\n","Accuracy ignores:\n","- Confidence\n","- Probability distribution\n","- Margin between classes\n","\n","It only cares about the final predicted class.\n","\n","**Intuition**\n","\n","Accuracy answers:\n","\n","> Did the model predict the correct class?\n","\n","Cross-entropy answers:\n","\n","> How confident was the model about the correct class?"]},{"cell_type":"markdown","id":"95822750","metadata":{"papermill":{"duration":0.004754,"end_time":"2026-03-01T03:40:46.654904","exception":false,"start_time":"2026-03-01T03:40:46.65015","status":"completed"},"tags":[]},"source":["## What Cross-Entropy Measures\n","\n","After softmax:\n","\n","$$\n","p_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n","$$\n","\n","Cross-entropy loss:\n","\n","$$\n","\\mathcal{L} = -\\log(p_y)\n","$$\n","\n","It measures how much probability the model assigns to the true class.\n","\n","- Higher $p_y$ → lower loss  \n","- Lower $p_y$ → higher loss  \n","\n","Cross-entropy evaluates **probability quality**, not just correctness."]},{"cell_type":"markdown","id":"619b29f2","metadata":{"papermill":{"duration":0.004113,"end_time":"2026-03-01T03:40:46.663531","exception":false,"start_time":"2026-03-01T03:40:46.659418","status":"completed"},"tags":[]},"source":["## Same Accuracy, Different Loss\n","\n","### Case A: Correct but Low Confidence\n","\n","True class = 1  \n","\n","$$\n","[0.40,\\ 0.35,\\ 0.25]\n","$$\n","\n","Accuracy = 1  \n","\n","Loss:\n","\n","$$\n","-\\log(0.40) \\approx 0.92\n","$$\n","\n","\n","### Case B: Correct with High Confidence\n","\n","$$\n","[0.85,\\ 0.10,\\ 0.05]\n","$$\n","\n","Accuracy = 1  \n","\n","Loss:\n","\n","$$\n","-\\log(0.85) \\approx 0.16\n","$$\n","\n","\n","Accuracy did not change.  \n","Loss improved significantly."]},{"cell_type":"markdown","id":"ddaca4db","metadata":{"papermill":{"duration":0.004115,"end_time":"2026-03-01T03:40:46.671882","exception":false,"start_time":"2026-03-01T03:40:46.667767","status":"completed"},"tags":[]},"source":["## Same Wrong Prediction, Different Loss\n","\n","### Very Wrong (Overconfident)\n","\n","$$\n","[0.90,\\ 0.05,\\ 0.05]\n","$$\n","\n","Loss:\n","\n","$$\n","-\\log(0.05) \\approx 3.00\n","$$\n","\n","### Less Wrong\n","\n","$$\n","[0.45,\\ 0.40,\\ 0.15]\n","$$\n","\n","Loss:\n","\n","$$\n","-\\log(0.40) \\approx 0.92\n","$$\n","\n","Accuracy = 0 in both cases.  \n","Loss improved dramatically.\n"]},{"cell_type":"markdown","id":"1273b890","metadata":{"papermill":{"duration":0.004124,"end_time":"2026-03-01T03:40:46.680249","exception":false,"start_time":"2026-03-01T03:40:46.676125","status":"completed"},"tags":[]},"source":["## Core Difference\n","\n","Accuracy:\n","\n","$$\n","\\mathbf{1}(\\arg\\max z = y)\n","$$\n","\n","Cross-Entropy:\n","\n","$$\n","-\\log(p_y)\n","$$\n","\n","Accuracy is discrete.  \n","Cross-entropy is continuous and smooth."]},{"cell_type":"markdown","id":"aa6cfabc","metadata":{"papermill":{"duration":0.004508,"end_time":"2026-03-01T03:40:46.688984","exception":false,"start_time":"2026-03-01T03:40:46.684476","status":"completed"},"tags":[]},"source":["# Practical Evaluation Strategy\n","\n","A serious evaluation pipeline tracks multiple signals, not just one number.\n","\n","Monitor:\n","\n","✔ Training loss  \n","✔ Validation loss  \n","✔ Top-1 accuracy  \n","✔ Top-5 accuracy (for large-class problems)  \n","✔ Per-class accuracy  \n","✔ Confusion matrix  \n","\n","Each metric answers a different question."]},{"cell_type":"markdown","id":"637e165f","metadata":{"papermill":{"duration":0.004119,"end_time":"2026-03-01T03:40:46.697445","exception":false,"start_time":"2026-03-01T03:40:46.693326","status":"completed"},"tags":[]},"source":["## What Each Metric Actually Tells You\n","\n","### 1️. Training Loss\n","\n","Measures:\n","\n","$$\n","\\mathcal{L}_{train} = -\\log(p_y)\n","$$\n","\n","Tells you:\n","- Is optimization working?\n","- Is the model learning useful representations?\n","\n","If training loss is flat → something is broken.\n","\n","\n","### 2️. Validation Loss\n","\n","Measures generalization:\n","\n","$$\n","\\mathcal{L}_{val}\n","$$\n","\n","Compare:\n","\n","- Train ↓, Val ↓ → healthy learning  \n","- Train ↓, Val ↑ → overfitting  \n","- Both high → underfitting  \n","\n","Loss reacts **before** accuracy does.\n","\n","### 3️. Top-1 Accuracy\n","\n","$$\n","\\text{Top-1} = \\frac{\\text{Correct predictions}}{\\text{Total samples}}\n","$$\n","\n","Strict correctness metric.\n","\n","Good for:\n","- Balanced datasets\n","- Small number of classes\n","\n","### 4️. Top-5 Accuracy (Large Class Problems)\n","\n","Correct if:\n","\n","$$\n","y \\in S_5\n","$$\n","\n","Where $S_5$ is the set of top 5 predicted classes.\n","\n","Useful when:\n","- Many classes (e.g., 1000)\n","- Fine-grained distinctions\n","- Ambiguous labeling\n","\n","It measures ranking quality."]},{"cell_type":"markdown","id":"8a0082c3","metadata":{"papermill":{"duration":0.00412,"end_time":"2026-03-01T03:40:46.705705","exception":false,"start_time":"2026-03-01T03:40:46.701585","status":"completed"},"tags":[]},"source":["### 5. Per-Class Accuracy\n","\n","For class $i$:\n","\n","$$\n","\\text{Accuracy}_i =\n","\\frac{\\text{Correct predictions for class } i}\n","{\\text{Total samples of class } i}\n","$$\n","\n","### Why Overall Accuracy Can Mislead\n","\n","Example:\n","\n","- Class A: 900 samples  \n","- Class B: 100 samples  \n","\n","Model predicts everything as Class A.\n","\n","Overall accuracy:\n","\n","$$\n","\\frac{900}{1000} = 90\\%\n","$$\n","\n","Looks good.\n","\n","But:\n","\n","$$\n","\\text{Accuracy}_A = 100\\%\n","$$\n","\n","$$\n","\\text{Accuracy}_B = 0\\%\n","$$\n","\n","The model completely fails minority class B.\n","\n","Overall accuracy hides this.\n"]},{"cell_type":"markdown","id":"7e42ccf8","metadata":{"papermill":{"duration":0.004056,"end_time":"2026-03-01T03:40:46.713894","exception":false,"start_time":"2026-03-01T03:40:46.709838","status":"completed"},"tags":[]},"source":["### 6. Confusion Matrix\n","\n","Matrix $C$ where:\n","\n","$$\n","C_{ij} = \\text{number of samples with } y=i \\text{ and } \\hat{y}=j\n","$$\n","\n","Where:\n","\n","- $i$ = true class  \n","- $j$ = predicted class\n","  \n","Reveals:\n","\n","- Systematic confusions\n","- Bias toward dominant classes\n","- Which classes collapse into each other\n","\n","It gives structural insight beyond scalar metrics.\n"]},{"cell_type":"markdown","id":"fda54e66","metadata":{"papermill":{"duration":0.004059,"end_time":"2026-03-01T03:40:46.722092","exception":false,"start_time":"2026-03-01T03:40:46.718033","status":"completed"},"tags":[]},"source":["# Professional Multi-Class CNN Pipeline\n","\n","1️⃣ Final layer → Linear(features, C)  \n","2️⃣ Use CrossEntropyLoss  \n","3️⃣ Track Top-1 accuracy  \n","4️⃣ If imbalance → use weighted loss or sampler  \n","5️⃣ If many classes → track Top-k  \n","6️⃣ Monitor both loss and accuracy  "]},{"cell_type":"markdown","id":"00919717","metadata":{"papermill":{"duration":0.004068,"end_time":"2026-03-01T03:40:46.730297","exception":false,"start_time":"2026-03-01T03:40:46.726229","status":"completed"},"tags":[]},"source":["# Key Takeaways from Day 39\n","\n","- CNN outputs logits, not probabilities  \n","- Softmax converts logits to probability distribution  \n","- CrossEntropyLoss includes Softmax internally  \n","- Class imbalance can distort accuracy  \n","- Weighted loss or sampling fixes imbalance  \n","- Top-k accuracy is essential in large-class problems  \n","- Loss quality and accuracy are different signals  "]},{"cell_type":"markdown","id":"f72c1a9f","metadata":{"papermill":{"duration":0.004294,"end_time":"2026-03-01T03:40:46.738785","exception":false,"start_time":"2026-03-01T03:40:46.734491","status":"completed"},"tags":[]},"source":["---\n","\n","<p style=\"text-align:center; color:skyblue; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31286,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":6.209543,"end_time":"2026-03-01T03:40:49.960931","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-03-01T03:40:43.751388","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}