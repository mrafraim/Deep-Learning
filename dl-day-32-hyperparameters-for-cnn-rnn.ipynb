{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrafraim/dl-day-32-hyperparameters-for-cnn-rnn?scriptVersionId=294808330\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"62fdc403","metadata":{"papermill":{"duration":0.00365,"end_time":"2026-01-29T17:31:10.422678","exception":false,"start_time":"2026-01-29T17:31:10.419028","status":"completed"},"tags":[]},"source":["# Day 32: Hyperparameters for CNN/RNN\n","\n","Welcome to Day 32!\n","\n","Today you'll learn:\n","\n","1. What hyperparameters actually control\n","2. Learning rate, the most dangerous knob\n","3. Optimizers and their behavior\n","4. CNN-specific tuning heuristics\n","5. RNN-specific tuning heuristics\n","6. Practical tuning workflow (real world)\n","\n","If you found this notebook helpful, your **<b style=\"color:orange;\">UPVOTE</b>** would be greatly appreciated! It helps others discover the work and supports continuous improvement.\n","\n","---\n"]},{"cell_type":"markdown","id":"3b9fbdd9","metadata":{"papermill":{"duration":0.002521,"end_time":"2026-01-29T17:31:10.427938","exception":false,"start_time":"2026-01-29T17:31:10.425417","status":"completed"},"tags":[]},"source":["# What Are Hyperparameters?\n","\n","Hyperparameters are:\n","- Set before training\n","- Not learned from data\n","- Control how learning happens\n","\n","Examples:\n","- Learning rate\n","- Optimizer type\n","- Batch size\n","- Number of layers\n","- Dropout rate\n"]},{"cell_type":"markdown","id":"7ab6c0b3","metadata":{"papermill":{"duration":0.002487,"end_time":"2026-01-29T17:31:10.432961","exception":false,"start_time":"2026-01-29T17:31:10.430474","status":"completed"},"tags":[]},"source":["# The Learning Rate\n","\n","Learning rate $\\alpha$ controls update size:\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta L\n","$$\n","\n","Where:\n","- $\\theta$ = model parameters\n","- $\\nabla_\\theta L$ = gradient\n","- $\\alpha$ = learning rate\n"]},{"cell_type":"markdown","id":"52465938","metadata":{"papermill":{"duration":0.002489,"end_time":"2026-01-29T17:31:10.437949","exception":false,"start_time":"2026-01-29T17:31:10.43546","status":"completed"},"tags":[]},"source":["## Learning Rate Intuition\n","\n","| Learning Rate | Behavior |\n","|-------------|---------|\n","| Too small | Very slow convergence |\n","| Too large | Loss oscillates / diverges |\n","| Just right | Fast, stable descent |\n","\n","Visual intuition:\n","- Small steps → safe but slow\n","- Large steps → unstable jumps\n"]},{"cell_type":"markdown","id":"bf5d8bd5","metadata":{"papermill":{"duration":0.002387,"end_time":"2026-01-29T17:31:10.442842","exception":false,"start_time":"2026-01-29T17:31:10.440455","status":"completed"},"tags":[]},"source":["# PART A: CNN Hyperparameters\n"]},{"cell_type":"markdown","id":"5c5fe4e5","metadata":{"papermill":{"duration":0.00239,"end_time":"2026-01-29T17:31:10.447733","exception":false,"start_time":"2026-01-29T17:31:10.445343","status":"completed"},"tags":[]},"source":["## 1. CNN Learning Rate Heuristics\n","\n","\n","### What Is Learning Rate (LR)?\n","\n","Learning rate ($\\eta$) controls how much the weights change during each update.\n","\n","$$\n","w_{new} = w_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n","$$\n","\n","- Small $\\eta$ → very slow learning  \n","- Large $\\eta$ → unstable training / divergence  \n","\n","Choosing the right LR is critical for CNN training.\n","\n","\n","### Why CNNs Tolerate Higher Learning Rates\n","\n","CNNs have built-in properties that stabilize training:\n","\n","#### 1️⃣ Weight Sharing\n","- Same filter applied across spatial locations\n","- Gradients are averaged over many pixels\n","- Reduces gradient noise\n","\n","#### 2️⃣ Local Receptive Fields\n","- Each neuron sees only a small region\n","- Gradients are less chaotic than fully connected layers\n","\n","#### 3️⃣ Structured Depth\n","- Repeated Conv → Norm → Activation blocks\n","- Optimizers adapt faster\n","\n","Result: **CNN gradients are naturally stable**\n","\n","\n","### Role of Batch Normalization in CNNs\n","\n","Batch Normalization normalizes activations per mini-batch:\n","\n","$$\n","\\hat{x} = \\frac{x - \\mu_{batch}}{\\sqrt{\\sigma^2_{batch} + \\epsilon}}\n","$$\n","\n","Then applies learnable scaling:\n","\n","$$\n","y = \\gamma \\hat{x} + \\beta\n","$$\n","\n","### Why BatchNorm Enables Higher LR\n","\n","BatchNorm:\n","- Keeps activations near mean $0$\n","- Prevents exploding/vanishing gradients\n","- Makes gradient scale predictable\n","\n","Effect:\n","> CNNs with BatchNorm can safely use **larger learning rates**\n","\n","\n","### Typical Learning Rates for CNNs (Practice)\n","\n","#### SGD (with momentum)\n","\n","| Setup | Learning Rate |\n","|------|---------------|\n","| No BatchNorm | $0.01$ |\n","| With BatchNorm | $0.05$ – $0.1$ |\n","| Large batch (≥128) | $0.1$ |\n","\n","Why:\n","- SGD depends heavily on gradient magnitude\n","- BatchNorm stabilizes gradients\n","\n","\n","#### Adam / AdamW\n","\n","| Setup | Learning Rate |\n","|------|---------------|\n","| Standard CNN | $1 \\times 10^{-3}$ |\n","| Deep CNN | $3 \\times 10^{-4}$ |\n","| With BatchNorm | up to $2 \\times 10^{-3}$ |\n","\n","Why lower than SGD:\n","- Adam adapts learning rate per parameter\n","- Too high LR causes overshooting\n","\n","\n","### Batch Size vs Learning Rate (CNN Rule)\n","\n","CNNs often follow linear LR scaling:\n","\n","$$\n","\\text{LR}_{new} = \\text{LR}_{base} \\times \\frac{\\text{Batch}_{new}}{\\text{Batch}_{base}}\n","$$\n","\n","#### Example\n","- Batch size = 32 → LR = $0.01$\n","- Batch size = 128 → LR = $0.04$\n","\n","⚠️ Works best when BatchNorm is used\n","\n","\n","### Signs of Incorrect Learning Rate\n","\n","#### LR Too High\n","- Loss oscillates or explodes\n","- Training accuracy stuck at random\n","- NaN values appear\n","\n","#### LR Too Low\n","- Very slow loss decrease\n","- Underfitting despite long training\n","- Wasted compute time\n","\n","\n","### Practical CNN LR Tuning Strategy\n","\n","1. Start with Adam at $1e^{-3}$\n","2. Add Batch Normalization\n","3. Increase LR gradually:\n","   - $1e^{-3}$ → $2e^{-3}$ → $3e^{-3}$\n","4. Observe loss curve:\n","   - Smooth decrease → good\n","   - Sudden spikes → too high\n","\n","\n","### Key Takeaways\n","\n","- CNNs have stable gradients\n","- BatchNorm increases stability further\n","- Higher stability → higher learning rates possible\n","- SGD benefits most from BatchNorm\n","- Large-batch CNNs almost require BatchNorm\n","\n"]},{"cell_type":"markdown","id":"b9a70650","metadata":{"papermill":{"duration":0.00251,"end_time":"2026-01-29T17:31:10.452609","exception":false,"start_time":"2026-01-29T17:31:10.450099","status":"completed"},"tags":[]},"source":["## 2. CNN Optimizers: When to Use Which\n","\n","Optimizers control how CNN weights are updated during training.  \n","They determine the speed, stability, and generalization of learning.\n","\n","### 1️⃣ SGD (Stochastic Gradient Descent)\n","\n","SGD updates weights using the gradient of the loss on one batch at a time:\n","\n","$$\n","w_{t+1} = w_t - \\eta \\cdot \\nabla L(w_t)\n","$$\n","\n","- $w_t$ → current weight  \n","- $\\eta$ → learning rate  \n","- $\\nabla L(w_t)$ → gradient of loss w.r.t. $w_t$  \n","\n","**Intuition:**  \n","- Move weights in the direction of decreasing loss.\n","- “Stochastic” because we use mini-batches, not the whole dataset.\n","\n","**Pros:**\n","- Simple and widely used  \n","- Works well for large datasets \n","- Often better generalization than adaptive optimizers\n","\n","**Cons:**\n","- Converges slowly, especially for deep CNNs  \n","- Can get stuck in local minima or saddle points\n","\n","**PyTorch Example:**\n","\n","```python\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","````\n","\n","**Use Case:**\n","\n","* Classic CNNs (e.g., ResNet, VGG)\n","* Large-scale image datasets\n","\n","\n","### 2️⃣ SGD + Momentum\n","\n","**Why Momentum?**\n","\n","* Standard SGD can oscillate along steep slopes\n","* Momentum “remembers” previous updates to accelerate in consistent directions:\n","\n","Mathematical Formulation:\n","\n","$$\n","v_{t+1} = \\mu v_t - \\eta \\nabla L(w_t)\n","$$\n","\n","$$\n","w_{t+1} = w_t + v_{t+1}\n","$$\n","\n","* $v_t$ → velocity (accumulated gradient)\n","* $\\mu$ → momentum coefficient (usually 0.9)\n","\n","**Intuition:**\n","\n","* Like a ball rolling down a hill: it keeps moving in the same direction, smoothing out oscillations.\n","\n","**Pros:**\n","\n","* Faster convergence than plain SGD\n","* Smoother updates\n","* Better for long-term training stability\n","\n","**Cons:**\n","\n","* Still sensitive to learning rate\n","* Can overshoot if LR too high\n","\n","**PyTorch Example:**\n","\n","```python\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","```\n","\n","**Use Case:**\n","\n","* Vision models (ResNet, EfficientNet)\n","* Large datasets with many epochs\n","\n","### 3️⃣ Adam (Adaptive Moment Estimation)\n","\n","Adam combines Momentum + RMSProp, adapting learning rate per parameter:\n","\n","Mathematical Steps:\n","\n","1. Compute moving averages of gradients:\n","\n","$$\n","m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla L(w_t)\n","$$\n","\n","2. Compute moving average of squared gradients:\n","\n","$$\n","v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla L(w_t))^2\n","$$\n","\n","3. Bias-corrected estimates:\n","\n","$$\n","\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n","$$\n","\n","4. Update weights:\n","\n","$$\n","w_{t+1} = w_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n","$$\n","\n","* $\\beta_1 \\approx 0.9$, $\\beta_2 \\approx 0.999$\n","* $\\epsilon \\approx 1e^{-8}$ for numerical stability\n","\n","**Intuition:**\n","\n","* Momentum-like term ($m_t$) accelerates learning\n","* RMSProp-like term ($v_t$) scales learning rate per weight\n","* Automatically adapts to gradient magnitude\n","\n","**Pros:**\n","\n","* Fast convergence\n","* Works well **out of the box**\n","* Handles sparse gradients\n","\n","**Cons:**\n","\n","* Can **overfit** if LR not tuned\n","* Sometimes **worse final accuracy** than SGD+Momentum\n","\n","**PyTorch Example:**\n","\n","```python\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","```\n","\n","**Use Case:**\n","\n","* Quick prototyping\n","* Small to medium CNNs\n","* When you need fast convergence\n","\n","\n","### Summary Table\n","\n","| Optimizer      | Key Idea                   | Pros                     | Cons                         | Best Use Case                  |\n","| -------------- | -------------------------- | ------------------------ | ---------------------------- | ------------------------------ |\n","| SGD            | Gradient descent per batch | Simple, generalizes well | Slow convergence             | Large datasets, vision CNNs    |\n","| SGD + Momentum | Accumulate past gradients  | Faster, smooth updates   | Sensitive to LR              | Deep CNNs, long training       |\n","| Adam           | Adaptive LR per weight     | Fast, good default       | Can overfit, final acc lower | Small/medium CNNs, prototyping |\n","\n","\n","**Learning Tip:**\n","\n","* For large, classic CNNs, start with SGD + Momentum.\n","* For quick experiments or small datasets, start with Adam.\n","* Always tune learning rate for best results.\n"]},{"cell_type":"markdown","id":"19bd5a68","metadata":{"papermill":{"duration":0.002425,"end_time":"2026-01-29T17:31:10.457506","exception":false,"start_time":"2026-01-29T17:31:10.455081","status":"completed"},"tags":[]},"source":["## 3️. CNN Batch Size:\n","\n","Batch size determines how many samples are processed before the model updates weights.  \n","\n","$$\n","\\text{Weight update happens after every batch of size } B\n","$$\n","\n","\n","### Effects of Batch Size\n","\n","1. **Large Batch Size**\n","- Examples: 128, 256  \n","- **Pros:**  \n","  - Gradients are averaged over many samples → smoother updates  \n","  - Can use higher learning rate\n","  - Training is more stable\n","- **Cons:**  \n","  - Requires more GPU memory  \n","  - Can sometimes generalize worse\n","\n","2. **Small Batch Size**\n","- Examples: 16, 32  \n","- **Pros:**  \n","  - More gradient noise → can help generalization \n","  - Works with limited memory\n","- **Cons:**  \n","  - Updates are noisy → loss may fluctuate  \n","  - May require smaller learning rate\n","\n","\n","### Typical CNN Batch Sizes\n","\n","| Batch Size | Notes |\n","|-----------|-------|\n","| 32 – 64  | Common default for modest GPUs, good balance of speed and generalization |\n","| 128 – 256 | Large GPUs, smoother training, faster convergence |\n","| >256     | Very large batch, requires careful LR tuning (Linear scaling rule) |\n","\n","\n","### Practical Tip\n","\n","- Start with batch size that fits your GPU memory\n","- Adjust learning rate based on batch size:\n","\n","$$\n","\\text{LR}_{new} = \\text{LR}_{base} \\times \\frac{\\text{Batch}_{new}}{\\text{Batch}_{base}}\n","$$\n","\n","- Combine with BatchNorm for stability at larger batches  \n","\n","\n","### Key Takeaways\n","\n","- Batch size = tradeoff between stability and generalization  \n","- Small batches → noisy gradients → better generalization  \n","- Large batches → smooth gradients → faster convergence  \n","- Always consider GPU memory limits\n"]},{"cell_type":"markdown","id":"1c1f6560","metadata":{"papermill":{"duration":0.002376,"end_time":"2026-01-29T17:31:10.462212","exception":false,"start_time":"2026-01-29T17:31:10.459836","status":"completed"},"tags":[]},"source":["# PART B: RNN Hyperparameters\n","\n","RNNs (Recurrent Neural Networks) behave differently from CNNs due to sequential dependencies.\n","This affects learning rate choices, optimizer selection, and batch size decisions.\n"]},{"cell_type":"markdown","id":"90ecdf19","metadata":{"papermill":{"duration":0.002379,"end_time":"2026-01-29T17:31:10.467149","exception":false,"start_time":"2026-01-29T17:31:10.46477","status":"completed"},"tags":[]},"source":["## Why RNNs Are Sensitive\n","\n","RNNs:\n","- Reuse parameters across time\n","- Accumulate gradients\n","- Prone to instability\n","\n","Result:\n","- Learning rate must be smaller\n","- Optimizer choice matters more"]},{"cell_type":"markdown","id":"15e2b382","metadata":{"papermill":{"duration":0.002302,"end_time":"2026-01-29T17:31:10.471893","exception":false,"start_time":"2026-01-29T17:31:10.469591","status":"completed"},"tags":[]},"source":["## 1️. RNN Learning Rate Heuristics\n","\n","RNNs are more sensitive to learning rate than CNNs because:\n","\n","- Gradients can explode or vanish due to repeated multiplications over time steps  \n","- Sequential dependencies make weight updates less stable  \n","\n","**Typical Learning Rates:**\n","\n","| Optimizer | LR (vanilla RNN) | Notes |\n","|-----------|-----------------|-------|\n","| SGD       | 0.01 – 0.05     | Small LR recommended for stability |\n","| SGD+Momentum | 0.01 – 0.05  | Helps smooth updates over time |\n","| Adam      | 1e-3 – 5e-4     | Good default for small/medium RNNs |\n","\n","**Tips:**\n","\n","- Use smaller LR than CNNs to prevent exploding gradients  \n","- Combine with *gradient clipping to handle large updates:\n","\n","$$\n","\\text{if } ||g|| > \\text{threshold}, \\quad g \\leftarrow g \\frac{\\text{threshold}}{||g||}\n","$$\n","\n","- Learning rate schedules (StepLR, Cosine, OneCycle) improve stability.\n","\n","***Gradient clipping** limits the magnitude of gradients during backpropagation to prevent exploding gradients and stabilize training."]},{"cell_type":"markdown","id":"704d96ef","metadata":{"papermill":{"duration":0.002318,"end_time":"2026-01-29T17:31:10.476594","exception":false,"start_time":"2026-01-29T17:31:10.474276","status":"completed"},"tags":[]},"source":["## 2️. RNN Optimizers\n","\n","### SGD / SGD + Momentum\n","\n","- Similar formulas as CNNs:\n","\n","$$\n","w_{t+1} = w_t - \\eta \\nabla L(w_t)\n","$$\n","\n","- Momentum helps smooth updates across time steps \n","- Works for long sequences if LR is small\n","\n","**Pros:**\n","- Often better generalization for sequential tasks  \n","- Simple and predictable\n","\n","**Cons:**\n","- Can be very slow for long sequences  \n","- Sensitive to learning rate\n","\n","**PyTorch Example:**\n","\n","```python\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","````\n","\n","\n","### Adam\n","\n","* Automatically adapts per-parameter learning rates\n","* Handles sparse or noisy gradients in sequential data\n","\n","**Pros:**\n","\n","* Fast convergence\n","* Handles varying gradient scales\n","\n","**Cons:**\n","\n","* Can overfit if LR not tuned\n","* Final accuracy sometimes worse than SGD+Momentum\n","\n","**PyTorch Example:**\n","\n","```python\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","```"]},{"cell_type":"markdown","id":"236b0844","metadata":{"papermill":{"duration":0.002425,"end_time":"2026-01-29T17:31:10.481382","exception":false,"start_time":"2026-01-29T17:31:10.478957","status":"completed"},"tags":[]},"source":["## 3️. RNN Batch Size\n","\n","Batch size in RNNs = number of sequences processed simultaneously.\n","\n","### Effects of Batch Size\n","\n","1. **Large Batch**\n","\n","* Examples: 64 – 256 sequences\n","* Pros:\n","\n","  * Smoother gradient estimates\n","  * Faster training per epoch\n","* Cons:\n","\n","  * Higher GPU memory usage\n","  * Can hurt generalization in sequential tasks\n","\n","2. **Small Batch**\n","\n","* Examples: 16 – 32 sequences\n","* Pros:\n","\n","  * Noisy gradients → better generalization\n","  * Works with limited memory\n","* Cons:\n","\n","  * Updates noisy → loss fluctuates\n","  * Training may be slower"]},{"cell_type":"markdown","id":"46d646a4","metadata":{"papermill":{"duration":0.002364,"end_time":"2026-01-29T17:31:10.486199","exception":false,"start_time":"2026-01-29T17:31:10.483835","status":"completed"},"tags":[]},"source":["## Practical Tips\n","\n","* Start with small batch sizes if sequences are long\n","* Use gradient clipping to stabilize training\n","* Combine with LayerNorm or BatchNorm (input only) to stabilize hidden states\n","\n","**Linear scaling rule for batch size:**\n","\n","$$\n","\\text{LR}*{new} = \\text{LR}*{base} \\times \\frac{\\text{Batch}*{new}}{\\text{Batch}*{base}}\n","$$\n","\n","* Works if RNN is properly normalized\n","\n","## Key Takeaways\n","\n","* **RNNs are fragile**: careful LR, optimizer, and batch size choices matter\n","* Use smaller learning rates than CNNs\n","* Prefer SGD+Momentum or Adam depending on task\n","* Small batches → better generalization, safer for long sequences\n","* Use normalization and gradient clipping to improve stability"]},{"cell_type":"markdown","id":"e1e483cf","metadata":{"papermill":{"duration":0.002339,"end_time":"2026-01-29T17:31:10.490922","exception":false,"start_time":"2026-01-29T17:31:10.488583","status":"completed"},"tags":[]},"source":["# Key Takeaways from Day 32\n","\n","- Learning rate is the primary control knob\n","- CNNs tolerate aggressive settings\n","- RNNs require caution and clipping\n","- Optimizer choice affects convergence, not intelligence\n","- Tuning is iterative, not magical\n","\n","---"]},{"cell_type":"markdown","id":"505c3220","metadata":{"papermill":{"duration":0.002389,"end_time":"2026-01-29T17:31:10.4958","exception":false,"start_time":"2026-01-29T17:31:10.493411","status":"completed"},"tags":[]},"source":["<p style=\"text-align:center; font-size:18px;\">\n","© 2026 Mostafizur Rahman\n","</p>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":3.695983,"end_time":"2026-01-29T17:31:10.816427","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-01-29T17:31:07.120444","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}